{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLanguage Translation with TorchText\n===================================\n\nThis tutorial shows how to use ``torchtext`` to preprocess\ndata from a well-known dataset containing sentences in both English and German and use it to\ntrain a sequence-to-sequence model with attention that can translate German sentences\ninto English.\n\nIt is based off of\n`this tutorial
<https: 3%20-%20neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate.ipynb="" bentrevett="" blob="" github.com="" master="" pytorch-seq2seq="">
 `__\nfrom PyTorch community member `Ben Trevett
 <https: bentrevett="" github.com="">
  `__\nwith Ben's permission. We update the tutorials by removing some legacy code.\n\nBy the end of this tutorial, you will be able to preprocess sentences into tensors for NLP modeling and use `torch.utils.data.DataLoader
  <https: data.html?highlight="dataloader#torch.utils.data.DataLoader" docs="" pytorch.org="" stable="">
   `__ for training and validing the model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Processing\n----------------\n``torchtext`` has utilities for creating datasets that can be easily\niterated through for the purposes of creating a language translation\nmodel. In this example, we show how to tokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor.\n\nNote: the tokenization in this tutorial requires `Spacy
   <https: spacy.io="">
    `__\nWe use Spacy because it provides strong support for tokenization in languages\nother than English. ``torchtext`` provides a ``basic_english`` tokenizer\nand supports other tokenizers for English (e.g.\n`Moses
    <https: bitbucket.org="" default="" luismsgomes="" mosestokenizer="" src="">
    </https:>
    `__)\nbut for language translation - where multiple languages are required -\nSpacy is your best bet.\n\nTo run this tutorial, first install ``spacy`` using ``pip`` or ``conda``.\nNext, download the raw data for the English and German Spacy tokenizers:\n\n::\n\n   python -m spacy download en\n   python -m spacy download de\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torchtext\nimport torch\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\nfrom torchtext.utils import download_from_url, extract_archive\nimport io\n\nurl_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\ntrain_urls = ('train.de.gz', 'train.en.gz')\nval_urls = ('val.de.gz', 'val.en.gz')\ntest_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n\ntrain_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\nval_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\ntest_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n\nde_tokenizer = get_tokenizer('spacy', language='de')\nen_tokenizer = get_tokenizer('spacy', language='en')\n\ndef build_vocab(filepath, tokenizer):\n  counter = Counter()\n  with io.open(filepath, encoding=\"utf8\") as f:\n    for string_ in f:\n      counter.update(tokenizer(string_))\n  return Vocab(counter, specials=['
    <unk>
     ', '
     <pad>
      ', '
      <bos>
       ', '
       <eos>
        '])\n\nde_vocab = build_vocab(train_filepaths[0], de_tokenizer)\nen_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n\ndef data_process(filepaths):\n  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n  data = []\n  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n                            dtype=torch.long)\n    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n                            dtype=torch.long)\n    data.append((de_tensor_, en_tensor_))\n  return data\n\ntrain_data = data_process(train_filepaths)\nval_data = data_process(val_filepaths)\ntest_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``DataLoader``\n----------------\nThe last ``torch`` specific feature we'll use is the ``DataLoader``,\nwhich is easy to use since it takes the data as its\nfirst argument. Specifically, as the docs say:\n``DataLoader`` combines a dataset and a sampler, and provides an iterable over the given dataset. The ``DataLoader`` supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. \n\nPlease pay attention to ``collate_fn`` (optional) that merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nBATCH_SIZE = 128\nPAD_IDX = de_vocab['
        <pad>
         ']\nBOS_IDX = de_vocab['
         <bos>
          ']\nEOS_IDX = de_vocab['
          <eos>
           ']\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\ndef generate_batch(data_batch):\n  de_batch, en_batch = [], []\n  for (de_item, en_item) in data_batch:\n    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n  return de_batch, en_batch\n\ntrain_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\nvalid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\ntest_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n                       shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining our ``nn.Module`` and ``Optimizer``\n----------------\nThat's mostly it from a ``torchtext`` perspecive: with the dataset built\nand the iterator defined, the rest of this tutorial simply defines our\nmodel as an ``nn.Module``, along with an ``Optimizer``, and then trains it.\n\nOur model specifically, follows the architecture described\n`here
           <https: 1409.0473="" abs="" arxiv.org="">
            `__ (you can find a\nsignificantly more commented version\n`here
            <https: 3%20-%20neural%20machine%20translation%20by%20jointly%20learning%20to%20align%20and%20translate.ipynb="" blob="" github.com="" master="" pytorch-seq2seq="" sethhweidman="">
             `__).\n\nNote: this model is just an example model that can be used for language\ntranslation; we choose it because it is a standard model for the task,\nnot because it is the recommended model to use for translation. As you're\nlikely aware, state-of-the-art models are currently based on Transformers;\nyou can see PyTorch's capabilities for implementing Transformer layers\n`here
             <https: docs="" nn.html#transformer-layers="" pytorch.org="" stable="">
              `__; and\nin particular, the \"attention\" used in the model below is different from\nthe multi-headed self-attention present in a transformer model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nfrom typing import Tuple\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass Encoder(nn.Module):\n    def __init__(self,\n                 input_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: float):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.dropout = dropout\n\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n\n        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n\n        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self,\n                src: Tensor) -&gt; Tuple[Tensor]:\n\n        embedded = self.dropout(self.embedding(src))\n\n        outputs, hidden = self.rnn(embedded)\n\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n\n        return outputs, hidden\n\n\nclass Attention(nn.Module):\n    def __init__(self,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 attn_dim: int):\n        super().__init__()\n\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n\n        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n\n        self.attn = nn.Linear(self.attn_in, attn_dim)\n\n    def forward(self,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -&gt; Tensor:\n\n        src_len = encoder_outputs.shape[0]\n\n        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        energy = torch.tanh(self.attn(torch.cat((\n            repeated_decoder_hidden,\n            encoder_outputs),\n            dim = 2)))\n\n        attention = torch.sum(energy, dim=2)\n\n        return F.softmax(attention, dim=1)\n\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 output_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: int,\n                 attention: nn.Module):\n        super().__init__()\n\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.output_dim = output_dim\n        self.dropout = dropout\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n\n        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n\n        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n\n    def _weighted_encoder_rep(self,\n                              decoder_hidden: Tensor,\n                              encoder_outputs: Tensor) -&gt; Tensor:\n\n        a = self.attention(decoder_hidden, encoder_outputs)\n\n        a = a.unsqueeze(1)\n\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n\n        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n\n        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n\n        return weighted_encoder_rep\n\n\n    def forward(self,\n                input: Tensor,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -&gt; Tuple[Tensor]:\n\n        input = input.unsqueeze(0)\n\n        embedded = self.dropout(self.embedding(input))\n\n        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden,\n                                                          encoder_outputs)\n\n        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n\n        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n\n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n\n        output = self.out(torch.cat((output,\n                                     weighted_encoder_rep,\n                                     embedded), dim = 1))\n\n        return output, decoder_hidden.squeeze(0)\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self,\n                 encoder: nn.Module,\n                 decoder: nn.Module,\n                 device: torch.device):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                teacher_forcing_ratio: float = 0.5) -&gt; Tensor:\n\n        batch_size = src.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n\n        encoder_outputs, hidden = self.encoder(src)\n\n        # first input to the decoder is the
              <sos>
               token\n        output = trg[0,:]\n\n        for t in range(1, max_len):\n            output, hidden = self.decoder(output, hidden, encoder_outputs)\n            outputs[t] = output\n            teacher_force = random.random() &lt; teacher_forcing_ratio\n            top1 = output.max(1)[1]\n            output = (trg[t] if teacher_force else top1)\n\n        return outputs\n\n\nINPUT_DIM = len(de_vocab)\nOUTPUT_DIM = len(en_vocab)\n# ENC_EMB_DIM = 256\n# DEC_EMB_DIM = 256\n# ENC_HID_DIM = 512\n# DEC_HID_DIM = 512\n# ATTN_DIM = 64\n# ENC_DROPOUT = 0.5\n# DEC_DROPOUT = 0.5\n\nENC_EMB_DIM = 32\nDEC_EMB_DIM = 32\nENC_HID_DIM = 64\nDEC_HID_DIM = 64\nATTN_DIM = 8\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n\nattn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n\nmodel = Seq2Seq(enc, dec, device).to(device)\n\n\ndef init_weights(m: nn.Module):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n\n\nmodel.apply(init_weights)\n\noptimizer = optim.Adam(model.parameters())\n\n\ndef count_parameters(model: nn.Module):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: when scoring the performance of a language translation model in\nparticular, we have to tell the ``nn.CrossEntropyLoss`` function to\nignore the indices where the target is simply padding.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PAD_IDX = en_vocab.stoi['
               <pad>
                ']\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can train and evaluate this model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\nimport time\n\n\ndef train(model: nn.Module,\n          iterator: torch.utils.data.DataLoader,\n          optimizer: optim.Optimizer,\n          criterion: nn.Module,\n          clip: float):\n\n    model.train()\n\n    epoch_loss = 0\n\n    for _, (src, trg) in enumerate(iterator):\n        src, trg = src.to(device), trg.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(src, trg)\n\n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg[1:].view(-1)\n\n        loss = criterion(output, trg)\n\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef evaluate(model: nn.Module,\n             iterator: torch.utils.data.DataLoader,\n             criterion: nn.Module):\n\n    model.eval()\n\n    epoch_loss = 0\n\n    with torch.no_grad():\n\n        for _, (src, trg) in enumerate(iterator):\n            src, trg = src.to(device), trg.to(device)\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg[1:].view(-1)\n\n            loss = criterion(output, trg)\n\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef epoch_time(start_time: int,\n               end_time: int):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\nN_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n\n    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iter, criterion)\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\ntest_loss = evaluate(model, test_iter, criterion)\n\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next steps\n--------------\n\n- Check out the rest of Ben Trevett's tutorials using ``torchtext``\n  `here
                <https: bentrevett="" github.com="">
                </https:>
                `__\n- Stay tuned for a tutorial using other ``torchtext`` features along\n  with ``nn.Transformer`` for language modeling via next word prediction!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
               </pad>
              </sos>
             </https:>
            </https:>
           </https:>
          </eos>
         </bos>
        </pad>
       </eos>
      </bos>
     </pad>
    </unk>
   </https:>
  </https:>
 </https:>
</https:>
