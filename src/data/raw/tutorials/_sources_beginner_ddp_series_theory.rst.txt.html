`Introduction
<ddp_series_intro.html>
 `__ \|\| **What is DDP** \|\|
`Single-Node Multi-GPU Training
 <ddp_series_multigpu.html>
  `__ \|\|
`Fault Tolerance
  <ddp_series_fault_tolerance.html>
   `__ \|\|
`Multi-Node training &lt;../intermediate/ddp_series_multinode.html&gt;`__ \|\|
`minGPT Training &lt;../intermediate/ddp_series_minGPT.html&gt;`__

What is Distributed Data Parallel (DDP)
=======================================

Authors: `Suraj Subramanian
   <https: github.com="" subramen="">
    `__

.. grid:: 2

   .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn
      :class-card: card-prerequisites

      *  How DDP works under the hood
      *  What is ``DistributedSampler``
      *  How gradients are synchronized across GPUs


   .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites
      :class-card: card-prerequisites

      * Familiarity with `basic non-distributed training
    <https: basics="" beginner="" pytorch.org="" quickstart_tutorial.html="" tutorials="">
     `__ in PyTorch

Follow along with the video below or on `youtube
     <https: cvdhwx-obbo="" watch="" www.youtube.com="">
      `__.

.. raw:: html
      <div style="margin-top:10px; margin-bottom:10px;">
       <iframe allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/Cvdhwx-OBBo" width="560">
       </iframe>
      </div>
      This tutorial is a gentle introduction to PyTorch `DistributedDataParallel
      <https: docs="" generated="" pytorch.org="" stable="" torch.nn.parallel.distributeddataparallel.html="">
       `__ (DDP)
which enables data parallel training in PyTorch. Data parallelism is a way to
process multiple data batches across multiple devices simultaneously
to achieve better performance. In PyTorch, the `DistributedSampler
       <https: data.html#torch.utils.data.distributed.distributedsampler="" docs="" pytorch.org="" stable="">
        `__
ensures each device gets a non-overlapping input batch. The model is replicated on all the devices;
each replica calculates gradients and simultaneously synchronizes with the others using the `ring all-reduce
algorithm
        <https: blog="" en="" tech.preferred.jp="" technologies-behind-distributed-deep-learning-allreduce="">
        </https:>
        `__.

This `illustrative tutorial
        <https: dist_tuto.html#="" intermediate="" pytorch.org="" tutorials="">
         `__ provides a more in-depth python view of the mechanics of DDP.

Why you should prefer DDP over ``DataParallel`` (DP)
----------------------------------------------------

`DataParallel
         <https: docs="" generated="" pytorch.org="" stable="" torch.nn.dataparallel.html="">
          `__ 
is an older approach to data parallelism. DP is trivially simple (with just one extra line of code) but it is much less performant.
DDP improves upon the architecture in a few ways:

+---------------------------------------+------------------------------+
| ``DataParallel``                      | ``DistributedDataParallel``  |
+=======================================+==============================+
| More overhead; model is replicated    | Model is replicated only     |
| and destroyed at each forward pass    | once                         |
+---------------------------------------+------------------------------+
| Only supports single-node parallelism | Supports scaling to multiple |
|                                       | machines                     |
+---------------------------------------+------------------------------+
| Slower; uses multithreading on a      | Faster (no GIL contention)   |
| single process and runs into Global   | because it uses              |
| Interpreter Lock (GIL) contention     | multiprocessing              |
+---------------------------------------+------------------------------+

Further Reading
---------------

-  `Multi-GPU training with DDP
          <ddp_series_multigpu.html>
           `__ (next tutorial in this series)
-  `DDP
   API
           <https: docs="" generated="" pytorch.org="" stable="" torch.nn.parallel.distributeddataparallel.html="">
            `__
-  `DDP Internal
   Design
            <https: ddp.html#internal-design="" docs="" master="" notes="" pytorch.org="">
             `__
-  `DDP Mechanics Tutorial
             <https: dist_tuto.html#="" intermediate="" pytorch.org="" tutorials="">
              `__
             </https:>
            </https:>
           </https:>
          </ddp_series_multigpu.html>
         </https:>
        </https:>
       </https:>
      </https:>
     </https:>
    </https:>
   </https:>
  </ddp_series_fault_tolerance.html>
 </ddp_series_multigpu.html>
</ddp_series_intro.html>
