<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Quantization API Reference — PyTorch 2.7 documentation
  </title>
  <link href="https://pytorch.org/docs/stable/quantization-support.html" rel="canonical"/>
  <link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link href="_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="_static/katex-math.css" rel="stylesheet" type="text/css"/>
  <link href="_static/sphinx-dropdown.css" rel="stylesheet" type="text/css"/>
  <link href="_static/panels-bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/jit.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/custom.css" rel="stylesheet" type="text/css"/>
  <link href="genindex.html" rel="index" title="Index"/>
  <link href="search.html" rel="search" title="Search"/>
  <link href="generated/torch.ao.quantization.quantize.html" rel="next" title="quantize"/>
  <link href="quantization.html" rel="prev" title="Quantization"/>
  <!-- Google Tag Manager -->
  <script>
   (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');
  </script>
  <!-- End Google Tag Manager -->
  <script src="_static/js/modernizr.min.js">
  </script>
  <!-- Preload the theme fonts -->
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
  <!-- Preload the katex fonts -->
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
  <link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
 </head>
 <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
   <div class="header-container">
    <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
    </a>
    <div class="main-menu">
     <ul>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Learn
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
          <span class="dropdown-title">
           Get Started
          </span>
          <p>
           Run PyTorch locally or get started quickly with one of the supported cloud platforms
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
          <span class="dropdown-title">
           Tutorials
          </span>
          <p>
           Whats new in PyTorch tutorials
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          <span class="dropdown-title">
           Learn the Basics
          </span>
          <p>
           Familiarize yourself with PyTorch concepts and modules
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          <span class="dropdown-title">
           PyTorch Recipes
          </span>
          <p>
           Bite-size, ready-to-deploy PyTorch code examples
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
          <span class="dropdown-title">
           Intro to PyTorch - YouTube Series
          </span>
          <p>
           Master PyTorch basics with our engaging YouTube tutorial series
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Ecosystem
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
          <span class="dropdown-title">
           Tools
          </span>
          <p>
           Learn about the tools and frameworks in the PyTorch Ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
          <span class="dropdown-title">
           Community
          </span>
          <p>
           Join the PyTorch developer community to contribute, learn, and get your questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
          <span class="dropdown-title">
           Forums
          </span>
          <p>
           A place to discuss PyTorch code, issues, install, research
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/resources">
          <span class="dropdown-title">
           Developer Resources
          </span>
          <p>
           Find resources and get questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
          <span class="dropdown-title">
           Contributor Awards - 2024
          </span>
          <p>
           Award winners announced at this year's PyTorch Conference
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Edge
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/edge">
          <span class="dropdown-title">
           About PyTorch Edge
          </span>
          <p>
           Build innovative and privacy-aware AI experiences for edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
          <span class="dropdown-title">
           ExecuTorch
          </span>
          <p>
           End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
          <span class="dropdown-title">
           ExecuTorch Docs
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Docs
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
          <span class="dropdown-title">
           PyTorch
          </span>
          <p>
           Explore the documentation for comprehensive guidance on how to use PyTorch
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
          <span class="dropdown-title">
           PyTorch Domains
          </span>
          <p>
           Read the PyTorch Domains documentation to learn more about domain-specific libraries
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Blogs &amp; News
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
          <span class="dropdown-title">
           PyTorch Blog
          </span>
          <p>
           Catch up on the latest technical news and happenings
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
          <span class="dropdown-title">
           Community Blog
          </span>
          <p>
           Stories from the PyTorch ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/videos">
          <span class="dropdown-title">
           Videos
          </span>
          <p>
           Learn about the latest PyTorch tutorials, new, and more
          </p>
          <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
           <span class="dropdown-title">
            Community Stories
           </span>
           <p>
            Learn how our community solves real, everyday machine learning problems with PyTorch
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/events">
           <span class="dropdown-title">
            Events
           </span>
           <p>
            Find events, webinars, and podcasts
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
           <span class="dropdown-title">
            Newsletter
           </span>
           <p>
            Stay up-to-date with the latest updates
           </p>
          </a>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         About
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
          <span class="dropdown-title">
           PyTorch Foundation
          </span>
          <p>
           Learn more about the PyTorch Foundation
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
          <span class="dropdown-title">
           Governing Board
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/credits">
          <span class="dropdown-title">
           Cloud Credit Program
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tac">
          <span class="dropdown-title">
           Technical Advisory Council
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/staff">
          <span class="dropdown-title">
           Staff
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
          <span class="dropdown-title">
           Contact Us
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="no-dropdown">
        <a data-cta="join" href="https://pytorch.org/join">
         Become a Member
        </a>
       </div>
      </li>
      <li>
       <div class="main-menu-item">
        <a class="github-icon" href="https://github.com/pytorch/pytorch">
        </a>
       </div>
      </li>
      <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
     </ul>
    </div>
    <a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
    </a>
   </div>
  </div>
 </div>
 <body class="pytorch-body">
  <div class="table-of-contents-link-wrapper">
   <span>
    Table of Contents
   </span>
   <a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#">
   </a>
  </div>
  <nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
   <div class="pytorch-side-scroll">
    <div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
     <div class="pytorch-left-menu-search">
      <div class="version">
       <a href="https://pytorch.org/docs/versions.html">
        2.7 ▼
       </a>
      </div>
      <div id="searchBox">
       <div class="searchbox" id="googleSearchBox">
        <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e">
        </script>
        <div class="gcse-search">
        </div>
       </div>
       <div id="sphinxSearchBox" style="display: none;">
        <div role="search">
         <form action="search.html" class="wy-form" id="rtd-search-form" method="get">
          <input name="q" placeholder="Search Docs" type="text"/>
          <input name="check_keywords" type="hidden" value="yes"/>
          <input name="area" type="hidden" value="default"/>
         </form>
        </div>
       </div>
      </div>
      <form id="searchForm">
       <label style="margin-bottom: 1rem">
        <input checked="" name="searchType" type="radio" value="google"/>
        Google Search
       </label>
       <label style="margin-bottom: 1rem">
        <input name="searchType" type="radio" value="sphinx"/>
        Classic Search
       </label>
      </form>
      <script>
       document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
      </script>
     </div>
     <p class="caption" role="heading">
      <span class="caption-text">
       Community
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="community/build_ci_governance.html">
        PyTorch Governance | Build + CI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/contribution_guide.html">
        PyTorch Contribution Guide
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/design.html">
        PyTorch Design Philosophy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/governance.html">
        PyTorch Governance | Mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/persons_of_interest.html">
        PyTorch Governance | Maintainers
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Developer Notes
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/amp_examples.html">
        Automatic Mixed Precision examples
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/autograd.html">
        Autograd mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/broadcasting.html">
        Broadcasting semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">
        CPU threading and TorchScript inference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/cuda.html">
        CUDA semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/custom_operators.html">
        PyTorch Custom Operators Landing Page
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/ddp.html">
        Distributed Data Parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/extending.html">
        Extending PyTorch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/extending.func.html">
        Extending torch.func with autograd.Function
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/faq.html">
        Frequently Asked Questions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/fsdp.html">
        FSDP Notes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/get_start_xpu.html">
        Getting Started on Intel GPU
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/gradcheck.html">
        Gradcheck mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/hip.html">
        HIP (ROCm) semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/large_scale_deployments.html">
        Features for large-scale deployments
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/libtorch_stable_abi.html">
        LibTorch Stable ABI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/modules.html">
        Modules
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/mps.html">
        MPS backend
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/multiprocessing.html">
        Multiprocessing best practices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/numerical_accuracy.html">
        Numerical accuracy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/randomness.html">
        Reproducibility
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/serialization.html">
        Serialization semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/windows.html">
        Windows FAQ
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Language Bindings
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="cpp_index.html">
        C++
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/javadoc/">
        Javadoc
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="deploy.html">
        torch::deploy
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Python API
      </span>
     </p>
     <ul class="current">
      <li class="toctree-l1">
       <a class="reference internal" href="torch.html">
        torch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.html">
        torch.nn
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.functional.html">
        torch.nn.functional
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensors.html">
        torch.Tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensor_attributes.html">
        Tensor Attributes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensor_view.html">
        Tensor Views
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="amp.html">
        torch.amp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="autograd.html">
        torch.autograd
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="library.html">
        torch.library
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="accelerator.html">
        torch.accelerator
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cpu.html">
        torch.cpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cuda.html">
        torch.cuda
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html">
        Understanding CUDA Memory Usage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">
        Generating a Snapshot
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">
        Using the visualizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">
        Snapshot API Reference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mps.html">
        torch.mps
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="xpu.html">
        torch.xpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mtia.html">
        torch.mtia
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mtia.memory.html">
        torch.mtia.memory
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="meta.html">
        Meta device
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="backends.html">
        torch.backends
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="export.html">
        torch.export
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.html">
        torch.distributed
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.tensor.html">
        torch.distributed.tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.algorithms.join.html">
        torch.distributed.algorithms.join
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.elastic.html">
        torch.distributed.elastic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fsdp.html">
        torch.distributed.fsdp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.fsdp.fully_shard.html">
        torch.distributed.fsdp.fully_shard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.tensor.parallel.html">
        torch.distributed.tensor.parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.optim.html">
        torch.distributed.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.pipelining.html">
        torch.distributed.pipelining
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.checkpoint.html">
        torch.distributed.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributions.html">
        torch.distributions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch.compiler.html">
        torch.compiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fft.html">
        torch.fft
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="func.html">
        torch.func
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="futures.html">
        torch.futures
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fx.html">
        torch.fx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fx.experimental.html">
        torch.fx.experimental
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="hub.html">
        torch.hub
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="jit.html">
        torch.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="linalg.html">
        torch.linalg
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="monitor.html">
        torch.monitor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="signal.html">
        torch.signal
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="special.html">
        torch.special
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch.overrides.html">
        torch.overrides
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="package.html">
        torch.package
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="profiler.html">
        torch.profiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.init.html">
        torch.nn.init
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.attention.html">
        torch.nn.attention
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="onnx.html">
        torch.onnx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="optim.html">
        torch.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="complex_numbers.html">
        Complex Numbers
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="ddp_comm_hooks.html">
        DDP Communication Hooks
       </a>
      </li>
      <li class="toctree-l1 current">
       <a class="reference internal" href="quantization.html">
        Quantization
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="rpc.html">
        Distributed RPC Framework
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="random.html">
        torch.random
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="masked.html">
        torch.masked
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nested.html">
        torch.nested
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="size.html">
        torch.Size
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="sparse.html">
        torch.sparse
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="storage.html">
        torch.Storage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="testing.html">
        torch.testing
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="utils.html">
        torch.utils
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="benchmark_utils.html">
        torch.utils.benchmark
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="bottleneck.html">
        torch.utils.bottleneck
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="checkpoint.html">
        torch.utils.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cpp_extension.html">
        torch.utils.cpp_extension
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="data.html">
        torch.utils.data
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="deterministic.html">
        torch.utils.deterministic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="jit_utils.html">
        torch.utils.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="dlpack.html">
        torch.utils.dlpack
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mobile_optimizer.html">
        torch.utils.mobile_optimizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="model_zoo.html">
        torch.utils.model_zoo
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensorboard.html">
        torch.utils.tensorboard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="module_tracker.html">
        torch.utils.module_tracker
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="type_info.html">
        Type Info
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="named_tensor.html">
        Named Tensors
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="name_inference.html">
        Named Tensors operator coverage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="config_mod.html">
        torch.__config__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="future_mod.html">
        torch.__future__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="logging.html">
        torch._logging
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_environment_variables.html">
        Torch Environment Variables
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Libraries
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/audio/stable">
        torchaudio
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/data">
        TorchData
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/torchrec">
        TorchRec
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/serve">
        TorchServe
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/text/stable">
        torchtext
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/vision/stable">
        torchvision
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/xla/">
        PyTorch on XLA Devices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/ao">
        torchao
       </a>
      </li>
     </ul>
    </div>
   </div>
  </nav>
  <div class="pytorch-container">
   <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
    <div class="pytorch-breadcrumbs-wrapper">
     <div aria-label="breadcrumbs navigation" role="navigation">
      <ul class="pytorch-breadcrumbs">
       <li>
        <a href="index.html">
         Docs
        </a>
        &gt;
       </li>
       <li>
        <a href="quantization.html">
         Quantization
        </a>
        &gt;
       </li>
       <li>
        Quantization API Reference
       </li>
       <li class="pytorch-breadcrumbs-aside">
        <a href="_sources/quantization-support.rst.txt" rel="nofollow">
         <img src="_static/images/view-page-source-icon.svg"/>
        </a>
       </li>
      </ul>
     </div>
    </div>
    <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
     Shortcuts
    </div>
   </div>
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <!-- Google Tag Manager (noscript) -->
     <noscript>
      <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0">
      </iframe>
     </noscript>
     <!-- End Google Tag Manager (noscript) -->
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="quantization-api-reference">
         <h1>
          Quantization API Reference
          <a class="headerlink" href="#quantization-api-reference" title="Permalink to this heading">
           ¶
          </a>
         </h1>
         <div class="section" id="torch-ao-quantization">
          <h2>
           torch.ao.quantization
           <a class="headerlink" href="#torch-ao-quantization" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module contains Eager mode quantization APIs.
          </p>
          <div class="section" id="top-level-apis">
           <h3>
            Top level APIs
            <a class="headerlink" href="#top-level-apis" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <table class="autosummary longtable docutils colwidths-auto align-default">
            <tbody>
             <tr class="row-odd">
              <td>
               <p>
                <p id="torch.ao.quantization.quantize">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.quantize.html#torch.ao.quantization.quantize" title="torch.ao.quantization.quantize">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   quantize
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Quantize the input float model with post training static quantization.
               </p>
              </td>
             </tr>
             <tr class="row-even">
              <td>
               <p>
                <p id="torch.ao.quantization.quantize_dynamic">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.quantize_dynamic.html#torch.ao.quantization.quantize_dynamic" title="torch.ao.quantization.quantize_dynamic">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   quantize_dynamic
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Converts a float model to dynamic (i.e.
               </p>
              </td>
             </tr>
             <tr class="row-odd">
              <td>
               <p>
                <p id="torch.ao.quantization.quantize_qat">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.quantize_qat.html#torch.ao.quantization.quantize_qat" title="torch.ao.quantization.quantize_qat">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   quantize_qat
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Do quantization aware training and output a quantized model
               </p>
              </td>
             </tr>
             <tr class="row-even">
              <td>
               <p>
                <p id="torch.ao.quantization.prepare">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.prepare.html#torch.ao.quantization.prepare" title="torch.ao.quantization.prepare">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   prepare
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Prepares a copy of the model for quantization calibration or quantization-aware training.
               </p>
              </td>
             </tr>
             <tr class="row-odd">
              <td>
               <p>
                <p id="torch.ao.quantization.prepare_qat">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.prepare_qat.html#torch.ao.quantization.prepare_qat" title="torch.ao.quantization.prepare_qat">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   prepare_qat
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.
               </p>
              </td>
             </tr>
             <tr class="row-even">
              <td>
               <p>
                <p id="torch.ao.quantization.convert">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.convert.html#torch.ao.quantization.convert" title="torch.ao.quantization.convert">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   convert
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Converts submodules in input module to a different module according to
                <cite>
                 mapping
                </cite>
                by calling
                <cite>
                 from_float
                </cite>
                method on the target module class.
               </p>
              </td>
             </tr>
            </tbody>
           </table>
          </div>
          <div class="section" id="preparing-model-for-quantization">
           <h3>
            Preparing model for quantization
            <a class="headerlink" href="#preparing-model-for-quantization" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <table class="autosummary longtable docutils colwidths-auto align-default">
            <tbody>
             <tr class="row-odd">
              <td>
               <p>
                <a class="reference internal" href="generated/torch.ao.quantization.fuse_modules.fuse_modules.html#torch.ao.quantization.fuse_modules.fuse_modules" title="torch.ao.quantization.fuse_modules.fuse_modules">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   fuse_modules.fuse_modules
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Fuse a list of modules into a single module.
               </p>
              </td>
             </tr>
             <tr class="row-even">
              <td>
               <p>
                <p id="torch.ao.quantization.QuantStub">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.QuantStub.html#torch.ao.quantization.QuantStub" title="torch.ao.quantization.QuantStub">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   QuantStub
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Quantize stub module, before calibration, this is same as an observer, it will be swapped as
                <cite>
                 nnq.Quantize
                </cite>
                in
                <cite>
                 convert
                </cite>
                .
               </p>
              </td>
             </tr>
             <tr class="row-odd">
              <td>
               <p>
                <p id="torch.ao.quantization.DeQuantStub">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.DeQuantStub.html#torch.ao.quantization.DeQuantStub" title="torch.ao.quantization.DeQuantStub">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   DeQuantStub
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Dequantize stub module, before calibration, this is same as identity, this will be swapped as
                <cite>
                 nnq.DeQuantize
                </cite>
                in
                <cite>
                 convert
                </cite>
                .
               </p>
              </td>
             </tr>
             <tr class="row-even">
              <td>
               <p>
                <p id="torch.ao.quantization.QuantWrapper">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.QuantWrapper.html#torch.ao.quantization.QuantWrapper" title="torch.ao.quantization.QuantWrapper">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   QuantWrapper
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.
               </p>
              </td>
             </tr>
             <tr class="row-odd">
              <td>
               <p>
                <p id="torch.ao.quantization.add_quant_dequant">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.add_quant_dequant.html#torch.ao.quantization.add_quant_dequant" title="torch.ao.quantization.add_quant_dequant">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   add_quant_dequant
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.
               </p>
              </td>
             </tr>
            </tbody>
           </table>
          </div>
          <div class="section" id="utility-functions">
           <h3>
            Utility functions
            <a class="headerlink" href="#utility-functions" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <table class="autosummary longtable docutils colwidths-auto align-default">
            <tbody>
             <tr class="row-odd">
              <td>
               <p>
                <p id="torch.ao.quantization.swap_module">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.swap_module.html#torch.ao.quantization.swap_module" title="torch.ao.quantization.swap_module">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   swap_module
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Swaps the module if it has a quantized counterpart and it has an
                <cite>
                 observer
                </cite>
                attached.
               </p>
              </td>
             </tr>
             <tr class="row-even">
              <td>
               <p>
                <p id="torch.ao.quantization.propagate_qconfig_">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.propagate_qconfig_.html#torch.ao.quantization.propagate_qconfig_" title="torch.ao.quantization.propagate_qconfig_">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   propagate_qconfig_
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Propagate qconfig through the module hierarchy and assign
                <cite>
                 qconfig
                </cite>
                attribute on each leaf module
               </p>
              </td>
             </tr>
             <tr class="row-odd">
              <td>
               <p>
                <p id="torch.ao.quantization.default_eval_fn">
                </p>
                <a class="reference internal" href="generated/torch.ao.quantization.default_eval_fn.html#torch.ao.quantization.default_eval_fn" title="torch.ao.quantization.default_eval_fn">
                 <code class="xref py py-obj docutils literal notranslate">
                  <span class="pre">
                   default_eval_fn
                  </span>
                 </code>
                </a>
               </p>
              </td>
              <td>
               <p>
                Define the default evaluation function.
               </p>
              </td>
             </tr>
            </tbody>
           </table>
          </div>
         </div>
         <div class="section" id="torch-ao-quantization-quantize-fx">
          <h2>
           torch.ao.quantization.quantize_fx
           <a class="headerlink" href="#torch-ao-quantization-quantize-fx" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module contains FX graph mode quantization APIs (prototype).
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.quantize_fx.prepare_fx">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx" title="torch.ao.quantization.quantize_fx.prepare_fx">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  prepare_fx
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Prepare a model for post training quantization
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.quantize_fx.prepare_qat_fx">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx" title="torch.ao.quantization.quantize_fx.prepare_qat_fx">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  prepare_qat_fx
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Prepare a model for quantization aware training
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.quantize_fx.convert_fx">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx" title="torch.ao.quantization.quantize_fx.convert_fx">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  convert_fx
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Convert a calibrated or trained model to a quantized model
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.quantize_fx.fuse_fx">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx" title="torch.ao.quantization.quantize_fx.fuse_fx">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  fuse_fx
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-ao-quantization-qconfig-mapping">
          <h2>
           torch.ao.quantization.qconfig_mapping
           <a class="headerlink" href="#torch-ao-quantization-qconfig-mapping" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module contains QConfigMapping for configuring FX graph mode quantization.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig_mapping.QConfigMapping">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.QConfigMapping.html#torch.ao.quantization.qconfig_mapping.QConfigMapping" title="torch.ao.quantization.qconfig_mapping.QConfigMapping">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  QConfigMapping
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Mapping from model ops to
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 torch.ao.quantization.QConfig
                </span>
               </code>
               s.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping" title="torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_default_qconfig_mapping
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the default QConfigMapping for post training quantization.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping.html#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping" title="torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_default_qat_qconfig_mapping
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the default QConfigMapping for quantization aware training.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-ao-quantization-backend-config">
          <h2>
           torch.ao.quantization.backend_config
           <a class="headerlink" href="#torch-ao-quantization-backend-config" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module contains BackendConfig, a config object that defines how quantization is supported
in a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode
Quantization to work with this as well.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.backend_config.BackendConfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendConfig.html#torch.ao.quantization.backend_config.BackendConfig" title="torch.ao.quantization.backend_config.BackendConfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BackendConfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Config that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.backend_config.BackendPatternConfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.backend_config.BackendPatternConfig.html#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.BackendPatternConfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BackendPatternConfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Config object that specifies quantization behavior for a given operator pattern.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.backend_config.DTypeConfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  DTypeConfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Config object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.backend_config.DTypeWithConstraints">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeWithConstraints.html#torch.ao.quantization.backend_config.DTypeWithConstraints" title="torch.ao.quantization.backend_config.DTypeWithConstraints">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  DTypeWithConstraints
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Config for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in
               <a class="reference internal" href="generated/torch.ao.quantization.backend_config.DTypeConfig.html#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  DTypeConfig
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.backend_config.ObservationType">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.backend_config.ObservationType.html#torch.ao.quantization.backend_config.ObservationType" title="torch.ao.quantization.backend_config.ObservationType">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ObservationType
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               An enum that represents different ways of how an operator/operator pattern should be observed
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-ao-quantization-fx-custom-config">
          <h2>
           torch.ao.quantization.fx.custom_config
           <a class="headerlink" href="#torch-ao-quantization-fx-custom-config" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module contains a few CustomConfig classes that’s used in both eager mode and FX graph mode quantization
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fx.custom_config.FuseCustomConfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.FuseCustomConfig.html#torch.ao.quantization.fx.custom_config.FuseCustomConfig" title="torch.ao.quantization.fx.custom_config.FuseCustomConfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  FuseCustomConfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Custom configuration for
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.fuse_fx.html#torch.ao.quantization.quantize_fx.fuse_fx" title="torch.ao.quantization.quantize_fx.fuse_fx">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  fuse_fx()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.PrepareCustomConfig.html#torch.ao.quantization.fx.custom_config.PrepareCustomConfig" title="torch.ao.quantization.fx.custom_config.PrepareCustomConfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PrepareCustomConfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Custom configuration for
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_fx.html#torch.ao.quantization.quantize_fx.prepare_fx" title="torch.ao.quantization.quantize_fx.prepare_fx">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  prepare_fx()
                 </span>
                </code>
               </a>
               and
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.prepare_qat_fx.html#torch.ao.quantization.quantize_fx.prepare_qat_fx" title="torch.ao.quantization.quantize_fx.prepare_qat_fx">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  prepare_qat_fx()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fx.custom_config.ConvertCustomConfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.ConvertCustomConfig.html#torch.ao.quantization.fx.custom_config.ConvertCustomConfig" title="torch.ao.quantization.fx.custom_config.ConvertCustomConfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvertCustomConfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Custom configuration for
               <a class="reference internal" href="generated/torch.ao.quantization.quantize_fx.convert_fx.html#torch.ao.quantization.quantize_fx.convert_fx" title="torch.ao.quantization.quantize_fx.convert_fx">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  convert_fx()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry.html#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry" title="torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  StandaloneModuleConfigEntry
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.quantization.quantizer">
          <span id="torch-ao-quantization-quantizer">
          </span>
          <h2>
           torch.ao.quantization.quantizer
           <a class="headerlink" href="#module-torch.ao.quantization.quantizer" title="Permalink to this heading">
            ¶
           </a>
          </h2>
         </div>
         <div class="section" id="module-torch.ao.quantization.pt2e">
          <span id="torch-ao-quantization-pt2e-quantization-in-pytorch-2-0-export-implementation">
          </span>
          <h2>
           torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation)
           <a class="headerlink" href="#module-torch.ao.quantization.pt2e" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.quantization.pt2e.representation">
          </span>
         </div>
         <div class="section" id="torch-ao-quantization-pt2e-export-utils">
          <h2>
           torch.ao.quantization.pt2e.export_utils
           <a class="headerlink" href="#torch-ao-quantization-pt2e-export-utils" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.pt2e.export_utils.model_is_exported">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.pt2e.export_utils.model_is_exported.html#torch.ao.quantization.pt2e.export_utils.model_is_exported" title="torch.ao.quantization.pt2e.export_utils.model_is_exported">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  model_is_exported
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return True if the
               <cite>
                torch.nn.Module
               </cite>
               was exported, False otherwise (e.g.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="pt2-export-pt2e-numeric-debugger">
          <h2>
           PT2 Export (pt2e) Numeric Debugger
           <a class="headerlink" href="#pt2-export-pt2e-numeric-debugger" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.generate_numeric_debug_handle">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.generate_numeric_debug_handle.html#torch.ao.quantization.generate_numeric_debug_handle" title="torch.ao.quantization.generate_numeric_debug_handle">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  generate_numeric_debug_handle
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Attach numeric_debug_handle_id for all nodes in the graph module of the given ExportedProgram, like conv2d, squeeze, conv1d, etc, except for placeholder.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.CUSTOM_KEY">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.CUSTOM_KEY.html#torch.ao.quantization.CUSTOM_KEY" title="torch.ao.quantization.CUSTOM_KEY">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  CUSTOM_KEY
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY.html#torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY" title="torch.ao.quantization.NUMERIC_DEBUG_HANDLE_KEY">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  NUMERIC_DEBUG_HANDLE_KEY
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               str(object='') -&gt; str str(bytes_or_buffer[, encoding[, errors]]) -&gt; str
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.prepare_for_propagation_comparison">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.prepare_for_propagation_comparison.html#torch.ao.quantization.prepare_for_propagation_comparison" title="torch.ao.quantization.prepare_for_propagation_comparison">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  prepare_for_propagation_comparison
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Add output loggers to node that has numeric_debug_handle
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.extract_results_from_loggers">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.extract_results_from_loggers.html#torch.ao.quantization.extract_results_from_loggers" title="torch.ao.quantization.extract_results_from_loggers">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  extract_results_from_loggers
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               For a given model, extract the tensors stats and related information for each debug handle.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.compare_results">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.compare_results.html#torch.ao.quantization.compare_results" title="torch.ao.quantization.compare_results">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  compare_results
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given two dict mapping from
               <cite>
                debug_handle_id
               </cite>
               (int) to list of tensors return a map from
               <cite>
                debug_handle_id
               </cite>
               to
               <cite>
                NodeAccuracySummary
               </cite>
               that contains comparison information like SQNR, MSE etc.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-quantization-related-functions">
          <h2>
           torch (quantization related functions)
           <a class="headerlink" href="#torch-quantization-related-functions" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This describes the quantization related functions of the
           <cite>
            torch
           </cite>
           namespace.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.quantize_per_tensor">
               </p>
               <a class="reference internal" href="generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor" title="torch.quantize_per_tensor">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  quantize_per_tensor
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Converts a float tensor to a quantized tensor with given scale and zero point.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.quantize_per_channel">
               </p>
               <a class="reference internal" href="generated/torch.quantize_per_channel.html#torch.quantize_per_channel" title="torch.quantize_per_channel">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  quantize_per_channel
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Converts a float tensor to a per-channel quantized tensor with given scales and zero points.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.dequantize">
               </p>
               <a class="reference internal" href="generated/torch.dequantize.html#torch.dequantize" title="torch.dequantize">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  dequantize
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns an fp32 Tensor by dequantizing a quantized Tensor
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-tensor-quantization-related-methods">
          <h2>
           torch.Tensor (quantization related methods)
           <a class="headerlink" href="#torch-tensor-quantization-related-methods" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           Quantized Tensors support a limited subset of data manipulation methods of the
regular full-precision tensor.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.view">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.view.html#torch.Tensor.view" title="torch.Tensor.view">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  view
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns a new tensor with the same data as the
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 self
                </span>
               </code>
               tensor but of a different
               <a class="reference internal" href="generated/torch.Tensor.shape.html#torch.Tensor.shape" title="torch.Tensor.shape">
                <code class="xref py py-attr docutils literal notranslate">
                 <span class="pre">
                  shape
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.as_strided">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided" title="torch.Tensor.as_strided">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  as_strided
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.as_strided.html#torch.as_strided" title="torch.as_strided">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.as_strided()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.expand">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.expand.html#torch.Tensor.expand" title="torch.Tensor.expand">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  expand
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns a new view of the
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 self
                </span>
               </code>
               tensor with singleton dimensions expanded to a larger size.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.flatten">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.flatten.html#torch.Tensor.flatten" title="torch.Tensor.flatten">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  flatten
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.flatten.html#torch.flatten" title="torch.flatten">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.flatten()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.select">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.select.html#torch.Tensor.select" title="torch.Tensor.select">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  select
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.select.html#torch.select" title="torch.select">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.select()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.ne">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.ne.html#torch.Tensor.ne" title="torch.Tensor.ne">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ne
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.ne.html#torch.ne" title="torch.ne">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.ne()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.eq">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.eq.html#torch.Tensor.eq" title="torch.Tensor.eq">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  eq
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.eq.html#torch.eq" title="torch.eq">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.eq()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.ge">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.ge.html#torch.Tensor.ge" title="torch.Tensor.ge">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ge
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.ge.html#torch.ge" title="torch.ge">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.ge()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.le">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.le.html#torch.Tensor.le" title="torch.Tensor.le">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  le
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.le.html#torch.le" title="torch.le">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.le()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.gt">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.gt.html#torch.Tensor.gt" title="torch.Tensor.gt">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  gt
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.gt.html#torch.gt" title="torch.gt">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.gt()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.lt">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.lt.html#torch.Tensor.lt" title="torch.Tensor.lt">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  lt
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.lt.html#torch.lt" title="torch.lt">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.lt()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.copy_">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.copy_.html#torch.Tensor.copy_" title="torch.Tensor.copy_">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  copy_
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Copies the elements from
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 src
                </span>
               </code>
               into
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 self
                </span>
               </code>
               tensor and returns
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 self
                </span>
               </code>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.clone">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.clone.html#torch.Tensor.clone" title="torch.Tensor.clone">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  clone
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.clone.html#torch.clone" title="torch.clone">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.clone()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.dequantize">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize" title="torch.Tensor.dequantize">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  dequantize
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.equal">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.equal.html#torch.Tensor.equal" title="torch.Tensor.equal">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  equal
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.equal.html#torch.equal" title="torch.equal">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.equal()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.int_repr">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr" title="torch.Tensor.int_repr">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  int_repr
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given a quantized Tensor,
               <code class="docutils literal notranslate">
                <span class="pre">
                 self.int_repr()
                </span>
               </code>
               returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.max">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.max.html#torch.Tensor.max" title="torch.Tensor.max">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.max.html#torch.max" title="torch.max">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.max()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.mean">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.mean.html#torch.Tensor.mean" title="torch.Tensor.mean">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  mean
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.mean.html#torch.mean" title="torch.mean">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.mean()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.min">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.min.html#torch.Tensor.min" title="torch.Tensor.min">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  min
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.min.html#torch.min" title="torch.min">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.min()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.q_scale">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale" title="torch.Tensor.q_scale">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  q_scale
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.q_zero_point">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point" title="torch.Tensor.q_zero_point">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  q_zero_point
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.q_per_channel_scales">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales" title="torch.Tensor.q_per_channel_scales">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  q_per_channel_scales
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.q_per_channel_zero_points">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points" title="torch.Tensor.q_per_channel_zero_points">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  q_per_channel_zero_points
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.q_per_channel_axis">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis" title="torch.Tensor.q_per_channel_axis">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  q_per_channel_axis
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.resize_">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.resize_.html#torch.Tensor.resize_" title="torch.Tensor.resize_">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  resize_
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Resizes
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 self
                </span>
               </code>
               tensor to the specified size.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.Tensor.sort">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.sort.html#torch.Tensor.sort" title="torch.Tensor.sort">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  sort
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.sort.html#torch.sort" title="torch.sort">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.sort()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.Tensor.topk">
               </p>
               <a class="reference internal" href="generated/torch.Tensor.topk.html#torch.Tensor.topk" title="torch.Tensor.topk">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  topk
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               See
               <a class="reference internal" href="generated/torch.topk.html#torch.topk" title="torch.topk">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  torch.topk()
                 </span>
                </code>
               </a>
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-ao-quantization-observer">
          <h2>
           torch.ao.quantization.observer
           <a class="headerlink" href="#torch-ao-quantization-observer" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module contains observers which are used to collect statistics about
the values observed during calibration (PTQ) or training (QAT).
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.ObserverBase">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase" title="torch.ao.quantization.observer.ObserverBase">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ObserverBase
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Base observer Module.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.MinMaxObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver" title="torch.ao.quantization.observer.MinMaxObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  MinMaxObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Observer module for computing the quantization parameters based on the running min and max values.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.MovingAverageMinMaxObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAverageMinMaxObserver.html#torch.ao.quantization.observer.MovingAverageMinMaxObserver" title="torch.ao.quantization.observer.MovingAverageMinMaxObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  MovingAverageMinMaxObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Observer module for computing the quantization parameters based on the moving average of the min and max values.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PerChannelMinMaxObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PerChannelMinMaxObserver.html#torch.ao.quantization.observer.PerChannelMinMaxObserver" title="torch.ao.quantization.observer.PerChannelMinMaxObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PerChannelMinMaxObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Observer module for computing the quantization parameters based on the running per channel min and max values.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver.html#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver" title="torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  MovingAveragePerChannelMinMaxObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Observer module for computing the quantization parameters based on the running per channel min and max values.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.HistogramObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.HistogramObserver.html#torch.ao.quantization.observer.HistogramObserver" title="torch.ao.quantization.observer.HistogramObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  HistogramObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               The module records the running histogram of tensor values along with min/max values.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PlaceholderObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PlaceholderObserver.html#torch.ao.quantization.observer.PlaceholderObserver" title="torch.ao.quantization.observer.PlaceholderObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PlaceholderObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Observer that doesn't do anything and just passes its configuration to the quantized module's
               <code class="docutils literal notranslate">
                <span class="pre">
                 .from_float()
                </span>
               </code>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.RecordingObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.RecordingObserver.html#torch.ao.quantization.observer.RecordingObserver" title="torch.ao.quantization.observer.RecordingObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  RecordingObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               The module is mainly for debug and records the tensor values during runtime.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.NoopObserver">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.NoopObserver.html#torch.ao.quantization.observer.NoopObserver" title="torch.ao.quantization.observer.NoopObserver">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  NoopObserver
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Observer that doesn't do anything and just passes its configuration to the quantized module's
               <code class="docutils literal notranslate">
                <span class="pre">
                 .from_float()
                </span>
               </code>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.get_observer_state_dict">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.get_observer_state_dict.html#torch.ao.quantization.observer.get_observer_state_dict" title="torch.ao.quantization.observer.get_observer_state_dict">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_observer_state_dict
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Returns the state dict corresponding to the observer stats.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.load_observer_state_dict">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.load_observer_state_dict.html#torch.ao.quantization.observer.load_observer_state_dict" title="torch.ao.quantization.observer.load_observer_state_dict">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  load_observer_state_dict
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Given input model and a state_dict containing model observer stats, load the stats back into the model.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_observer.html#torch.ao.quantization.observer.default_observer" title="torch.ao.quantization.observer.default_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default observer for static quantization, usually used for debugging.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_placeholder_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_placeholder_observer.html#torch.ao.quantization.observer.default_placeholder_observer" title="torch.ao.quantization.observer.default_placeholder_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_placeholder_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default placeholder observer, usually used for quantization to torch.float16.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_debug_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_debug_observer.html#torch.ao.quantization.observer.default_debug_observer" title="torch.ao.quantization.observer.default_debug_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_debug_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default debug-only observer.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_weight_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_weight_observer.html#torch.ao.quantization.observer.default_weight_observer" title="torch.ao.quantization.observer.default_weight_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_weight_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default weight observer.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_histogram_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_histogram_observer.html#torch.ao.quantization.observer.default_histogram_observer" title="torch.ao.quantization.observer.default_histogram_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_histogram_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default histogram observer, usually used for PTQ.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_per_channel_weight_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_per_channel_weight_observer.html#torch.ao.quantization.observer.default_per_channel_weight_observer" title="torch.ao.quantization.observer.default_per_channel_weight_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_per_channel_weight_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such as
               <cite>
                fbgemm
               </cite>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_dynamic_quant_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_dynamic_quant_observer.html#torch.ao.quantization.observer.default_dynamic_quant_observer" title="torch.ao.quantization.observer.default_dynamic_quant_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_dynamic_quant_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default observer for dynamic quantization.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.default_float_qparams_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.default_float_qparams_observer.html#torch.ao.quantization.observer.default_float_qparams_observer" title="torch.ao.quantization.observer.default_float_qparams_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_float_qparams_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default observer for a floating point zero-point.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.AffineQuantizedObserverBase">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.AffineQuantizedObserverBase.html#torch.ao.quantization.observer.AffineQuantizedObserverBase" title="torch.ao.quantization.observer.AffineQuantizedObserverBase">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  AffineQuantizedObserverBase
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Observer module for affine quantization (
               <a class="reference external" href="https://github.com/pytorch/ao/tree/main/torchao/quantization#affine-quantization">
                https://github.com/pytorch/ao/tree/main/torchao/quantization#affine-quantization
               </a>
               )
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.Granularity">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.Granularity.html#torch.ao.quantization.observer.Granularity" title="torch.ao.quantization.observer.Granularity">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Granularity
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Base class for representing the granularity of quantization.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.MappingType">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.MappingType.html#torch.ao.quantization.observer.MappingType" title="torch.ao.quantization.observer.MappingType">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  MappingType
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               How floating point number is mapped to integer number
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PerAxis">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PerAxis.html#torch.ao.quantization.observer.PerAxis" title="torch.ao.quantization.observer.PerAxis">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PerAxis
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Represents per-axis granularity in quantization.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PerBlock">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PerBlock.html#torch.ao.quantization.observer.PerBlock" title="torch.ao.quantization.observer.PerBlock">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PerBlock
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Represents per-block granularity in quantization.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PerGroup">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PerGroup.html#torch.ao.quantization.observer.PerGroup" title="torch.ao.quantization.observer.PerGroup">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PerGroup
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Represents per-channel group granularity in quantization.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PerRow">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PerRow.html#torch.ao.quantization.observer.PerRow" title="torch.ao.quantization.observer.PerRow">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PerRow
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Represents row-wise granularity in quantization.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PerTensor">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PerTensor.html#torch.ao.quantization.observer.PerTensor" title="torch.ao.quantization.observer.PerTensor">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PerTensor
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Represents per-tensor granularity in quantization.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.PerToken">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.PerToken.html#torch.ao.quantization.observer.PerToken" title="torch.ao.quantization.observer.PerToken">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  PerToken
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Represents per-token granularity in quantization.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.TorchAODType">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.TorchAODType.html#torch.ao.quantization.observer.TorchAODType" title="torch.ao.quantization.observer.TorchAODType">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  TorchAODType
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Placeholder for dtypes that do not exist in PyTorch core yet.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.ZeroPointDomain">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.ZeroPointDomain.html#torch.ao.quantization.observer.ZeroPointDomain" title="torch.ao.quantization.observer.ZeroPointDomain">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ZeroPointDomain
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Enum that indicate whether zero_point is in integer domain or floating point domain
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.observer.get_block_size">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.observer.get_block_size.html#torch.ao.quantization.observer.get_block_size" title="torch.ao.quantization.observer.get_block_size">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_block_size
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Get the block size based on the input shape and granularity type.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-ao-quantization-fake-quantize">
          <h2>
           torch.ao.quantization.fake_quantize
           <a class="headerlink" href="#torch-ao-quantization-fake-quantize" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module implements modules which are used to perform fake quantization
during QAT.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.FakeQuantizeBase">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantizeBase.html#torch.ao.quantization.fake_quantize.FakeQuantizeBase" title="torch.ao.quantization.fake_quantize.FakeQuantizeBase">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  FakeQuantizeBase
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Base fake quantize module.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.FakeQuantize">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FakeQuantize.html#torch.ao.quantization.fake_quantize.FakeQuantize" title="torch.ao.quantization.fake_quantize.FakeQuantize">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  FakeQuantize
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Simulate the quantize and dequantize operations in training time.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize.html#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize" title="torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  FixedQParamsFakeQuantize
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Simulate quantize and dequantize in training time.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize.html#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize" title="torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  FusedMovingAvgObsFakeQuantize
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Define a fused module to observe the tensor.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.default_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fake_quant.html#torch.ao.quantization.fake_quantize.default_fake_quant" title="torch.ao.quantization.fake_quantize.default_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default fake_quant for activations.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.default_weight_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_weight_fake_quant" title="torch.ao.quantization.fake_quantize.default_weight_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_weight_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default fake_quant for weights.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant.html#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant" title="torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_per_channel_weight_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default fake_quant for per-channel weights.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.default_histogram_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant.html#torch.ao.quantization.fake_quantize.default_histogram_fake_quant" title="torch.ao.quantization.fake_quantize.default_histogram_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_histogram_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Fake_quant for activations using a histogram..
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.default_fused_act_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_act_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_fused_act_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Fused version of
               <cite>
                default_fake_quant
               </cite>
               , with improved performance.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_fused_wt_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Fused version of
               <cite>
                default_weight_fake_quant
               </cite>
               , with improved performance.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant.html#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant" title="torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_fused_per_channel_wt_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Fused version of
               <cite>
                default_per_channel_weight_fake_quant
               </cite>
               , with improved performance.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.disable_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_fake_quant.html#torch.ao.quantization.fake_quantize.disable_fake_quant" title="torch.ao.quantization.fake_quantize.disable_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  disable_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Disable fake quantization for the module.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.enable_fake_quant">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_fake_quant.html#torch.ao.quantization.fake_quantize.enable_fake_quant" title="torch.ao.quantization.fake_quantize.enable_fake_quant">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  enable_fake_quant
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Enable fake quantization for the module.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.disable_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.disable_observer.html#torch.ao.quantization.fake_quantize.disable_observer" title="torch.ao.quantization.fake_quantize.disable_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  disable_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Disable observation for this module.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.fake_quantize.enable_observer">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.fake_quantize.enable_observer.html#torch.ao.quantization.fake_quantize.enable_observer" title="torch.ao.quantization.fake_quantize.enable_observer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  enable_observer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Enable observation for this module.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-ao-quantization-qconfig">
          <h2>
           torch.ao.quantization.qconfig
           <a class="headerlink" href="#torch-ao-quantization-qconfig" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module defines
           <cite>
            QConfig
           </cite>
           objects which are used
to configure quantization settings for individual ops.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.QConfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig" title="torch.ao.quantization.qconfig.QConfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  QConfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qconfig.html#torch.ao.quantization.qconfig.default_qconfig" title="torch.ao.quantization.qconfig.default_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default qconfig configuration.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_debug_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_debug_qconfig.html#torch.ao.quantization.qconfig.default_debug_qconfig" title="torch.ao.quantization.qconfig.default_debug_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_debug_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default qconfig configuration for debugging.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_per_channel_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_per_channel_qconfig.html#torch.ao.quantization.qconfig.default_per_channel_qconfig" title="torch.ao.quantization.qconfig.default_per_channel_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_per_channel_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default qconfig configuration for per channel weight quantization.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_dynamic_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_dynamic_qconfig.html#torch.ao.quantization.qconfig.default_dynamic_qconfig" title="torch.ao.quantization.qconfig.default_dynamic_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_dynamic_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default dynamic qconfig.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.float16_dynamic_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig.html#torch.ao.quantization.qconfig.float16_dynamic_qconfig" title="torch.ao.quantization.qconfig.float16_dynamic_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  float16_dynamic_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Dynamic qconfig with weights quantized to
               <cite>
                torch.float16
               </cite>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.float16_static_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.float16_static_qconfig.html#torch.ao.quantization.qconfig.float16_static_qconfig" title="torch.ao.quantization.qconfig.float16_static_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  float16_static_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Dynamic qconfig with both activations and weights quantized to
               <cite>
                torch.float16
               </cite>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.per_channel_dynamic_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig.html#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig" title="torch.ao.quantization.qconfig.per_channel_dynamic_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  per_channel_dynamic_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Dynamic qconfig with weights quantized per channel.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig.html#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig" title="torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  float_qparams_weight_only_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Dynamic qconfig with weights quantized with a floating point zero_point.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_qat_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig.html#torch.ao.quantization.qconfig.default_qat_qconfig" title="torch.ao.quantization.qconfig.default_qat_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_qat_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default qconfig for QAT.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_weight_only_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_weight_only_qconfig.html#torch.ao.quantization.qconfig.default_weight_only_qconfig" title="torch.ao.quantization.qconfig.default_weight_only_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_weight_only_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default qconfig for quantizing weights only.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_activation_only_qconfig">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_activation_only_qconfig.html#torch.ao.quantization.qconfig.default_activation_only_qconfig" title="torch.ao.quantization.qconfig.default_activation_only_qconfig">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_activation_only_qconfig
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Default qconfig for quantizing activations only.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.quantization.qconfig.default_qat_qconfig_v2">
               </p>
               <a class="reference internal" href="generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2.html#torch.ao.quantization.qconfig.default_qat_qconfig_v2" title="torch.ao.quantization.qconfig.default_qat_qconfig_v2">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  default_qat_qconfig_v2
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Fused version of
               <cite>
                default_qat_config
               </cite>
               , has performance benefits.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.intrinsic">
          <span id="torch-ao-nn-intrinsic">
          </span>
          <h2>
           torch.ao.nn.intrinsic
           <a class="headerlink" href="#module-torch.ao.nn.intrinsic" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.nn.intrinsic.modules">
          </span>
          <p>
           This module implements the combined (fused) modules conv + relu which can
then be quantized.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvReLU1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU1d.html#torch.ao.nn.intrinsic.ConvReLU1d" title="torch.ao.nn.intrinsic.ConvReLU1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv1d and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvReLU2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU2d.html#torch.ao.nn.intrinsic.ConvReLU2d" title="torch.ao.nn.intrinsic.ConvReLU2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv2d and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvReLU3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvReLU3d.html#torch.ao.nn.intrinsic.ConvReLU3d" title="torch.ao.nn.intrinsic.ConvReLU3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv3d and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.LinearReLU">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.LinearReLU.html#torch.ao.nn.intrinsic.LinearReLU" title="torch.ao.nn.intrinsic.LinearReLU">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LinearReLU
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Linear and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvBn1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn1d.html#torch.ao.nn.intrinsic.ConvBn1d" title="torch.ao.nn.intrinsic.ConvBn1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBn1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv 1d and Batch Norm 1d modules.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvBn2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn2d.html#torch.ao.nn.intrinsic.ConvBn2d" title="torch.ao.nn.intrinsic.ConvBn2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBn2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv 2d and Batch Norm 2d modules.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvBn3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBn3d.html#torch.ao.nn.intrinsic.ConvBn3d" title="torch.ao.nn.intrinsic.ConvBn3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBn3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv 3d and Batch Norm 3d modules.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvBnReLU1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU1d.html#torch.ao.nn.intrinsic.ConvBnReLU1d" title="torch.ao.nn.intrinsic.ConvBnReLU1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBnReLU1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvBnReLU2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU2d.html#torch.ao.nn.intrinsic.ConvBnReLU2d" title="torch.ao.nn.intrinsic.ConvBnReLU2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBnReLU2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.ConvBnReLU3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.ConvBnReLU3d.html#torch.ao.nn.intrinsic.ConvBnReLU3d" title="torch.ao.nn.intrinsic.ConvBnReLU3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBnReLU3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.BNReLU2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU2d.html#torch.ao.nn.intrinsic.BNReLU2d" title="torch.ao.nn.intrinsic.BNReLU2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BNReLU2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the BatchNorm 2d and ReLU modules.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.BNReLU3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.BNReLU3d.html#torch.ao.nn.intrinsic.BNReLU3d" title="torch.ao.nn.intrinsic.BNReLU3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BNReLU3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is a sequential container which calls the BatchNorm 3d and ReLU modules.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.intrinsic.qat">
          <span id="torch-ao-nn-intrinsic-qat">
          </span>
          <h2>
           torch.ao.nn.intrinsic.qat
           <a class="headerlink" href="#module-torch.ao.nn.intrinsic.qat" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.nn.intrinsic.qat.modules">
          </span>
          <p>
           This module implements the versions of those fused operations needed for
quantization aware training.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.LinearReLU">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.LinearReLU.html#torch.ao.nn.intrinsic.qat.LinearReLU" title="torch.ao.nn.intrinsic.qat.LinearReLU">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LinearReLU
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvBn1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn1d.html#torch.ao.nn.intrinsic.qat.ConvBn1d" title="torch.ao.nn.intrinsic.qat.ConvBn1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBn1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvBnReLU1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU1d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU1d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBnReLU1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvBn2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn2d.html#torch.ao.nn.intrinsic.qat.ConvBn2d" title="torch.ao.nn.intrinsic.qat.ConvBn2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBn2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvBnReLU2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU2d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU2d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBnReLU2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvReLU2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU2d.html#torch.ao.nn.intrinsic.qat.ConvReLU2d" title="torch.ao.nn.intrinsic.qat.ConvReLU2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvBn3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBn3d.html#torch.ao.nn.intrinsic.qat.ConvBn3d" title="torch.ao.nn.intrinsic.qat.ConvBn3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBn3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvBnReLU3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvBnReLU3d.html#torch.ao.nn.intrinsic.qat.ConvBnReLU3d" title="torch.ao.nn.intrinsic.qat.ConvBnReLU3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvBnReLU3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.ConvReLU3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.ConvReLU3d.html#torch.ao.nn.intrinsic.qat.ConvReLU3d" title="torch.ao.nn.intrinsic.qat.ConvReLU3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.update_bn_stats">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.update_bn_stats.html#torch.ao.nn.intrinsic.qat.update_bn_stats" title="torch.ao.nn.intrinsic.qat.update_bn_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  update_bn_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.qat.freeze_bn_stats">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats.html#torch.ao.nn.intrinsic.qat.freeze_bn_stats" title="torch.ao.nn.intrinsic.qat.freeze_bn_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  freeze_bn_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.intrinsic.quantized">
          <span id="torch-ao-nn-intrinsic-quantized">
          </span>
          <h2>
           torch.ao.nn.intrinsic.quantized
           <a class="headerlink" href="#module-torch.ao.nn.intrinsic.quantized" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.nn.intrinsic.quantized.modules">
          </span>
          <p>
           This module implements the quantized implementations of fused operations
like conv + relu. No BatchNorm variants as it’s usually folded into convolution
for inference.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.quantized.BNReLU2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU2d.html#torch.ao.nn.intrinsic.quantized.BNReLU2d" title="torch.ao.nn.intrinsic.quantized.BNReLU2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BNReLU2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A BNReLU2d module is a fused module of BatchNorm2d and ReLU
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.quantized.BNReLU3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.BNReLU3d.html#torch.ao.nn.intrinsic.quantized.BNReLU3d" title="torch.ao.nn.intrinsic.quantized.BNReLU3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BNReLU3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A BNReLU3d module is a fused module of BatchNorm3d and ReLU
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.quantized.ConvReLU1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU1d.html#torch.ao.nn.intrinsic.quantized.ConvReLU1d" title="torch.ao.nn.intrinsic.quantized.ConvReLU1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvReLU1d module is a fused module of Conv1d and ReLU
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.quantized.ConvReLU2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU2d.html#torch.ao.nn.intrinsic.quantized.ConvReLU2d" title="torch.ao.nn.intrinsic.quantized.ConvReLU2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvReLU2d module is a fused module of Conv2d and ReLU
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.quantized.ConvReLU3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.ConvReLU3d.html#torch.ao.nn.intrinsic.quantized.ConvReLU3d" title="torch.ao.nn.intrinsic.quantized.ConvReLU3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvReLU3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A ConvReLU3d module is a fused module of Conv3d and ReLU
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.quantized.LinearReLU">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.LinearReLU.html#torch.ao.nn.intrinsic.quantized.LinearReLU" title="torch.ao.nn.intrinsic.quantized.LinearReLU">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LinearReLU
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A LinearReLU module fused from Linear and ReLU modules
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.intrinsic.quantized.dynamic">
          <span id="torch-ao-nn-intrinsic-quantized-dynamic">
          </span>
          <h2>
           torch.ao.nn.intrinsic.quantized.dynamic
           <a class="headerlink" href="#module-torch.ao.nn.intrinsic.quantized.dynamic" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.nn.intrinsic.quantized.dynamic.modules">
          </span>
          <p>
           This module implements the quantized dynamic implementations of fused operations
like linear + relu.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU.html#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU" title="torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LinearReLU
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.qat">
          <span id="torch-ao-nn-qat">
          </span>
          <h2>
           torch.ao.nn.qat
           <a class="headerlink" href="#module-torch.ao.nn.qat" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.nn.qat.modules">
          </span>
          <p>
           This module implements versions of the key nn modules
           <strong>
            Conv2d()
           </strong>
           and
           <strong>
            Linear()
           </strong>
           which run in FP32 but with rounding applied to simulate the
effect of INT8 quantization.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.qat.Conv2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.qat.Conv2d.html#torch.ao.nn.qat.Conv2d" title="torch.ao.nn.qat.Conv2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Conv2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.qat.Conv3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.qat.Conv3d.html#torch.ao.nn.qat.Conv3d" title="torch.ao.nn.qat.Conv3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Conv3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.qat.Linear">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.qat.Linear.html#torch.ao.nn.qat.Linear" title="torch.ao.nn.qat.Linear">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Linear
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A linear module attached with FakeQuantize modules for weight, used for quantization aware training.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.qat.dynamic">
          <span id="torch-ao-nn-qat-dynamic">
          </span>
          <h2>
           torch.ao.nn.qat.dynamic
           <a class="headerlink" href="#module-torch.ao.nn.qat.dynamic" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.nn.qat.dynamic.modules">
          </span>
          <p>
           This module implements versions of the key nn modules such as
           <strong>
            Linear()
           </strong>
           which run in FP32 but with rounding applied to simulate the effect of INT8
quantization and will be dynamically quantized during inference.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.qat.dynamic.Linear">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.qat.dynamic.Linear.html#torch.ao.nn.qat.dynamic.Linear" title="torch.ao.nn.qat.dynamic.Linear">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Linear
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.quantized.modules">
          <span id="torch-ao-nn-quantized">
          </span>
          <h2>
           torch.ao.nn.quantized
           <a class="headerlink" href="#module-torch.ao.nn.quantized.modules" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module implements the quantized versions of the nn layers such as
~`torch.nn.Conv2d` and
           <cite>
            torch.nn.ReLU
           </cite>
           .
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.ReLU6">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.ReLU6.html#torch.ao.nn.quantized.ReLU6" title="torch.ao.nn.quantized.ReLU6">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ReLU6
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies the element-wise function:
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.Hardswish">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.Hardswish.html#torch.ao.nn.quantized.Hardswish" title="torch.ao.nn.quantized.Hardswish">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Hardswish
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.Hardswish.html#torch.nn.Hardswish" title="torch.nn.Hardswish">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  Hardswish
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.ELU">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.ELU.html#torch.ao.nn.quantized.ELU" title="torch.ao.nn.quantized.ELU">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ELU
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized equivalent of
               <a class="reference internal" href="generated/torch.nn.ELU.html#torch.nn.ELU" title="torch.nn.ELU">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  ELU
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.LeakyReLU">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.LeakyReLU.html#torch.ao.nn.quantized.LeakyReLU" title="torch.ao.nn.quantized.LeakyReLU">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LeakyReLU
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized equivalent of
               <a class="reference internal" href="generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU" title="torch.nn.LeakyReLU">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  LeakyReLU
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.Sigmoid">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.Sigmoid.html#torch.ao.nn.quantized.Sigmoid" title="torch.ao.nn.quantized.Sigmoid">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Sigmoid
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized equivalent of
               <a class="reference internal" href="generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid" title="torch.nn.Sigmoid">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  Sigmoid
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.BatchNorm2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm2d.html#torch.ao.nn.quantized.BatchNorm2d" title="torch.ao.nn.quantized.BatchNorm2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BatchNorm2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  BatchNorm2d
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.BatchNorm3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.BatchNorm3d.html#torch.ao.nn.quantized.BatchNorm3d" title="torch.ao.nn.quantized.BatchNorm3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  BatchNorm3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  BatchNorm3d
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.Conv1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.Conv1d.html#torch.ao.nn.quantized.Conv1d" title="torch.ao.nn.quantized.Conv1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Conv1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 1D convolution over a quantized input signal composed of several quantized input planes.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.Conv2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.Conv2d.html#torch.ao.nn.quantized.Conv2d" title="torch.ao.nn.quantized.Conv2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Conv2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 2D convolution over a quantized input signal composed of several quantized input planes.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.Conv3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.Conv3d.html#torch.ao.nn.quantized.Conv3d" title="torch.ao.nn.quantized.Conv3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Conv3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 3D convolution over a quantized input signal composed of several quantized input planes.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.ConvTranspose1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose1d.html#torch.ao.nn.quantized.ConvTranspose1d" title="torch.ao.nn.quantized.ConvTranspose1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvTranspose1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 1D transposed convolution operator over an input image composed of several input planes.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.ConvTranspose2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose2d.html#torch.ao.nn.quantized.ConvTranspose2d" title="torch.ao.nn.quantized.ConvTranspose2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvTranspose2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 2D transposed convolution operator over an input image composed of several input planes.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.ConvTranspose3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.ConvTranspose3d.html#torch.ao.nn.quantized.ConvTranspose3d" title="torch.ao.nn.quantized.ConvTranspose3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ConvTranspose3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 3D transposed convolution operator over an input image composed of several input planes.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.Embedding">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.Embedding.html#torch.ao.nn.quantized.Embedding" title="torch.ao.nn.quantized.Embedding">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Embedding
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A quantized Embedding module with quantized packed weights as inputs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.EmbeddingBag">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.EmbeddingBag.html#torch.ao.nn.quantized.EmbeddingBag" title="torch.ao.nn.quantized.EmbeddingBag">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  EmbeddingBag
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A quantized EmbeddingBag module with quantized packed weights as inputs.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.FloatFunctional">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.FloatFunctional.html#torch.ao.nn.quantized.FloatFunctional" title="torch.ao.nn.quantized.FloatFunctional">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  FloatFunctional
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               State collector class for float operations.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.FXFloatFunctional">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.FXFloatFunctional.html#torch.ao.nn.quantized.FXFloatFunctional" title="torch.ao.nn.quantized.FXFloatFunctional">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  FXFloatFunctional
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.QFunctional">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.QFunctional.html#torch.ao.nn.quantized.QFunctional" title="torch.ao.nn.quantized.QFunctional">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  QFunctional
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper class for quantized operations.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.Linear">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.Linear.html#torch.ao.nn.quantized.Linear" title="torch.ao.nn.quantized.Linear">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Linear
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A quantized linear module with quantized tensor as inputs and outputs.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.LayerNorm">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.LayerNorm.html#torch.ao.nn.quantized.LayerNorm" title="torch.ao.nn.quantized.LayerNorm">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LayerNorm
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm" title="torch.nn.LayerNorm">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  LayerNorm
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.GroupNorm">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.GroupNorm.html#torch.ao.nn.quantized.GroupNorm" title="torch.ao.nn.quantized.GroupNorm">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  GroupNorm
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm" title="torch.nn.GroupNorm">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  GroupNorm
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.InstanceNorm1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm1d.html#torch.ao.nn.quantized.InstanceNorm1d" title="torch.ao.nn.quantized.InstanceNorm1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  InstanceNorm1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  InstanceNorm1d
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.InstanceNorm2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm2d.html#torch.ao.nn.quantized.InstanceNorm2d" title="torch.ao.nn.quantized.InstanceNorm2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  InstanceNorm2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  InstanceNorm2d
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.InstanceNorm3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.InstanceNorm3d.html#torch.ao.nn.quantized.InstanceNorm3d" title="torch.ao.nn.quantized.InstanceNorm3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  InstanceNorm3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  InstanceNorm3d
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.quantized.functional">
          <span id="torch-ao-nn-quantized-functional">
          </span>
          <h2>
           torch.ao.nn.quantized.functional
           <a class="headerlink" href="#module-torch.ao.nn.quantized.functional" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           Functional interface (quantized).
          </p>
          <p>
           This module implements the quantized versions of the functional layers such as
~`torch.nn.functional.conv2d` and
           <cite>
            torch.nn.functional.relu
           </cite>
           . Note:
           <a class="reference internal" href="generated/torch.nn.functional.relu.html#torch.nn.functional.relu" title="torch.nn.functional.relu">
            <code class="xref py py-meth docutils literal notranslate">
             <span class="pre">
              relu()
             </span>
            </code>
           </a>
           supports quantized inputs.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.avg_pool2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool2d.html#torch.ao.nn.quantized.functional.avg_pool2d" title="torch.ao.nn.quantized.functional.avg_pool2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  avg_pool2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies 2D average-pooling operation in
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <mi>
                      k
                     </mi>
                     <mi>
                      H
                     </mi>
                     <mo>
                      ×
                     </mo>
                     <mi>
                      k
                     </mi>
                     <mi>
                      W
                     </mi>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     kH \times kW
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.7778em;vertical-align:-0.0833em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.03148em;">
                    k
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.08125em;">
                    H
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                   <span class="mbin">
                    ×
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                  </span>
                  <span class="base">
                   <span class="strut" style="height:0.6944em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.13889em;">
                    kW
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               regions by step size
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <mi>
                      s
                     </mi>
                     <mi>
                      H
                     </mi>
                     <mo>
                      ×
                     </mo>
                     <mi>
                      s
                     </mi>
                     <mi>
                      W
                     </mi>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     sH \times sW
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.7667em;vertical-align:-0.0833em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.08125em;">
                    sH
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                   <span class="mbin">
                    ×
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                  </span>
                  <span class="base">
                   <span class="strut" style="height:0.6833em;">
                   </span>
                   <span class="mord mathnormal">
                    s
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.13889em;">
                    W
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               steps.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.avg_pool3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.avg_pool3d.html#torch.ao.nn.quantized.functional.avg_pool3d" title="torch.ao.nn.quantized.functional.avg_pool3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  avg_pool3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies 3D average-pooling operation in
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <mi>
                      k
                     </mi>
                     <mi>
                      D
                     </mi>
                     <mtext>
                     </mtext>
                     <mi>
                      t
                     </mi>
                     <mi>
                      i
                     </mi>
                     <mi>
                      m
                     </mi>
                     <mi>
                      e
                     </mi>
                     <mi>
                      s
                     </mi>
                     <mi>
                      k
                     </mi>
                     <mi>
                      H
                     </mi>
                     <mo>
                      ×
                     </mo>
                     <mi>
                      k
                     </mi>
                     <mi>
                      W
                     </mi>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     kD \ times kH \times kW
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.7778em;vertical-align:-0.0833em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.03148em;">
                    k
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.02778em;">
                    D
                   </span>
                   <span class="mspace">
                   </span>
                   <span class="mord mathnormal">
                    t
                   </span>
                   <span class="mord mathnormal">
                    im
                   </span>
                   <span class="mord mathnormal">
                    es
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.03148em;">
                    k
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.08125em;">
                    H
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                   <span class="mbin">
                    ×
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                  </span>
                  <span class="base">
                   <span class="strut" style="height:0.6944em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.13889em;">
                    kW
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               regions by step size
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <mi>
                      s
                     </mi>
                     <mi>
                      D
                     </mi>
                     <mo>
                      ×
                     </mo>
                     <mi>
                      s
                     </mi>
                     <mi>
                      H
                     </mi>
                     <mo>
                      ×
                     </mo>
                     <mi>
                      s
                     </mi>
                     <mi>
                      W
                     </mi>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     sD \times sH \times sW
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.7667em;vertical-align:-0.0833em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.02778em;">
                    sD
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                   <span class="mbin">
                    ×
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                  </span>
                  <span class="base">
                   <span class="strut" style="height:0.7667em;vertical-align:-0.0833em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.08125em;">
                    sH
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                   <span class="mbin">
                    ×
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                  </span>
                  <span class="base">
                   <span class="strut" style="height:0.6833em;">
                   </span>
                   <span class="mord mathnormal">
                    s
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.13889em;">
                    W
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               steps.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.adaptive_avg_pool2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool2d" title="torch.ao.nn.quantized.functional.adaptive_avg_pool2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  adaptive_avg_pool2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.adaptive_avg_pool3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d.html#torch.ao.nn.quantized.functional.adaptive_avg_pool3d" title="torch.ao.nn.quantized.functional.adaptive_avg_pool3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  adaptive_avg_pool3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.conv1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv1d.html#torch.ao.nn.quantized.functional.conv1d" title="torch.ao.nn.quantized.functional.conv1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  conv1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 1D convolution over a quantized 1D input composed of several input planes.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.conv2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv2d.html#torch.ao.nn.quantized.functional.conv2d" title="torch.ao.nn.quantized.functional.conv2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  conv2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 2D convolution over a quantized 2D input composed of several input planes.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.conv3d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.conv3d.html#torch.ao.nn.quantized.functional.conv3d" title="torch.ao.nn.quantized.functional.conv3d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  conv3d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 3D convolution over a quantized 3D input composed of several input planes.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.interpolate">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.interpolate.html#torch.ao.nn.quantized.functional.interpolate" title="torch.ao.nn.quantized.functional.interpolate">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  interpolate
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Down/up samples the input to either the given
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 size
                </span>
               </code>
               or the given
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 scale_factor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.linear">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.linear.html#torch.ao.nn.quantized.functional.linear" title="torch.ao.nn.quantized.functional.linear">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  linear
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a linear transformation to the incoming quantized data:
               <span class="math">
                <span class="katex">
                 <span class="katex-mathml">
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                   <semantics>
                    <mrow>
                     <mi>
                      y
                     </mi>
                     <mo>
                      =
                     </mo>
                     <mi>
                      x
                     </mi>
                     <msup>
                      <mi>
                       A
                      </mi>
                      <mi>
                       T
                      </mi>
                     </msup>
                     <mo>
                      +
                     </mo>
                     <mi>
                      b
                     </mi>
                    </mrow>
                    <annotation encoding="application/x-tex">
                     y = xA^T + b
                    </annotation>
                   </semantics>
                  </math>
                 </span>
                 <span aria-hidden="true" class="katex-html">
                  <span class="base">
                   <span class="strut" style="height:0.625em;vertical-align:-0.1944em;">
                   </span>
                   <span class="mord mathnormal" style="margin-right:0.03588em;">
                    y
                   </span>
                   <span class="mspace" style="margin-right:0.2778em;">
                   </span>
                   <span class="mrel">
                    =
                   </span>
                   <span class="mspace" style="margin-right:0.2778em;">
                   </span>
                  </span>
                  <span class="base">
                   <span class="strut" style="height:0.9247em;vertical-align:-0.0833em;">
                   </span>
                   <span class="mord mathnormal">
                    x
                   </span>
                   <span class="mord">
                    <span class="mord mathnormal">
                     A
                    </span>
                    <span class="msupsub">
                     <span class="vlist-t">
                      <span class="vlist-r">
                       <span class="vlist" style="height:0.8413em;">
                        <span style="top:-3.063em;margin-right:0.05em;">
                         <span class="pstrut" style="height:2.7em;">
                         </span>
                         <span class="sizing reset-size6 size3 mtight">
                          <span class="mord mathnormal mtight" style="margin-right:0.13889em;">
                           T
                          </span>
                         </span>
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                   <span class="mbin">
                    +
                   </span>
                   <span class="mspace" style="margin-right:0.2222em;">
                   </span>
                  </span>
                  <span class="base">
                   <span class="strut" style="height:0.6944em;">
                   </span>
                   <span class="mord mathnormal">
                    b
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.max_pool1d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool1d.html#torch.ao.nn.quantized.functional.max_pool1d" title="torch.ao.nn.quantized.functional.max_pool1d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_pool1d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 1D max pooling over a quantized input signal composed of several quantized input planes.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.max_pool2d">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.max_pool2d.html#torch.ao.nn.quantized.functional.max_pool2d" title="torch.ao.nn.quantized.functional.max_pool2d">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_pool2d
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.celu">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.celu.html#torch.ao.nn.quantized.functional.celu" title="torch.ao.nn.quantized.functional.celu">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  celu
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies the quantized CELU function element-wise.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.leaky_relu">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.leaky_relu.html#torch.ao.nn.quantized.functional.leaky_relu" title="torch.ao.nn.quantized.functional.leaky_relu">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  leaky_relu
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Quantized version of the.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.hardtanh">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardtanh.html#torch.ao.nn.quantized.functional.hardtanh" title="torch.ao.nn.quantized.functional.hardtanh">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  hardtanh
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.functional.hardtanh.html#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  hardtanh()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.hardswish">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardswish.html#torch.ao.nn.quantized.functional.hardswish" title="torch.ao.nn.quantized.functional.hardswish">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  hardswish
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.functional.hardswish.html#torch.nn.functional.hardswish" title="torch.nn.functional.hardswish">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  hardswish()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.threshold">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.threshold.html#torch.ao.nn.quantized.functional.threshold" title="torch.ao.nn.quantized.functional.threshold">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  threshold
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies the quantized version of the threshold function element-wise:
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.elu">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.elu.html#torch.ao.nn.quantized.functional.elu" title="torch.ao.nn.quantized.functional.elu">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  elu
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.functional.elu.html#torch.nn.functional.elu" title="torch.nn.functional.elu">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  elu()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.hardsigmoid">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.hardsigmoid.html#torch.ao.nn.quantized.functional.hardsigmoid" title="torch.ao.nn.quantized.functional.hardsigmoid">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  hardsigmoid
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               This is the quantized version of
               <a class="reference internal" href="generated/torch.nn.functional.hardsigmoid.html#torch.nn.functional.hardsigmoid" title="torch.nn.functional.hardsigmoid">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  hardsigmoid()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.clamp">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.clamp.html#torch.ao.nn.quantized.functional.clamp" title="torch.ao.nn.quantized.functional.clamp">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  clamp
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               float(input, min_, max_) -&gt; Tensor
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.upsample">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample.html#torch.ao.nn.quantized.functional.upsample" title="torch.ao.nn.quantized.functional.upsample">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  upsample
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Upsamples the input to either the given
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 size
                </span>
               </code>
               or the given
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 scale_factor
                </span>
               </code>
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.upsample_bilinear">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_bilinear.html#torch.ao.nn.quantized.functional.upsample_bilinear" title="torch.ao.nn.quantized.functional.upsample_bilinear">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  upsample_bilinear
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Upsamples the input, using bilinear upsampling.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.functional.upsample_nearest">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.functional.upsample_nearest.html#torch.ao.nn.quantized.functional.upsample_nearest" title="torch.ao.nn.quantized.functional.upsample_nearest">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  upsample_nearest
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Upsamples the input, using nearest neighbours' pixel values.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="torch-ao-nn-quantizable">
          <h2>
           torch.ao.nn.quantizable
           <a class="headerlink" href="#torch-ao-nn-quantizable" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This module implements the quantizable versions of some of the nn layers.
These modules can be used in conjunction with the custom module mechanism,
by providing the
           <code class="docutils literal notranslate">
            <span class="pre">
             custom_module_config
            </span>
           </code>
           argument to both prepare and convert.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantizable.LSTM">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantizable.LSTM.html#torch.ao.nn.quantizable.LSTM" title="torch.ao.nn.quantizable.LSTM">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LSTM
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A quantizable long short-term memory (LSTM).
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantizable.MultiheadAttention">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantizable.MultiheadAttention.html#torch.ao.nn.quantizable.MultiheadAttention" title="torch.ao.nn.quantizable.MultiheadAttention">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  MultiheadAttention
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="module-torch.ao.nn.quantized.dynamic">
          <span id="torch-ao-nn-quantized-dynamic">
          </span>
          <h2>
           torch.ao.nn.quantized.dynamic
           <a class="headerlink" href="#module-torch.ao.nn.quantized.dynamic" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <span class="target" id="module-torch.ao.nn.quantized.dynamic.modules">
          </span>
          <p>
           Dynamically quantized
           <a class="reference internal" href="generated/torch.nn.Linear.html#torch.nn.Linear" title="torch.nn.Linear">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              Linear
             </span>
            </code>
           </a>
           ,
           <a class="reference internal" href="generated/torch.nn.LSTM.html#torch.nn.LSTM" title="torch.nn.LSTM">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              LSTM
             </span>
            </code>
           </a>
           ,
           <a class="reference internal" href="generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell" title="torch.nn.LSTMCell">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              LSTMCell
             </span>
            </code>
           </a>
           ,
           <a class="reference internal" href="generated/torch.nn.GRUCell.html#torch.nn.GRUCell" title="torch.nn.GRUCell">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              GRUCell
             </span>
            </code>
           </a>
           , and
           <a class="reference internal" href="generated/torch.nn.RNNCell.html#torch.nn.RNNCell" title="torch.nn.RNNCell">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              RNNCell
             </span>
            </code>
           </a>
           .
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.dynamic.Linear">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.Linear.html#torch.ao.nn.quantized.dynamic.Linear" title="torch.ao.nn.quantized.dynamic.Linear">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Linear
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A dynamic quantized linear module with floating point tensor as inputs and outputs.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.dynamic.LSTM">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTM.html#torch.ao.nn.quantized.dynamic.LSTM" title="torch.ao.nn.quantized.dynamic.LSTM">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LSTM
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A dynamic quantized LSTM module with floating point tensor as inputs and outputs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.dynamic.GRU">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRU.html#torch.ao.nn.quantized.dynamic.GRU" title="torch.ao.nn.quantized.dynamic.GRU">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  GRU
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.dynamic.RNNCell">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.RNNCell.html#torch.ao.nn.quantized.dynamic.RNNCell" title="torch.ao.nn.quantized.dynamic.RNNCell">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  RNNCell
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               An Elman RNN cell with tanh or ReLU non-linearity.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.dynamic.LSTMCell">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.LSTMCell.html#torch.ao.nn.quantized.dynamic.LSTMCell" title="torch.ao.nn.quantized.dynamic.LSTMCell">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  LSTMCell
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A long short-term memory (LSTM) cell.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.ao.nn.quantized.dynamic.GRUCell">
               </p>
               <a class="reference internal" href="generated/torch.ao.nn.quantized.dynamic.GRUCell.html#torch.ao.nn.quantized.dynamic.GRUCell" title="torch.ao.nn.quantized.dynamic.GRUCell">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  GRUCell
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               A gated recurrent unit (GRU) cell
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="quantized-dtypes-and-quantization-schemes">
          <h2>
           Quantized dtypes and quantization schemes
           <a class="headerlink" href="#quantized-dtypes-and-quantization-schemes" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           Note that operator implementations currently only
support per channel quantization for weights of the
           <strong>
            conv
           </strong>
           and
           <strong>
            linear
           </strong>
           operators. Furthermore, the input data is
mapped linearly to the quantized data and vice versa
as follows:
          </p>
          <blockquote>
           <div>
            <div class="math">
             <span class="katex-display">
              <span class="katex">
               <span class="katex-mathml">
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                 <semantics>
                  <mtable columnalign="right left" columnspacing="0em" rowspacing="0.25em">
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mtext>
                       Quantization:
                      </mtext>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                       <mrow>
                       </mrow>
                       <msub>
                        <mi>
                         Q
                        </mi>
                        <mtext>
                         out
                        </mtext>
                       </msub>
                       <mo>
                        =
                       </mo>
                       <mtext>
                        clamp
                       </mtext>
                       <mo stretchy="false">
                        (
                       </mo>
                       <msub>
                        <mi>
                         x
                        </mi>
                        <mtext>
                         input
                        </mtext>
                       </msub>
                       <mi mathvariant="normal">
                        /
                       </mi>
                       <mi>
                        s
                       </mi>
                       <mo>
                        +
                       </mo>
                       <mi>
                        z
                       </mi>
                       <mo separator="true">
                        ,
                       </mo>
                       <msub>
                        <mi>
                         Q
                        </mi>
                        <mtext>
                         min
                        </mtext>
                       </msub>
                       <mo separator="true">
                        ,
                       </mo>
                       <msub>
                        <mi>
                         Q
                        </mi>
                        <mtext>
                         max
                        </mtext>
                       </msub>
                       <mo stretchy="false">
                        )
                       </mo>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mtext>
                       Dequantization:
                      </mtext>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                       <mrow>
                       </mrow>
                       <msub>
                        <mi>
                         x
                        </mi>
                        <mtext>
                         out
                        </mtext>
                       </msub>
                       <mo>
                        =
                       </mo>
                       <mo stretchy="false">
                        (
                       </mo>
                       <msub>
                        <mi>
                         Q
                        </mi>
                        <mtext>
                         input
                        </mtext>
                       </msub>
                       <mo>
                        −
                       </mo>
                       <mi>
                        z
                       </mi>
                       <mo stretchy="false">
                        )
                       </mo>
                       <mo>
                        ∗
                       </mo>
                       <mi>
                        s
                       </mi>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                  </mtable>
                  <annotation encoding="application/x-tex">
                   \begin{aligned}
    \text{Quantization:}&amp;\\
    &amp;Q_\text{out} = \text{clamp}(x_\text{input}/s+z, Q_\text{min}, Q_\text{max})\\
    \text{Dequantization:}&amp;\\
    &amp;x_\text{out} = (Q_\text{input}-z)*s
\end{aligned}
                  </annotation>
                 </semantics>
                </math>
               </span>
               <span aria-hidden="true" class="katex-html">
                <span class="base">
                 <span class="strut" style="height:6em;vertical-align:-2.75em;">
                 </span>
                 <span class="mord">
                  <span class="mtable">
                   <span class="col-align-r">
                    <span class="vlist-t vlist-t2">
                     <span class="vlist-r">
                      <span class="vlist" style="height:3.25em;">
                       <span style="top:-5.41em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                         <span class="mord text">
                          <span class="mord">
                           Quantization:
                          </span>
                         </span>
                        </span>
                       </span>
                       <span style="top:-3.91em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                        </span>
                       </span>
                       <span style="top:-2.41em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                         <span class="mord text">
                          <span class="mord">
                           Dequantization:
                          </span>
                         </span>
                        </span>
                       </span>
                       <span style="top:-0.91em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                        </span>
                       </span>
                      </span>
                      <span class="vlist-s">
                       ​
                      </span>
                     </span>
                     <span class="vlist-r">
                      <span class="vlist" style="height:2.75em;">
                       <span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                   <span class="col-align-l">
                    <span class="vlist-t vlist-t2">
                     <span class="vlist-r">
                      <span class="vlist" style="height:3.25em;">
                       <span style="top:-5.41em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                        </span>
                       </span>
                       <span style="top:-3.91em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           Q
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.2806em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  out
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mrel">
                          =
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mord text">
                          <span class="mord">
                           clamp
                          </span>
                         </span>
                         <span class="mopen">
                          (
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           x
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.3175em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  input
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.2861em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mord">
                          /
                         </span>
                         <span class="mord mathnormal">
                          s
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mbin">
                          +
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mord mathnormal" style="margin-right:0.04398em;">
                          z
                         </span>
                         <span class="mpunct">
                          ,
                         </span>
                         <span class="mspace" style="margin-right:0.1667em;">
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           Q
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.3175em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  min
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mpunct">
                          ,
                         </span>
                         <span class="mspace" style="margin-right:0.1667em;">
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           Q
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.1514em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  max
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mclose">
                          )
                         </span>
                        </span>
                       </span>
                       <span style="top:-2.41em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                        </span>
                       </span>
                       <span style="top:-0.91em;">
                        <span class="pstrut" style="height:3em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           x
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.2806em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  out
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mrel">
                          =
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mopen">
                          (
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           Q
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.3175em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  input
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.2861em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mbin">
                          −
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mord mathnormal" style="margin-right:0.04398em;">
                          z
                         </span>
                         <span class="mclose">
                          )
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mbin">
                          ∗
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mord mathnormal">
                          s
                         </span>
                        </span>
                       </span>
                      </span>
                      <span class="vlist-s">
                       ​
                      </span>
                     </span>
                     <span class="vlist-r">
                      <span class="vlist" style="height:2.75em;">
                       <span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
              </span>
             </span>
            </div>
           </div>
          </blockquote>
          <p>
           where
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <mtext>
                  clamp
                 </mtext>
                 <mo stretchy="false">
                  (
                 </mo>
                 <mi mathvariant="normal">
                  .
                 </mi>
                 <mo stretchy="false">
                  )
                 </mo>
                </mrow>
                <annotation encoding="application/x-tex">
                 \text{clamp}(.)
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:1em;vertical-align:-0.25em;">
               </span>
               <span class="mord text">
                <span class="mord">
                 clamp
                </span>
               </span>
               <span class="mopen">
                (
               </span>
               <span class="mord">
                .
               </span>
               <span class="mclose">
                )
               </span>
              </span>
             </span>
            </span>
           </span>
           is the same as
           <a class="reference internal" href="generated/torch.clamp.html#torch.clamp" title="torch.clamp">
            <code class="xref py py-func docutils literal notranslate">
             <span class="pre">
              clamp()
             </span>
            </code>
           </a>
           while the
scale
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <mi>
                  s
                 </mi>
                </mrow>
                <annotation encoding="application/x-tex">
                 s
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:0.4306em;">
               </span>
               <span class="mord mathnormal">
                s
               </span>
              </span>
             </span>
            </span>
           </span>
           and zero point
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <mi>
                  z
                 </mi>
                </mrow>
                <annotation encoding="application/x-tex">
                 z
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:0.4306em;">
               </span>
               <span class="mord mathnormal" style="margin-right:0.04398em;">
                z
               </span>
              </span>
             </span>
            </span>
           </span>
           are then computed
as described in
           <a class="reference internal" href="generated/torch.ao.quantization.observer.MinMaxObserver.html#torch.ao.quantization.observer.MinMaxObserver" title="torch.ao.quantization.observer.MinMaxObserver">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              MinMaxObserver
             </span>
            </code>
           </a>
           , specifically:
          </p>
          <blockquote>
           <div>
            <div class="math">
             <span class="katex-display">
              <span class="katex">
               <span class="katex-mathml">
                <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                 <semantics>
                  <mtable columnalign="right left" columnspacing="0em" rowspacing="0.25em">
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mtext>
                       if Symmetric:
                      </mtext>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                       <mrow>
                       </mrow>
                       <mi>
                        s
                       </mi>
                       <mo>
                        =
                       </mo>
                       <mn>
                        2
                       </mn>
                       <mi>
                        max
                       </mi>
                       <mo>
                        ⁡
                       </mo>
                       <mo stretchy="false">
                        (
                       </mo>
                       <mi mathvariant="normal">
                        ∣
                       </mi>
                       <msub>
                        <mi>
                         x
                        </mi>
                        <mtext>
                         min
                        </mtext>
                       </msub>
                       <mi mathvariant="normal">
                        ∣
                       </mi>
                       <mo separator="true">
                        ,
                       </mo>
                       <msub>
                        <mi>
                         x
                        </mi>
                        <mtext>
                         max
                        </mtext>
                       </msub>
                       <mo stretchy="false">
                        )
                       </mo>
                       <mi mathvariant="normal">
                        /
                       </mi>
                       <mrow>
                        <mo fence="true">
                         (
                        </mo>
                        <msub>
                         <mi>
                          Q
                         </mi>
                         <mtext>
                          max
                         </mtext>
                        </msub>
                        <mo>
                         −
                        </mo>
                        <msub>
                         <mi>
                          Q
                         </mi>
                         <mtext>
                          min
                         </mtext>
                        </msub>
                        <mo fence="true">
                         )
                        </mo>
                       </mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                       <mrow>
                       </mrow>
                       <mi>
                        z
                       </mi>
                       <mo>
                        =
                       </mo>
                       <mrow>
                        <mo fence="true">
                         {
                        </mo>
                        <mtable columnalign="left left" columnspacing="1em" rowspacing="0.36em">
                         <mtr>
                          <mtd>
                           <mstyle displaystyle="false" scriptlevel="0">
                            <mn>
                             0
                            </mn>
                           </mstyle>
                          </mtd>
                          <mtd>
                           <mstyle displaystyle="false" scriptlevel="0">
                            <mtext>
                             if dtype is qint8
                            </mtext>
                           </mstyle>
                          </mtd>
                         </mtr>
                         <mtr>
                          <mtd>
                           <mstyle displaystyle="false" scriptlevel="0">
                            <mn>
                             128
                            </mn>
                           </mstyle>
                          </mtd>
                          <mtd>
                           <mstyle displaystyle="false" scriptlevel="0">
                            <mtext>
                             otherwise
                            </mtext>
                           </mstyle>
                          </mtd>
                         </mtr>
                        </mtable>
                       </mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mtext>
                       Otherwise:
                      </mtext>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                       <mrow>
                       </mrow>
                       <mi>
                        s
                       </mi>
                       <mo>
                        =
                       </mo>
                       <mrow>
                        <mo fence="true">
                         (
                        </mo>
                        <msub>
                         <mi>
                          x
                         </mi>
                         <mtext>
                          max
                         </mtext>
                        </msub>
                        <mo>
                         −
                        </mo>
                        <msub>
                         <mi>
                          x
                         </mi>
                         <mtext>
                          min
                         </mtext>
                        </msub>
                        <mo fence="true">
                         )
                        </mo>
                       </mrow>
                       <mi mathvariant="normal">
                        /
                       </mi>
                       <mrow>
                        <mo fence="true">
                         (
                        </mo>
                        <msub>
                         <mi>
                          Q
                         </mi>
                         <mtext>
                          max
                         </mtext>
                        </msub>
                        <mo>
                         −
                        </mo>
                        <msub>
                         <mi>
                          Q
                         </mi>
                         <mtext>
                          min
                         </mtext>
                        </msub>
                        <mo fence="true">
                         )
                        </mo>
                       </mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                   <mtr>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                      </mrow>
                     </mstyle>
                    </mtd>
                    <mtd>
                     <mstyle displaystyle="true" scriptlevel="0">
                      <mrow>
                       <mrow>
                       </mrow>
                       <mi>
                        z
                       </mi>
                       <mo>
                        =
                       </mo>
                       <msub>
                        <mi>
                         Q
                        </mi>
                        <mtext>
                         min
                        </mtext>
                       </msub>
                       <mo>
                        −
                       </mo>
                       <mtext>
                        round
                       </mtext>
                       <mo stretchy="false">
                        (
                       </mo>
                       <msub>
                        <mi>
                         x
                        </mi>
                        <mtext>
                         min
                        </mtext>
                       </msub>
                       <mi mathvariant="normal">
                        /
                       </mi>
                       <mi>
                        s
                       </mi>
                       <mo stretchy="false">
                        )
                       </mo>
                      </mrow>
                     </mstyle>
                    </mtd>
                   </mtr>
                  </mtable>
                  <annotation encoding="application/x-tex">
                   \begin{aligned}
    \text{if Symmetric:}&amp;\\
    &amp;s = 2 \max(|x_\text{min}|, x_\text{max}) /
        \left( Q_\text{max} - Q_\text{min} \right) \\
    &amp;z = \begin{cases}
        0 &amp; \text{if dtype is qint8} \\
        128 &amp; \text{otherwise}
    \end{cases}\\
    \text{Otherwise:}&amp;\\
        &amp;s = \left( x_\text{max} - x_\text{min}  \right ) /
            \left( Q_\text{max} - Q_\text{min} \right ) \\
        &amp;z = Q_\text{min} - \text{round}(x_\text{min} / s)
\end{aligned}
                  </annotation>
                 </semantics>
                </math>
               </span>
               <span aria-hidden="true" class="katex-html">
                <span class="base">
                 <span class="strut" style="height:10.8em;vertical-align:-5.15em;">
                 </span>
                 <span class="mord">
                  <span class="mtable">
                   <span class="col-align-r">
                    <span class="vlist-t vlist-t2">
                     <span class="vlist-r">
                      <span class="vlist" style="height:5.65em;">
                       <span style="top:-8.56em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord text">
                          <span class="mord">
                           if Symmetric:
                          </span>
                         </span>
                        </span>
                       </span>
                       <span style="top:-7.06em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                        </span>
                       </span>
                       <span style="top:-4.65em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                        </span>
                       </span>
                       <span style="top:-2.26em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord text">
                          <span class="mord">
                           Otherwise:
                          </span>
                         </span>
                        </span>
                       </span>
                       <span style="top:-0.76em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                        </span>
                       </span>
                       <span style="top:0.74em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                        </span>
                       </span>
                      </span>
                      <span class="vlist-s">
                       ​
                      </span>
                     </span>
                     <span class="vlist-r">
                      <span class="vlist" style="height:5.15em;">
                       <span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                   <span class="col-align-l">
                    <span class="vlist-t vlist-t2">
                     <span class="vlist-r">
                      <span class="vlist" style="height:5.65em;">
                       <span style="top:-8.56em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                        </span>
                       </span>
                       <span style="top:-7.06em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                         <span class="mord mathnormal">
                          s
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mrel">
                          =
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mord">
                          2
                         </span>
                         <span class="mspace" style="margin-right:0.1667em;">
                         </span>
                         <span class="mop">
                          max
                         </span>
                         <span class="mopen">
                          (
                         </span>
                         <span class="mord">
                          ∣
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           x
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.3175em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  min
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mord">
                          ∣
                         </span>
                         <span class="mpunct">
                          ,
                         </span>
                         <span class="mspace" style="margin-right:0.1667em;">
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           x
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.1514em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  max
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mclose">
                          )
                         </span>
                         <span class="mord">
                          /
                         </span>
                         <span class="mspace" style="margin-right:0.1667em;">
                         </span>
                         <span class="minner">
                          <span class="mopen delimcenter" style="top:0em;">
                           (
                          </span>
                          <span class="mord">
                           <span class="mord mathnormal">
                            Q
                           </span>
                           <span class="msupsub">
                            <span class="vlist-t vlist-t2">
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.1514em;">
                               <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                                <span class="pstrut" style="height:2.7em;">
                                </span>
                                <span class="sizing reset-size6 size3 mtight">
                                 <span class="mord text mtight">
                                  <span class="mord mtight">
                                   max
                                  </span>
                                 </span>
                                </span>
                               </span>
                              </span>
                              <span class="vlist-s">
                               ​
                              </span>
                             </span>
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.15em;">
                               <span>
                               </span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                          <span class="mspace" style="margin-right:0.2222em;">
                          </span>
                          <span class="mbin">
                           −
                          </span>
                          <span class="mspace" style="margin-right:0.2222em;">
                          </span>
                          <span class="mord">
                           <span class="mord mathnormal">
                            Q
                           </span>
                           <span class="msupsub">
                            <span class="vlist-t vlist-t2">
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.3175em;">
                               <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                                <span class="pstrut" style="height:2.7em;">
                                </span>
                                <span class="sizing reset-size6 size3 mtight">
                                 <span class="mord text mtight">
                                  <span class="mord mtight">
                                   min
                                  </span>
                                 </span>
                                </span>
                               </span>
                              </span>
                              <span class="vlist-s">
                               ​
                              </span>
                             </span>
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.15em;">
                               <span>
                               </span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                          <span class="mclose delimcenter" style="top:0em;">
                           )
                          </span>
                         </span>
                        </span>
                       </span>
                       <span style="top:-4.65em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                         <span class="mord mathnormal" style="margin-right:0.04398em;">
                          z
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mrel">
                          =
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="minner">
                          <span class="mopen delimcenter" style="top:0em;">
                           <span class="delimsizing size4">
                            {
                           </span>
                          </span>
                          <span class="mord">
                           <span class="mtable">
                            <span class="col-align-l">
                             <span class="vlist-t vlist-t2">
                              <span class="vlist-r">
                               <span class="vlist" style="height:1.69em;">
                                <span style="top:-3.69em;">
                                 <span class="pstrut" style="height:3.008em;">
                                 </span>
                                 <span class="mord">
                                  <span class="mord">
                                   0
                                  </span>
                                 </span>
                                </span>
                                <span style="top:-2.25em;">
                                 <span class="pstrut" style="height:3.008em;">
                                 </span>
                                 <span class="mord">
                                  <span class="mord">
                                   128
                                  </span>
                                 </span>
                                </span>
                               </span>
                               <span class="vlist-s">
                                ​
                               </span>
                              </span>
                              <span class="vlist-r">
                               <span class="vlist" style="height:1.19em;">
                                <span>
                                </span>
                               </span>
                              </span>
                             </span>
                            </span>
                            <span class="arraycolsep" style="width:1em;">
                            </span>
                            <span class="col-align-l">
                             <span class="vlist-t vlist-t2">
                              <span class="vlist-r">
                               <span class="vlist" style="height:1.69em;">
                                <span style="top:-3.69em;">
                                 <span class="pstrut" style="height:3.008em;">
                                 </span>
                                 <span class="mord">
                                  <span class="mord text">
                                   <span class="mord">
                                    if dtype is qint8
                                   </span>
                                  </span>
                                 </span>
                                </span>
                                <span style="top:-2.25em;">
                                 <span class="pstrut" style="height:3.008em;">
                                 </span>
                                 <span class="mord">
                                  <span class="mord text">
                                   <span class="mord">
                                    otherwise
                                   </span>
                                  </span>
                                 </span>
                                </span>
                               </span>
                               <span class="vlist-s">
                                ​
                               </span>
                              </span>
                              <span class="vlist-r">
                               <span class="vlist" style="height:1.19em;">
                                <span>
                                </span>
                               </span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                          <span class="mclose nulldelimiter">
                          </span>
                         </span>
                        </span>
                       </span>
                       <span style="top:-2.26em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                        </span>
                       </span>
                       <span style="top:-0.76em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                         <span class="mord mathnormal">
                          s
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mrel">
                          =
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="minner">
                          <span class="mopen delimcenter" style="top:0em;">
                           (
                          </span>
                          <span class="mord">
                           <span class="mord mathnormal">
                            x
                           </span>
                           <span class="msupsub">
                            <span class="vlist-t vlist-t2">
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.1514em;">
                               <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                                <span class="pstrut" style="height:2.7em;">
                                </span>
                                <span class="sizing reset-size6 size3 mtight">
                                 <span class="mord text mtight">
                                  <span class="mord mtight">
                                   max
                                  </span>
                                 </span>
                                </span>
                               </span>
                              </span>
                              <span class="vlist-s">
                               ​
                              </span>
                             </span>
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.15em;">
                               <span>
                               </span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                          <span class="mspace" style="margin-right:0.2222em;">
                          </span>
                          <span class="mbin">
                           −
                          </span>
                          <span class="mspace" style="margin-right:0.2222em;">
                          </span>
                          <span class="mord">
                           <span class="mord mathnormal">
                            x
                           </span>
                           <span class="msupsub">
                            <span class="vlist-t vlist-t2">
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.3175em;">
                               <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                                <span class="pstrut" style="height:2.7em;">
                                </span>
                                <span class="sizing reset-size6 size3 mtight">
                                 <span class="mord text mtight">
                                  <span class="mord mtight">
                                   min
                                  </span>
                                 </span>
                                </span>
                               </span>
                              </span>
                              <span class="vlist-s">
                               ​
                              </span>
                             </span>
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.15em;">
                               <span>
                               </span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                          <span class="mclose delimcenter" style="top:0em;">
                           )
                          </span>
                         </span>
                         <span class="mspace" style="margin-right:0.1667em;">
                         </span>
                         <span class="mord">
                          /
                         </span>
                         <span class="mspace" style="margin-right:0.1667em;">
                         </span>
                         <span class="minner">
                          <span class="mopen delimcenter" style="top:0em;">
                           (
                          </span>
                          <span class="mord">
                           <span class="mord mathnormal">
                            Q
                           </span>
                           <span class="msupsub">
                            <span class="vlist-t vlist-t2">
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.1514em;">
                               <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                                <span class="pstrut" style="height:2.7em;">
                                </span>
                                <span class="sizing reset-size6 size3 mtight">
                                 <span class="mord text mtight">
                                  <span class="mord mtight">
                                   max
                                  </span>
                                 </span>
                                </span>
                               </span>
                              </span>
                              <span class="vlist-s">
                               ​
                              </span>
                             </span>
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.15em;">
                               <span>
                               </span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                          <span class="mspace" style="margin-right:0.2222em;">
                          </span>
                          <span class="mbin">
                           −
                          </span>
                          <span class="mspace" style="margin-right:0.2222em;">
                          </span>
                          <span class="mord">
                           <span class="mord mathnormal">
                            Q
                           </span>
                           <span class="msupsub">
                            <span class="vlist-t vlist-t2">
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.3175em;">
                               <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                                <span class="pstrut" style="height:2.7em;">
                                </span>
                                <span class="sizing reset-size6 size3 mtight">
                                 <span class="mord text mtight">
                                  <span class="mord mtight">
                                   min
                                  </span>
                                 </span>
                                </span>
                               </span>
                              </span>
                              <span class="vlist-s">
                               ​
                              </span>
                             </span>
                             <span class="vlist-r">
                              <span class="vlist" style="height:0.15em;">
                               <span>
                               </span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                          <span class="mclose delimcenter" style="top:0em;">
                           )
                          </span>
                         </span>
                        </span>
                       </span>
                       <span style="top:0.74em;">
                        <span class="pstrut" style="height:3.75em;">
                        </span>
                        <span class="mord">
                         <span class="mord">
                         </span>
                         <span class="mord mathnormal" style="margin-right:0.04398em;">
                          z
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mrel">
                          =
                         </span>
                         <span class="mspace" style="margin-right:0.2778em;">
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           Q
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.3175em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  min
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mbin">
                          −
                         </span>
                         <span class="mspace" style="margin-right:0.2222em;">
                         </span>
                         <span class="mord text">
                          <span class="mord">
                           round
                          </span>
                         </span>
                         <span class="mopen">
                          (
                         </span>
                         <span class="mord">
                          <span class="mord mathnormal">
                           x
                          </span>
                          <span class="msupsub">
                           <span class="vlist-t vlist-t2">
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.3175em;">
                              <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                               <span class="pstrut" style="height:2.7em;">
                               </span>
                               <span class="sizing reset-size6 size3 mtight">
                                <span class="mord text mtight">
                                 <span class="mord mtight">
                                  min
                                 </span>
                                </span>
                               </span>
                              </span>
                             </span>
                             <span class="vlist-s">
                              ​
                             </span>
                            </span>
                            <span class="vlist-r">
                             <span class="vlist" style="height:0.15em;">
                              <span>
                              </span>
                             </span>
                            </span>
                           </span>
                          </span>
                         </span>
                         <span class="mord">
                          /
                         </span>
                         <span class="mord mathnormal">
                          s
                         </span>
                         <span class="mclose">
                          )
                         </span>
                        </span>
                       </span>
                      </span>
                      <span class="vlist-s">
                       ​
                      </span>
                     </span>
                     <span class="vlist-r">
                      <span class="vlist" style="height:5.15em;">
                       <span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
              </span>
             </span>
            </div>
           </div>
          </blockquote>
          <p>
           where
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <mo stretchy="false">
                  [
                 </mo>
                 <msub>
                  <mi>
                   x
                  </mi>
                  <mtext>
                   min
                  </mtext>
                 </msub>
                 <mo separator="true">
                  ,
                 </mo>
                 <msub>
                  <mi>
                   x
                  </mi>
                  <mtext>
                   max
                  </mtext>
                 </msub>
                 <mo stretchy="false">
                  ]
                 </mo>
                </mrow>
                <annotation encoding="application/x-tex">
                 [x_\text{min}, x_\text{max}]
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:1em;vertical-align:-0.25em;">
               </span>
               <span class="mopen">
                [
               </span>
               <span class="mord">
                <span class="mord mathnormal">
                 x
                </span>
                <span class="msupsub">
                 <span class="vlist-t vlist-t2">
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.3175em;">
                    <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                     <span class="pstrut" style="height:2.7em;">
                     </span>
                     <span class="sizing reset-size6 size3 mtight">
                      <span class="mord text mtight">
                       <span class="mord mtight">
                        min
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                   <span class="vlist-s">
                    ​
                   </span>
                  </span>
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.15em;">
                    <span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               <span class="mpunct">
                ,
               </span>
               <span class="mspace" style="margin-right:0.1667em;">
               </span>
               <span class="mord">
                <span class="mord mathnormal">
                 x
                </span>
                <span class="msupsub">
                 <span class="vlist-t vlist-t2">
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.1514em;">
                    <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                     <span class="pstrut" style="height:2.7em;">
                     </span>
                     <span class="sizing reset-size6 size3 mtight">
                      <span class="mord text mtight">
                       <span class="mord mtight">
                        max
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                   <span class="vlist-s">
                    ​
                   </span>
                  </span>
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.15em;">
                    <span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               <span class="mclose">
                ]
               </span>
              </span>
             </span>
            </span>
           </span>
           denotes the range of the input data while
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <msub>
                  <mi>
                   Q
                  </mi>
                  <mtext>
                   min
                  </mtext>
                 </msub>
                </mrow>
                <annotation encoding="application/x-tex">
                 Q_\text{min}
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:0.8778em;vertical-align:-0.1944em;">
               </span>
               <span class="mord">
                <span class="mord mathnormal">
                 Q
                </span>
                <span class="msupsub">
                 <span class="vlist-t vlist-t2">
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.3175em;">
                    <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                     <span class="pstrut" style="height:2.7em;">
                     </span>
                     <span class="sizing reset-size6 size3 mtight">
                      <span class="mord text mtight">
                       <span class="mord mtight">
                        min
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                   <span class="vlist-s">
                    ​
                   </span>
                  </span>
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.15em;">
                    <span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           and
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <msub>
                  <mi>
                   Q
                  </mi>
                  <mtext>
                   max
                  </mtext>
                 </msub>
                </mrow>
                <annotation encoding="application/x-tex">
                 Q_\text{max}
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:0.8778em;vertical-align:-0.1944em;">
               </span>
               <span class="mord">
                <span class="mord mathnormal">
                 Q
                </span>
                <span class="msupsub">
                 <span class="vlist-t vlist-t2">
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.1514em;">
                    <span style="top:-2.55em;margin-left:0em;margin-right:0.05em;">
                     <span class="pstrut" style="height:2.7em;">
                     </span>
                     <span class="sizing reset-size6 size3 mtight">
                      <span class="mord text mtight">
                       <span class="mord mtight">
                        max
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                   <span class="vlist-s">
                    ​
                   </span>
                  </span>
                  <span class="vlist-r">
                   <span class="vlist" style="height:0.15em;">
                    <span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           are respectively the minimum and maximum values of the quantized dtype.
          </p>
          <p>
           Note that the choice of
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <mi>
                  s
                 </mi>
                </mrow>
                <annotation encoding="application/x-tex">
                 s
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:0.4306em;">
               </span>
               <span class="mord mathnormal">
                s
               </span>
              </span>
             </span>
            </span>
           </span>
           and
           <span class="math">
            <span class="katex">
             <span class="katex-mathml">
              <math xmlns="http://www.w3.org/1998/Math/MathML">
               <semantics>
                <mrow>
                 <mi>
                  z
                 </mi>
                </mrow>
                <annotation encoding="application/x-tex">
                 z
                </annotation>
               </semantics>
              </math>
             </span>
             <span aria-hidden="true" class="katex-html">
              <span class="base">
               <span class="strut" style="height:0.4306em;">
               </span>
               <span class="mord mathnormal" style="margin-right:0.04398em;">
                z
               </span>
              </span>
             </span>
            </span>
           </span>
           implies that zero is represented with no quantization error whenever zero is within
the range of the input data or symmetric quantization is being used.
          </p>
          <p>
           Additional data types and quantization schemes can be implemented through
the
           <a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">
            custom operator mechanism
           </a>
           .
          </p>
          <ul class="simple">
           <li>
            <p>
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               torch.qscheme
              </span>
             </code>
             — Type to describe the quantization scheme of a tensor.
Supported types:
            </p>
            <ul>
             <li>
              <p>
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 torch.per_tensor_affine
                </span>
               </code>
               — per tensor, asymmetric
              </p>
             </li>
             <li>
              <p>
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 torch.per_channel_affine
                </span>
               </code>
               — per channel, asymmetric
              </p>
             </li>
             <li>
              <p>
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 torch.per_tensor_symmetric
                </span>
               </code>
               — per tensor, symmetric
              </p>
             </li>
             <li>
              <p>
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 torch.per_channel_symmetric
                </span>
               </code>
               — per channel, symmetric
              </p>
             </li>
            </ul>
           </li>
           <li>
            <p>
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.dtype
              </span>
             </code>
             — Type to describe the data. Supported types:
            </p>
            <ul>
             <li>
              <p>
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 torch.quint8
                </span>
               </code>
               — 8-bit unsigned integer
              </p>
             </li>
             <li>
              <p>
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 torch.qint8
                </span>
               </code>
               — 8-bit signed integer
              </p>
             </li>
             <li>
              <p>
               <code class="xref py py-attr docutils literal notranslate">
                <span class="pre">
                 torch.qint32
                </span>
               </code>
               — 32-bit signed integer
              </p>
             </li>
            </ul>
           </li>
          </ul>
          <span class="target" id="module-torch.nn.quantizable">
          </span>
          <span class="target" id="module-torch.nn.qat.dynamic.modules">
          </span>
          <span class="target" id="module-torch.nn.qat.modules">
          </span>
          <p>
           QAT Modules.
          </p>
          <p>
           This package is in the process of being deprecated.
Please, use
           <cite>
            torch.ao.nn.qat.modules
           </cite>
           instead.
          </p>
          <span class="target" id="module-torch.nn.qat">
          </span>
          <p>
           QAT Dynamic Modules.
          </p>
          <p>
           This package is in the process of being deprecated.
Please, use
           <cite>
            torch.ao.nn.qat.dynamic
           </cite>
           instead.
          </p>
          <span class="target" id="module-torch.nn.intrinsic.qat.modules">
          </span>
          <span class="target" id="module-torch.nn.quantized.dynamic">
          </span>
          <span class="target" id="module-torch.nn.intrinsic">
          </span>
          <span class="target" id="module-torch.nn.intrinsic.quantized.modules">
          </span>
          <span class="target" id="module-torch.quantization.fx">
          </span>
          <p>
           This file is in the process of migration to
           <cite>
            torch/ao/quantization
           </cite>
           , and
is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate files under
           <cite>
            torch/ao/quantization/fx/
           </cite>
           , while adding an import statement
here.
          </p>
          <span class="target" id="module-torch.nn.intrinsic.quantized.dynamic">
          </span>
          <span class="target" id="module-torch.nn.qat.dynamic">
          </span>
          <p>
           QAT Dynamic Modules.
          </p>
          <p>
           This package is in the process of being deprecated.
Please, use
           <cite>
            torch.ao.nn.qat.dynamic
           </cite>
           instead.
          </p>
          <span class="target" id="module-torch.nn.intrinsic.qat">
          </span>
          <span class="target" id="module-torch.nn.quantized.modules">
          </span>
          <p>
           Quantized Modules.
          </p>
          <dl class="simple">
           <dt>
            Note::
           </dt>
           <dd>
            <p>
             The
             <cite>
              torch.nn.quantized
             </cite>
             namespace is in the process of being deprecated.
Please, use
             <cite>
              torch.ao.nn.quantized
             </cite>
             instead.
            </p>
           </dd>
          </dl>
          <span class="target" id="module-torch.nn.intrinsic.quantized">
          </span>
          <span class="target" id="module-torch.nn.quantizable.modules">
          </span>
          <span class="target" id="module-torch.nn.quantized">
          </span>
          <span class="target" id="module-torch.nn.intrinsic.quantized.dynamic.modules">
          </span>
          <span class="target" id="module-torch.nn.quantized.dynamic.modules">
          </span>
          <p>
           Quantized Dynamic Modules.
          </p>
          <p>
           This file is in the process of migration to
           <cite>
            torch/ao/nn/quantized/dynamic
           </cite>
           ,
and is kept here for compatibility while the migration process is ongoing.
If you are adding a new entry/functionality, please, add it to the
appropriate file under the
           <cite>
            torch/ao/nn/quantized/dynamic
           </cite>
           ,
while adding an import statement here.
          </p>
          <span class="target" id="module-torch.quantization">
          </span>
          <span class="target" id="module-torch.nn.intrinsic.modules">
          </span>
         </div>
        </div>
       </article>
      </div>
      <footer>
       <div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
        <a accesskey="n" class="btn btn-neutral float-right" href="generated/torch.ao.quantization.quantize.html" rel="next" title="quantize">
         Next
         <img class="next-page" src="_static/images/chevron-right-orange.svg"/>
        </a>
        <a accesskey="p" class="btn btn-neutral" href="quantization.html" rel="prev" title="Quantization">
         <img class="previous-page" src="_static/images/chevron-right-orange.svg"/>
         Previous
        </a>
       </div>
       <hr/>
       <div role="contentinfo">
        <p>
         © Copyright PyTorch Contributors.
        </p>
       </div>
       <div>
        Built with
        <a href="http://sphinx-doc.org/">
         Sphinx
        </a>
        using a
        <a href="https://github.com/rtfd/sphinx_rtd_theme">
         theme
        </a>
        provided by
        <a href="https://readthedocs.org">
         Read the Docs
        </a>
        .
       </div>
      </footer>
     </div>
     <script>
      var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
     </script>
    </div>
    <div class="pytorch-content-right" id="pytorch-content-right">
     <div class="pytorch-right-menu" id="pytorch-right-menu">
      <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
       <ul>
        <li>
         <a class="reference internal" href="#">
          Quantization API Reference
         </a>
         <ul>
          <li>
           <a class="reference internal" href="#torch-ao-quantization">
            torch.ao.quantization
           </a>
           <ul>
            <li>
             <a class="reference internal" href="#top-level-apis">
              Top level APIs
             </a>
            </li>
            <li>
             <a class="reference internal" href="#preparing-model-for-quantization">
              Preparing model for quantization
             </a>
            </li>
            <li>
             <a class="reference internal" href="#utility-functions">
              Utility functions
             </a>
            </li>
           </ul>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-quantize-fx">
            torch.ao.quantization.quantize_fx
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-qconfig-mapping">
            torch.ao.quantization.qconfig_mapping
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-backend-config">
            torch.ao.quantization.backend_config
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-fx-custom-config">
            torch.ao.quantization.fx.custom_config
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.quantization.quantizer">
            torch.ao.quantization.quantizer
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.quantization.pt2e">
            torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation)
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-pt2e-export-utils">
            torch.ao.quantization.pt2e.export_utils
           </a>
          </li>
          <li>
           <a class="reference internal" href="#pt2-export-pt2e-numeric-debugger">
            PT2 Export (pt2e) Numeric Debugger
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-quantization-related-functions">
            torch (quantization related functions)
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-tensor-quantization-related-methods">
            torch.Tensor (quantization related methods)
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-observer">
            torch.ao.quantization.observer
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-fake-quantize">
            torch.ao.quantization.fake_quantize
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-quantization-qconfig">
            torch.ao.quantization.qconfig
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.intrinsic">
            torch.ao.nn.intrinsic
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.intrinsic.qat">
            torch.ao.nn.intrinsic.qat
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.intrinsic.quantized">
            torch.ao.nn.intrinsic.quantized
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.intrinsic.quantized.dynamic">
            torch.ao.nn.intrinsic.quantized.dynamic
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.qat">
            torch.ao.nn.qat
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.qat.dynamic">
            torch.ao.nn.qat.dynamic
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.quantized.modules">
            torch.ao.nn.quantized
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.quantized.functional">
            torch.ao.nn.quantized.functional
           </a>
          </li>
          <li>
           <a class="reference internal" href="#torch-ao-nn-quantizable">
            torch.ao.nn.quantizable
           </a>
          </li>
          <li>
           <a class="reference internal" href="#module-torch.ao.nn.quantized.dynamic">
            torch.ao.nn.quantized.dynamic
           </a>
          </li>
          <li>
           <a class="reference internal" href="#quantized-dtypes-and-quantization-schemes">
            Quantized dtypes and quantization schemes
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </div>
     </div>
    </div>
   </section>
  </div>
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js" type="text/javascript">
  </script>
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js">
  </script>
  <script src="_static/jquery.js">
  </script>
  <script src="_static/underscore.js">
  </script>
  <script src="_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="_static/doctools.js">
  </script>
  <script src="_static/sphinx_highlight.js">
  </script>
  <script src="_static/clipboard.min.js">
  </script>
  <script src="_static/copybutton.js">
  </script>
  <script src="_static/js/vendor/popper.min.js" type="text/javascript">
  </script>
  <script src="_static/js/vendor/bootstrap.min.js" type="text/javascript">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js">
  </script>
  <script src="_static/js/theme.js" type="text/javascript">
  </script>
  <script type="text/javascript">
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <script script="" type="text/javascript">
   var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
  </script>
  <img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1">
   <!-- Begin Footer -->
   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
     <div class="row">
      <div class="col-md-4 text-center">
       <h2>
        Docs
       </h2>
       <p>
        Access comprehensive developer documentation for PyTorch
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">
        View Docs
       </a>
      </div>
      <div class="col-md-4 text-center">
       <h2>
        Tutorials
       </h2>
       <p>
        Get in-depth tutorials for beginners and advanced developers
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/tutorials">
        View Tutorials
       </a>
      </div>
      <div class="col-md-4 text-center">
       <h2>
        Resources
       </h2>
       <p>
        Find development resources and get your questions answered
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/resources">
        View Resources
       </a>
      </div>
     </div>
    </div>
   </div>
   <footer class="site-footer">
    <div class="container footer-container">
     <div class="footer-logo-wrapper">
      <a class="footer-logo" href="https://pytorch.org/">
      </a>
     </div>
     <div class="footer-links-wrapper">
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         <a href="https://pytorch.org/">
          PyTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/get-started">
          Get Started
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/features">
          Features
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/ecosystem">
          Ecosystem
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/blog/">
          Blog
         </a>
        </li>
        <li>
         <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">
          Contributing
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         <a href="https://pytorch.org/resources">
          Resources
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials">
          Tutorials
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/docs/stable/index.html">
          Docs
         </a>
        </li>
        <li>
         <a href="https://discuss.pytorch.org" target="_blank">
          Discuss
         </a>
        </li>
        <li>
         <a href="https://github.com/pytorch/pytorch/issues" target="_blank">
          Github Issues
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">
          Brand Guidelines
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         Stay up to date
        </li>
        <li>
         <a href="https://www.facebook.com/pytorch" target="_blank">
          Facebook
         </a>
        </li>
        <li>
         <a href="https://twitter.com/pytorch" target="_blank">
          Twitter
         </a>
        </li>
        <li>
         <a href="https://www.youtube.com/pytorch" target="_blank">
          YouTube
         </a>
        </li>
        <li>
         <a href="https://www.linkedin.com/company/pytorch" target="_blank">
          LinkedIn
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         PyTorch Podcasts
        </li>
        <li>
         <a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">
          Spotify
         </a>
        </li>
        <li>
         <a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">
          Apple
         </a>
        </li>
        <li>
         <a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">
          Google
         </a>
        </li>
        <li>
         <a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">
          Amazon
         </a>
        </li>
       </ul>
      </div>
     </div>
     <div class="privacy-policy">
      <ul>
       <li class="privacy-policy-links">
        <a href="https://www.linuxfoundation.org/terms/" target="_blank">
         Terms
        </a>
       </li>
       <li class="privacy-policy-links">
        |
       </li>
       <li class="privacy-policy-links">
        <a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">
         Privacy
        </a>
       </li>
      </ul>
     </div>
     <div class="copyright">
      <p>
       © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
       <a href="https://www.linuxfoundation.org/policies/">
        www.linuxfoundation.org/policies/
       </a>
       . The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see
       <a href="https://www.lfprojects.org/policies/">
        www.lfprojects.org/policies/
       </a>
       .
      </p>
     </div>
    </div>
   </footer>
   <div class="cookie-banner-wrapper">
    <div class="container">
     <p class="gdpr-notice">
      To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls:
      <a href="https://www.facebook.com/policies/cookies/">
       Cookies Policy
      </a>
      .
     </p>
     <img class="close-button" src="_static/images/pytorch-x.svg"/>
    </div>
   </div>
   <!-- End Footer -->
   <!-- Begin Mobile Menu -->
   <div class="mobile-main-menu">
    <div class="container-fluid">
     <div class="container">
      <div class="mobile-main-menu-header-container">
       <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
       </a>
       <a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
       </a>
      </div>
     </div>
    </div>
    <div class="mobile-main-menu-links-container">
     <div class="main-menu">
      <ul>
       <li class="resources-mobile-menu-title">
        <a>
         Learn
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/get-started">
          Get Started
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials">
          Tutorials
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          Learn the Basics
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          PyTorch Recipes
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/beginner/introyt.html">
          Introduction to PyTorch - YouTube Series
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Ecosystem
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/ecosystem">
          Tools
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/#community-module">
          Community
         </a>
        </li>
        <li>
         <a href="https://discuss.pytorch.org/">
          Forums
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/resources">
          Developer Resources
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/ecosystem/contributor-awards-2023">
          Contributor Awards - 2024
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Edge
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/edge">
          About PyTorch Edge
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/executorch-overview">
          ExecuTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/executorch/stable/index.html">
          ExecuTorch Documentation
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Docs
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/docs/stable/index.html">
          PyTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/pytorch-domains">
          PyTorch Domains
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Blog &amp; News
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/blog/">
          PyTorch Blog
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/community-blog">
          Community Blog
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/videos">
          Videos
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/community-stories">
          Community Stories
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/events">
          Events
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/newsletter">
          Newsletter
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         About
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/foundation">
          PyTorch Foundation
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/governing-board">
          Governing Board
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/credits">
          Cloud Credit Program
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tac">
          Technical Advisory Council
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/staff">
          Staff
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/contact-us">
          Contact Us
         </a>
        </li>
       </ul>
      </ul>
     </div>
    </div>
   </div>
   <!-- End Mobile Menu -->
   <script src="_static/js/vendor/anchor.min.js" type="text/javascript">
   </script>
   <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
   </script>
  </img>
 </body>
</html>
