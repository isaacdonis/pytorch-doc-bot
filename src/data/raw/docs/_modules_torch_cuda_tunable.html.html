<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta content="noindex" name="robots"/>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.cuda.tunable â€” PyTorch 2.7 documentation
  </title>
  <link href="https://pytorch.org/docs/stable/_modules/torch/cuda/tunable.html" rel="canonical"/>
  <link href="../../../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link href="../../../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/katex-math.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/sphinx-dropdown.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/panels-bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/css/jit.css" rel="stylesheet" type="text/css"/>
  <link href="../../../_static/css/custom.css" rel="stylesheet" type="text/css"/>
  <link href="../../../genindex.html" rel="index" title="Index"/>
  <link href="../../../search.html" rel="search" title="Search"/>
  <!-- Google Tag Manager -->
  <script>
   (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');
  </script>
  <!-- End Google Tag Manager -->
  <script src="../../../_static/js/modernizr.min.js">
  </script>
  <!-- Preload the theme fonts -->
  <link as="font" crossorigin="anonymous" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
  <!-- Preload the katex fonts -->
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
  <link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
 </head>
 <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
   <div class="header-container">
    <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
    </a>
    <div class="main-menu">
     <ul>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Learn
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
          <span class="dropdown-title">
           Get Started
          </span>
          <p>
           Run PyTorch locally or get started quickly with one of the supported cloud platforms
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
          <span class="dropdown-title">
           Tutorials
          </span>
          <p>
           Whats new in PyTorch tutorials
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          <span class="dropdown-title">
           Learn the Basics
          </span>
          <p>
           Familiarize yourself with PyTorch concepts and modules
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          <span class="dropdown-title">
           PyTorch Recipes
          </span>
          <p>
           Bite-size, ready-to-deploy PyTorch code examples
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
          <span class="dropdown-title">
           Intro to PyTorch - YouTube Series
          </span>
          <p>
           Master PyTorch basics with our engaging YouTube tutorial series
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Ecosystem
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
          <span class="dropdown-title">
           Tools
          </span>
          <p>
           Learn about the tools and frameworks in the PyTorch Ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
          <span class="dropdown-title">
           Community
          </span>
          <p>
           Join the PyTorch developer community to contribute, learn, and get your questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
          <span class="dropdown-title">
           Forums
          </span>
          <p>
           A place to discuss PyTorch code, issues, install, research
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/resources">
          <span class="dropdown-title">
           Developer Resources
          </span>
          <p>
           Find resources and get questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
          <span class="dropdown-title">
           Contributor Awards - 2024
          </span>
          <p>
           Award winners announced at this year's PyTorch Conference
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Edge
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/edge">
          <span class="dropdown-title">
           About PyTorch Edge
          </span>
          <p>
           Build innovative and privacy-aware AI experiences for edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
          <span class="dropdown-title">
           ExecuTorch
          </span>
          <p>
           End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
          <span class="dropdown-title">
           ExecuTorch Docs
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Docs
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
          <span class="dropdown-title">
           PyTorch
          </span>
          <p>
           Explore the documentation for comprehensive guidance on how to use PyTorch
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
          <span class="dropdown-title">
           PyTorch Domains
          </span>
          <p>
           Read the PyTorch Domains documentation to learn more about domain-specific libraries
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Blogs &amp; News
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
          <span class="dropdown-title">
           PyTorch Blog
          </span>
          <p>
           Catch up on the latest technical news and happenings
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
          <span class="dropdown-title">
           Community Blog
          </span>
          <p>
           Stories from the PyTorch ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/videos">
          <span class="dropdown-title">
           Videos
          </span>
          <p>
           Learn about the latest PyTorch tutorials, new, and more
          </p>
          <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
           <span class="dropdown-title">
            Community Stories
           </span>
           <p>
            Learn how our community solves real, everyday machine learning problems with PyTorch
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/events">
           <span class="dropdown-title">
            Events
           </span>
           <p>
            Find events, webinars, and podcasts
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
           <span class="dropdown-title">
            Newsletter
           </span>
           <p>
            Stay up-to-date with the latest updates
           </p>
          </a>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         About
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
          <span class="dropdown-title">
           PyTorch Foundation
          </span>
          <p>
           Learn more about the PyTorch Foundation
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
          <span class="dropdown-title">
           Governing Board
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/credits">
          <span class="dropdown-title">
           Cloud Credit Program
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tac">
          <span class="dropdown-title">
           Technical Advisory Council
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/staff">
          <span class="dropdown-title">
           Staff
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
          <span class="dropdown-title">
           Contact Us
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="no-dropdown">
        <a data-cta="join" href="https://pytorch.org/join">
         Become a Member
        </a>
       </div>
      </li>
      <li>
       <div class="main-menu-item">
        <a class="github-icon" href="https://github.com/pytorch/pytorch">
        </a>
       </div>
      </li>
      <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
     </ul>
    </div>
    <a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
    </a>
   </div>
  </div>
 </div>
 <body class="pytorch-body">
  <div class="table-of-contents-link-wrapper">
   <span>
    Table of Contents
   </span>
   <a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#">
   </a>
  </div>
  <nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
   <div class="pytorch-side-scroll">
    <div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
     <div class="pytorch-left-menu-search">
      <div class="version">
       <a href="https://pytorch.org/docs/versions.html">
        2.7 â–¼
       </a>
      </div>
      <div id="searchBox">
       <div class="searchbox" id="googleSearchBox">
        <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e">
        </script>
        <div class="gcse-search">
        </div>
       </div>
       <div id="sphinxSearchBox" style="display: none;">
        <div role="search">
         <form action="../../../search.html" class="wy-form" id="rtd-search-form" method="get">
          <input name="q" placeholder="Search Docs" type="text"/>
          <input name="check_keywords" type="hidden" value="yes"/>
          <input name="area" type="hidden" value="default"/>
         </form>
        </div>
       </div>
      </div>
      <form id="searchForm">
       <label style="margin-bottom: 1rem">
        <input checked="" name="searchType" type="radio" value="google"/>
        Google Search
       </label>
       <label style="margin-bottom: 1rem">
        <input name="searchType" type="radio" value="sphinx"/>
        Classic Search
       </label>
      </form>
      <script>
       document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
      </script>
     </div>
     <p class="caption" role="heading">
      <span class="caption-text">
       Community
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../community/build_ci_governance.html">
        PyTorch Governance | Build + CI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../community/contribution_guide.html">
        PyTorch Contribution Guide
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../community/design.html">
        PyTorch Design Philosophy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../community/governance.html">
        PyTorch Governance | Mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../community/persons_of_interest.html">
        PyTorch Governance | Maintainers
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Developer Notes
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/amp_examples.html">
        Automatic Mixed Precision examples
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/autograd.html">
        Autograd mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/broadcasting.html">
        Broadcasting semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">
        CPU threading and TorchScript inference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/cuda.html">
        CUDA semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/custom_operators.html">
        PyTorch Custom Operators Landing Page
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/ddp.html">
        Distributed Data Parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/extending.html">
        Extending PyTorch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/extending.func.html">
        Extending torch.func with autograd.Function
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/faq.html">
        Frequently Asked Questions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/fsdp.html">
        FSDP Notes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/get_start_xpu.html">
        Getting Started on Intel GPU
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/gradcheck.html">
        Gradcheck mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/hip.html">
        HIP (ROCm) semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/large_scale_deployments.html">
        Features for large-scale deployments
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/libtorch_stable_abi.html">
        LibTorch Stable ABI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/modules.html">
        Modules
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/mps.html">
        MPS backend
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/multiprocessing.html">
        Multiprocessing best practices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/numerical_accuracy.html">
        Numerical accuracy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/randomness.html">
        Reproducibility
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/serialization.html">
        Serialization semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../notes/windows.html">
        Windows FAQ
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Language Bindings
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../cpp_index.html">
        C++
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/javadoc/">
        Javadoc
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../deploy.html">
        torch::deploy
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Python API
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch.html">
        torch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../nn.html">
        torch.nn
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../nn.functional.html">
        torch.nn.functional
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../tensors.html">
        torch.Tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../tensor_attributes.html">
        Tensor Attributes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../tensor_view.html">
        Tensor Views
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../amp.html">
        torch.amp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../autograd.html">
        torch.autograd
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../library.html">
        torch.library
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../accelerator.html">
        torch.accelerator
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../cpu.html">
        torch.cpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../cuda.html">
        torch.cuda
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch_cuda_memory.html">
        Understanding CUDA Memory Usage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch_cuda_memory.html#generating-a-snapshot">
        Generating a Snapshot
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch_cuda_memory.html#using-the-visualizer">
        Using the visualizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch_cuda_memory.html#snapshot-api-reference">
        Snapshot API Reference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../mps.html">
        torch.mps
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../xpu.html">
        torch.xpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../mtia.html">
        torch.mtia
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../mtia.memory.html">
        torch.mtia.memory
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../meta.html">
        Meta device
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../backends.html">
        torch.backends
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../export.html">
        torch.export
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.html">
        torch.distributed
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.tensor.html">
        torch.distributed.tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.algorithms.join.html">
        torch.distributed.algorithms.join
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.elastic.html">
        torch.distributed.elastic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../fsdp.html">
        torch.distributed.fsdp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.fsdp.fully_shard.html">
        torch.distributed.fsdp.fully_shard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.tensor.parallel.html">
        torch.distributed.tensor.parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.optim.html">
        torch.distributed.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.pipelining.html">
        torch.distributed.pipelining
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributed.checkpoint.html">
        torch.distributed.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../distributions.html">
        torch.distributions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch.compiler.html">
        torch.compiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../fft.html">
        torch.fft
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../func.html">
        torch.func
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../futures.html">
        torch.futures
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../fx.html">
        torch.fx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../fx.experimental.html">
        torch.fx.experimental
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../hub.html">
        torch.hub
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../jit.html">
        torch.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../linalg.html">
        torch.linalg
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../monitor.html">
        torch.monitor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../signal.html">
        torch.signal
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../special.html">
        torch.special
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch.overrides.html">
        torch.overrides
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../package.html">
        torch.package
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../profiler.html">
        torch.profiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../nn.init.html">
        torch.nn.init
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../nn.attention.html">
        torch.nn.attention
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../onnx.html">
        torch.onnx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../optim.html">
        torch.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../complex_numbers.html">
        Complex Numbers
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../ddp_comm_hooks.html">
        DDP Communication Hooks
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../quantization.html">
        Quantization
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../rpc.html">
        Distributed RPC Framework
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../random.html">
        torch.random
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../masked.html">
        torch.masked
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../nested.html">
        torch.nested
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../size.html">
        torch.Size
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../sparse.html">
        torch.sparse
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../storage.html">
        torch.Storage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../testing.html">
        torch.testing
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../utils.html">
        torch.utils
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../benchmark_utils.html">
        torch.utils.benchmark
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../bottleneck.html">
        torch.utils.bottleneck
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../checkpoint.html">
        torch.utils.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../cpp_extension.html">
        torch.utils.cpp_extension
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../data.html">
        torch.utils.data
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../deterministic.html">
        torch.utils.deterministic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../jit_utils.html">
        torch.utils.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../dlpack.html">
        torch.utils.dlpack
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../mobile_optimizer.html">
        torch.utils.mobile_optimizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../model_zoo.html">
        torch.utils.model_zoo
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../tensorboard.html">
        torch.utils.tensorboard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../module_tracker.html">
        torch.utils.module_tracker
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../type_info.html">
        Type Info
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../named_tensor.html">
        Named Tensors
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../name_inference.html">
        Named Tensors operator coverage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../config_mod.html">
        torch.__config__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../future_mod.html">
        torch.__future__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../logging.html">
        torch._logging
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../../torch_environment_variables.html">
        Torch Environment Variables
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Libraries
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/audio/stable">
        torchaudio
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/data">
        TorchData
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/torchrec">
        TorchRec
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/serve">
        TorchServe
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/text/stable">
        torchtext
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/vision/stable">
        torchvision
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/xla/">
        PyTorch on XLA Devices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/ao">
        torchao
       </a>
      </li>
     </ul>
    </div>
   </div>
  </nav>
  <div class="pytorch-container">
   <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
    <div class="pytorch-breadcrumbs-wrapper">
     <div aria-label="breadcrumbs navigation" role="navigation">
      <ul class="pytorch-breadcrumbs">
       <li>
        <a href="../../../index.html">
         Docs
        </a>
        &gt;
       </li>
       <li>
        <a href="../../index.html">
         Module code
        </a>
        &gt;
       </li>
       <li>
        <a href="../../torch.html">
         torch
        </a>
        &gt;
       </li>
       <li>
        <a href="../cuda.html">
         torch.cuda
        </a>
        &gt;
       </li>
       <li>
        torch.cuda.tunable
       </li>
       <li class="pytorch-breadcrumbs-aside">
       </li>
      </ul>
     </div>
    </div>
    <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
     Shortcuts
    </div>
   </div>
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <!-- Google Tag Manager (noscript) -->
     <noscript>
      <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0">
      </iframe>
     </noscript>
     <!-- End Google Tag Manager (noscript) -->
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <h1>
         Source code for torch.cuda.tunable
        </h1>
        <div class="highlight">
         <pre>
<span></span><span class="sa">r</span><span class="sd">"""</span>
<span class="sd">This module exposes a TunableOp interface.</span>

<span class="sd">Some operations, such as GEMMs, could be implemented using more than one library</span>
<span class="sd">or more than one technique. For example, a GEMM could be implemented for CUDA or</span>
<span class="sd">ROCm using either the blas or blasLt libraries. Further, ROCm's rocblas and</span>
<span class="sd">hipblaslt libraries allow the user to query for all possible algorithms and then</span>
<span class="sd">choose one. How does one know which implementation is the fastest and should be</span>
<span class="sd">chosen? That's what TunableOp provides.</span>

<span class="sd">Enabling TunableOp and Tuning Separately</span>
<span class="sd">========================================</span>

<span class="sd">The TunableOp feature is enabled separately from enabling the tuning phase</span>
<span class="sd">itself. Enabling TunableOp means that PyTorch will replace any standard</span>
<span class="sd">operators with their Tunable implementations. Any call to a TunableOp first</span>
<span class="sd">checks whether it has already been tuned for the given operator inputs. If so,</span>
<span class="sd">it will immediately call the tuned operation; no further tuning will take place</span>
<span class="sd">even when the tuning setting is enabled. Instead if no tuning result is found,</span>
<span class="sd">and tuning is enabled, the TunableOp will benchmark every registered</span>
<span class="sd">implementation of that operator for the given set of inputs and select the</span>
<span class="sd">fastest.</span>

<span class="sd">File Input and Output</span>
<span class="sd">=====================</span>

<span class="sd">The first time any TunableOp is invoked, the internal database of tuned</span>
<span class="sd">operations will be prepared by attempting to read the results from the given</span>
<span class="sd">file. The default filename is 'tunableop_results.csv'. To support tuning when</span>
<span class="sd">multiple GPUs are used across multiple processes, the GPU device ordinal is</span>
<span class="sd">automatically inserted into the filename to avoid multiple processes overwriting</span>
<span class="sd">the same file.</span>

<span class="sd">If tuning is enabled and new tunings are discovered during the course of your</span>
<span class="sd">workload, it will also write out to this same filename with all tunings, both</span>
<span class="sd">the ones it read in at startup as well as the new ones found at runtime. This</span>
<span class="sd">can be used, for example, to build up a tunings file across many workloads by</span>
<span class="sd">reusing the same file. The output file is automatically created when the</span>
<span class="sd">application terminates. This behavior can be controlled by the C++ and Python</span>
<span class="sd">APIs but not the environment variables.</span>

<span class="sd">Assuming you specified a filename, you'll end up with a CSV file with contents</span>
<span class="sd">like so::</span>

<span class="sd">  Validator,PT_VERSION,2.2.0</span>
<span class="sd">  Validator,ROCM_VERSION,6.0.0.0-12969-1544e39</span>
<span class="sd">  Validator,HIPBLASLT_VERSION,0.6.0-a9c5cc7</span>
<span class="sd">  Validator,ROCBLAS_VERSION,4.0.0-72e57364-dirty</span>
<span class="sd">  GemmTunableOp_float_NT,nt_25088_4096_64,Gemm_Hipblaslt_1219,1.262</span>
<span class="sd">  GemmTunableOp_float_NT,nt_4096_4096_64,Gemm_Rocblas_1216,0.033</span>

<span class="sd">Note the "Validator" lines. If you change a library version, or ROCm version, or</span>
<span class="sd">PyTorch version, TunableOp will detect this and reject the tunings file because</span>
<span class="sd">the prior tunings are likely affected by other software changes.</span>

<span class="sd">The remaining lines are the tuned solutions for each TunableOp encountered</span>
<span class="sd">during your execution. Each line consists of 4 comma-separated fields: operator</span>
<span class="sd">name, operator parameters, solution name, and average execution time. The</span>
<span class="sd">execution time is an optional field. The CSV file can be edited, but with</span>
<span class="sd">caution. For example, the solution name (field 3) can be changed to "Default"</span>
<span class="sd">and it will fall back to the original PyTorch untuned implementation. Or, in the</span>
<span class="sd">case of ROCm's hipBLAS or hipBLASLt libraries, if you know the specific solution</span>
<span class="sd">index you can override the solution that TunableOp selected by replacing the</span>
<span class="sd">value. The operator name and parameters (fields 1 and 2) are internally named</span>
<span class="sd">and should not be modified. In the case of GemmTunableOp, field 1 indicates the</span>
<span class="sd">datatype and whether the inputs are transposed (T) or not (N) and field 2</span>
<span class="sd">indicates the M, N, K input shapes.</span>

<span class="sd">There is an option to enable verbose output but it is only recommended for</span>
<span class="sd">debugging purposes. This will produce a lot of diagnostic messages but may be</span>
<span class="sd">useful to see if TunableOp is being used at all. Otherwise, TunableOp is</span>
<span class="sd">completely silent, besides file output, unless there is a warning or error</span>
<span class="sd">during its use. The verbose option is only available by setting the environment</span>
<span class="sd">variable PYTORCH_TUNABLEOP_VEROBSE=1.</span>

<span class="sd">A Note on Tuning Behavior, Warmup, and Cache Effects</span>
<span class="sd">====================================================</span>

<span class="sd">Tuning an operator consists of iterating through the list or registered</span>
<span class="sd">implementations and profiling each one. The profile is established by running a</span>
<span class="sd">single implementation in a loop multiple times and taking the average execution</span>
<span class="sd">time. There is also an optional warmup phase prior to tuning that can help with</span>
<span class="sd">reaching stable power states by the hardware. During tuning of a workload the</span>
<span class="sd">various hardware caches will more likely produce hits than when not tuning.</span>
<span class="sd">There are options for flushing the instruction cache and rotate the input tensors</span>
<span class="sd">which might help produce a more faithful profile of the tuned operator as if the</span>
<span class="sd">operator were run within a larger workload instead of in a tight, repetitive loop.</span>

<span class="sd">By default, each possible solution for a given operator will be run for either</span>
<span class="sd">100 iterations or as many iterations that can be run within 30ms, whichever is</span>
<span class="sd">smaller, and its average execution will be calculated. The fastest solution</span>
<span class="sd">among all that were successfully profiled will be chosen. A profile might fail</span>
<span class="sd">if the given solution doesn't achieve the same accuracy as the default</span>
<span class="sd">implementation or if the solution returns an error code.</span>

<span class="sd">Current Tunable Operators</span>
<span class="sd">=========================</span>

<span class="sd">TunableGemm for ROCm</span>
<span class="sd">--------------------</span>

<span class="sd">Currently only a TunableGemm for ROCm is implemented. Note that CUDA builds of</span>
<span class="sd">PyTorch will function correctly when using TunableOp but the only solution</span>
<span class="sd">available to CUDA builds is the 'Default' implementation i.e. the original</span>
<span class="sd">cuBLAS default, now called through TunableOp. Any call to at::cuda::blas::gemm()</span>
<span class="sd">or ::bgemm() will be routed through TunableOp when enabled. Calling gemm() for a</span>
<span class="sd">given set of input arguments (transa, transb, m, n, k) will attempt to use the</span>
<span class="sd">fastest available implementation across both rocblas and hipblaslt.</span>

<span class="sd">Offline Tuning</span>
<span class="sd">==============</span>

<span class="sd">Motivation</span>
<span class="sd">----------</span>
<span class="sd">There are several use cases for offline tuning.</span>

<span class="sd">One use case involves a workload with a high-memory utilization, where regular tuning might lead to running out of memory.</span>

<span class="sd">Another use case is for compute-intensive workloads. In such cases, it is more resource-efficient to collect</span>
<span class="sd">the GEMMs for the workload once and then tune repeatedly with different tuning parameters or libraries.</span>

<span class="sd">Workflow</span>
<span class="sd">--------</span>
<span class="sd">There are basically two steps:</span>
<span class="sd">1) Set the environment variables to collect the untuned GEMM and this will generate ``tunableop_untuned0.csv``:</span>

<span class="sd">.. code-block:: python</span>

<span class="sd">   PYTORCH_TUNABLEOP_ENABLED=1</span>
<span class="sd">   PYTORCH_TUNABLEOP_TUNING=0</span>
<span class="sd">   PYTORCH_TUNABLEOP_RECORD_UNTUNED=1</span>
<span class="sd">   ...</span>

<span class="sd">2) Run a Python script that reads the ``tunableop_untuned0.csv`` and generates the ``tunableop_results0.csv``, like this:</span>

<span class="sd">.. code-block:: python</span>

<span class="sd">   import torch.cuda.tunable as tunable</span>
<span class="sd">   import os</span>

<span class="sd">   os.putenv('PYTORCH_TUNABLEOP_ENABLED', '1')</span>
<span class="sd">   os.putenv('PYTORCH_TUNABLEOP_TUNING', '1')</span>
<span class="sd">   os.putenv('PYTORCH_TUNABLEOP_RECORD_UNTUNED', '0')</span>
<span class="sd">   tunable.tune_gemm_in_file("tunableop_untuned0.csv")</span>


<span class="sd">It is also possible to take multiple untuned files and distribute the GEMMs for tuning to multiple GPUs</span>
<span class="sd">within a single node. In the first step, the GEMMs are first gathered and duplicate GEMMs are eliminated.</span>
<span class="sd">Next, the GEMMs are distributed to different GPUs for tuning. After all GEMMs are tuned, the results from</span>
<span class="sd">all the GPUs are then gathered into a single file whose base filename has ``_full0`` appended to it</span>
<span class="sd">(for example ``tunableop_results_full0.csv``). Finally, this new file, containing the gathered results, will be</span>
<span class="sd">duplicated N times, once for each GPU as convenience to the user will run the workload with the tuned</span>
<span class="sd">configuration on N GPUs.</span>

<span class="sd">.. code-block:: python</span>

<span class="sd">   if __name__ == "__main__":</span>
<span class="sd">       num_gpus = 8 # number of GPUs that will be used during the tuning process</span>
<span class="sd">       tunable.mgpu_tune_gemm_in_file("tunableop_untuned?.csv", num_gpus)</span>

<span class="sd">Note that the usage of the ``mgpu_tune_gemm_in_file`` API is different from its single GPU counterpart</span>
<span class="sd">(``tune_gemm_in_file``). The body of the Python script that calls the API must be wrapped in ``main()`` as shown</span>
<span class="sd">due to the use of concurrent futures module. The argument to ``mgpu_tune_gemm_in_file`` must contain a wild card</span>
<span class="sd">expression (``?`` or ``*``) to generate the list of untuned files containing the GEMMs to be processed. The ``num_gpus``</span>
<span class="sd">must between 1 and the total number of GPUs available.</span>

<span class="sd">Tuning Context</span>
<span class="sd">==============</span>

<span class="sd">The behavior of TunableOp is currently manipulated through environment</span>
<span class="sd">variables, the C++ interface of at::cuda::tunable::getTuningContext(), or the</span>
<span class="sd">torch.cuda.tunable python interfaces. The environment variables take precedence</span>
<span class="sd">over any setting you manipulate using the C++ or Python APIs.</span>

<span class="sd">Environment Variable Interface</span>
<span class="sd">------------------------------</span>
<span class="sd">Environment variables are cached the first time they are read. You cannot use the</span>
<span class="sd">environment variable interface programmatically since the settings become fixed.</span>
<span class="sd">Use the C++ or Python APIs instead.</span>

<span class="sd">"""</span>
<span class="kn">import</span> <span class="nn">concurrent.futures</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">torch</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"enable"</span><span class="p">,</span>
    <span class="s2">"is_enabled"</span><span class="p">,</span>
    <span class="s2">"tuning_enable"</span><span class="p">,</span>
    <span class="s2">"tuning_is_enabled"</span><span class="p">,</span>
    <span class="s2">"record_untuned_enable"</span><span class="p">,</span>
    <span class="s2">"record_untuned_is_enabled"</span><span class="p">,</span>
    <span class="s2">"set_max_tuning_duration"</span><span class="p">,</span>
    <span class="s2">"get_max_tuning_duration"</span><span class="p">,</span>
    <span class="s2">"set_max_tuning_iterations"</span><span class="p">,</span>
    <span class="s2">"get_max_tuning_iterations"</span><span class="p">,</span>
    <span class="s2">"set_filename"</span><span class="p">,</span>
    <span class="s2">"get_filename"</span><span class="p">,</span>
    <span class="s2">"get_results"</span><span class="p">,</span>
    <span class="s2">"get_validators"</span><span class="p">,</span>
    <span class="s2">"write_file_on_exit"</span><span class="p">,</span>
    <span class="s2">"write_file"</span><span class="p">,</span>
    <span class="s2">"read_file"</span><span class="p">,</span>
    <span class="s2">"tune_gemm_in_file"</span><span class="p">,</span>
    <span class="s2">"mgpu_tune_gemm_in_file"</span><span class="p">,</span>
    <span class="s2">"set_rotating_buffer_size"</span><span class="p">,</span>
    <span class="s2">"get_rotating_buffer_size"</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="enable"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.enable">[docs]</a><span class="k">def</span> <span class="nf">enable</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""This is the big on/off switch for all TunableOp implementations."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_enable</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="is_enabled"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.is_enabled">[docs]</a><span class="k">def</span> <span class="nf">is_enabled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Returns whether the TunableOp feature is enabled."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_is_enabled</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="tuning_enable"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.tuning_enable">[docs]</a><span class="k">def</span> <span class="nf">tuning_enable</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Enable tuning of TunableOp implementations.</span>

<span class="sd">    When enabled, if a tuned entry isn't found, run the tuning step and record</span>
<span class="sd">    the entry.</span>
<span class="sd">    """</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_tuning_enable</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="tuning_is_enabled"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.tuning_is_enabled">[docs]</a><span class="k">def</span> <span class="nf">tuning_is_enabled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Returns whether TunableOp implementations can be tuned."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_tuning_is_enabled</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="record_untuned_enable"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.record_untuned_enable">[docs]</a><span class="k">def</span> <span class="nf">record_untuned_enable</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Enable recording untuned of TunableOp perations for offline tuning.</span>

<span class="sd">    When enabled, if a tuned entry isn't found, write it to the untuned file.</span>
<span class="sd">    """</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_record_untuned_enable</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="record_untuned_is_enabled"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.record_untuned_is_enabled">[docs]</a><span class="k">def</span> <span class="nf">record_untuned_is_enabled</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Returns whether TunableOp operations are recorded for offline tuning."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_record_untuned_is_enabled</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="set_max_tuning_duration"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.set_max_tuning_duration">[docs]</a><span class="k">def</span> <span class="nf">set_max_tuning_duration</span><span class="p">(</span><span class="n">duration</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Set max time in milliseconds to spend tuning a given solution.</span>

<span class="sd">    If both max tuning duration and iterations are set, the smaller of the two</span>
<span class="sd">    will be honored. At minimum 1 tuning iteration will always be run.</span>
<span class="sd">    """</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_set_max_tuning_duration</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="get_max_tuning_duration"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.get_max_tuning_duration">[docs]</a><span class="k">def</span> <span class="nf">get_max_tuning_duration</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Get max time to spend tuning a given solution."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_get_max_tuning_duration</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="set_max_tuning_iterations"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.set_max_tuning_iterations">[docs]</a><span class="k">def</span> <span class="nf">set_max_tuning_iterations</span><span class="p">(</span><span class="n">iterations</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Set max number of iterations to spend tuning a given solution.</span>

<span class="sd">    If both max tuning duration and iterations are set, the smaller of the two</span>
<span class="sd">    will be honored. At minimum 1 tuning iteration will always be run.</span>
<span class="sd">    """</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_set_max_tuning_iterations</span><span class="p">(</span><span class="n">iterations</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="get_max_tuning_iterations"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.get_max_tuning_iterations">[docs]</a><span class="k">def</span> <span class="nf">get_max_tuning_iterations</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Get max iterations to spend tuning a given solution."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_get_max_tuning_iterations</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="set_filename"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.set_filename">[docs]</a><span class="k">def</span> <span class="nf">set_filename</span><span class="p">(</span><span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">insert_device_ordinal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Set the filename to use for input/output of tuning results.</span>

<span class="sd">    If :attr:`insert_device_ordinal` is ``True`` then the current device ordinal</span>
<span class="sd">    will be added to the given filename automatically. This can be used in a</span>
<span class="sd">    1-process-per-gpu cenario to ensure all processes write to a separate file.</span>
<span class="sd">    """</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_set_filename</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">insert_device_ordinal</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="get_filename"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.get_filename">[docs]</a><span class="k">def</span> <span class="nf">get_filename</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Get the results filename."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_get_filename</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="get_results"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.get_results">[docs]</a><span class="k">def</span> <span class="nf">get_results</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Return all TunableOp results."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_get_results</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="get_validators"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.get_validators">[docs]</a><span class="k">def</span> <span class="nf">get_validators</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Return the TunableOp validators."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_get_validators</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="write_file_on_exit"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.write_file_on_exit">[docs]</a><span class="k">def</span> <span class="nf">write_file_on_exit</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""During Tuning Context destruction, write file to disk.</span>

<span class="sd">    This is useful as a final flush of your results to disk if your application</span>
<span class="sd">    terminates as result of normal operation or an error. Manual flushing of</span>
<span class="sd">    your results can be achieved by manually calling ``write_file()``."""</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_write_file_on_exit</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="write_file"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.write_file">[docs]</a><span class="k">def</span> <span class="nf">write_file</span><span class="p">(</span><span class="n">filename</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Write results to a CSV file.</span>

<span class="sd">    If :attr:`filename` is not given, ``get_filename()`` is called.</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">get_filename</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_write_file</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="read_file"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.read_file">[docs]</a><span class="k">def</span> <span class="nf">read_file</span><span class="p">(</span><span class="n">filename</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Read results from a TunableOp CSV file.</span>

<span class="sd">    If :attr:`filename` is not given, ``get_filename()`` is called.</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">get_filename</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_read_file</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="set_rotating_buffer_size"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.set_rotating_buffer_size">[docs]</a><span class="k">def</span> <span class="nf">set_rotating_buffer_size</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Set rotating buffer size to this value in MB, if the buffer size is greater than zero.</span>

<span class="sd">    If less than zero, query L2 cache size. If equal to zero, means deactivate rotating buffer.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_set_rotating_buffer_size</span><span class="p">(</span><span class="n">buffer_size</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="get_rotating_buffer_size"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.get_rotating_buffer_size">[docs]</a><span class="k">def</span> <span class="nf">get_rotating_buffer_size</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Get the rotating buffer size in kilobytes."""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_tunableop_get_rotating_buffer_size</span><span class="p">()</span>  <span class="c1"># type: ignore[attr-defined]</span></div>


<div class="viewcode-block" id="tune_gemm_in_file"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.tune_gemm_in_file">[docs]</a><span class="k">def</span> <span class="nf">tune_gemm_in_file</span><span class="p">(</span><span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""tune GEMM in file."""</span>

    <span class="k">assert</span> <span class="n">is_enabled</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">tuning_is_enabled</span><span class="p">()</span>

    <span class="n">deviceid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">startswith</span><span class="p">((</span><span class="s2">"Gemm"</span><span class="p">,</span> <span class="s2">"ScaledGemm"</span><span class="p">)):</span>
                <span class="n">_process_single_offline_gemm</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">deviceid</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_gather_unique_untuned_gemm_from_files</span><span class="p">(</span><span class="n">filename_pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">set</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Process multiple untuned results file and return a set with duplicates removed."""</span>
    <span class="n">unique_gemm_entries</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># set will avoid duplicates</span>

    <span class="k">for</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">filename_pattern</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">startswith</span><span class="p">((</span><span class="s2">"Gemm"</span><span class="p">,</span> <span class="s2">"ScaledGemm"</span><span class="p">)):</span>
                    <span class="n">unique_gemm_entries</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">unique_gemm_entries</span>


<span class="k">def</span> <span class="nf">_gather_tunableop_results</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Gather results from multiple tunableop results file and create a single file."""</span>
    <span class="n">gemm_lines</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">validator_lines</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Need to allow for the possibility that results filename was</span>
    <span class="c1"># set with the Python API instead of with environment variable.</span>
    <span class="c1"># Also possible that results filename was not set at all.</span>
    <span class="c1"># There are several test cases to check, but ultimately we</span>
    <span class="c1"># need a glob-able expression</span>
    <span class="n">results_filename</span> <span class="o">=</span> <span class="n">get_filename</span><span class="p">()</span>  <span class="c1"># Note empty string could be returned here</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">results_filename</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">results_filename</span> <span class="o">!=</span> <span class="s2">""</span>
    <span class="p">):</span>  <span class="c1"># Case were the Python API was used to set the filename</span>
        <span class="n">dot_pos</span> <span class="o">=</span> <span class="n">results_filename</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s2">"."</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dot_pos</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">dot_pos</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Replace the character just to the left of the dot</span>
            <span class="n">filename_pattern</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">results_filename</span><span class="p">[:</span> <span class="n">dot_pos</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">"?"</span> <span class="o">+</span> <span class="n">results_filename</span><span class="p">[</span><span class="n">dot_pos</span><span class="p">:]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">filename_pattern</span> <span class="o">=</span> <span class="s2">""</span>  <span class="c1"># Needed to make linter happy</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Case where the environment variable was used to set the filename.</span>
        <span class="n">results_filename_env</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"PYTORCH_TUNABLEOP_FILENAME"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">results_filename_env</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">results_filename_env</span> <span class="o">==</span> <span class="s2">""</span><span class="p">:</span>
            <span class="n">filename_pattern</span> <span class="o">=</span> <span class="s2">"tunableop_results?.csv"</span>
        <span class="k">elif</span> <span class="s2">"</span><span class="si">%d</span><span class="s2">"</span> <span class="ow">in</span> <span class="n">results_filename_env</span><span class="p">:</span>
            <span class="n">filename_pattern</span> <span class="o">=</span> <span class="n">results_filename_env</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"</span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"?"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">filename_pattern</span> <span class="o">=</span> <span class="n">results_filename_env</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"."</span><span class="p">,</span> <span class="s2">"?."</span><span class="p">)</span>

    <span class="k">assert</span> <span class="s2">"?"</span> <span class="ow">in</span> <span class="n">filename_pattern</span>

    <span class="n">FirstFile</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">matching_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">filename_pattern</span><span class="p">)</span>
    <span class="n">num_matching_files</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">matching_files</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">matching_files</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"Validator"</span><span class="p">):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">FirstFile</span><span class="p">):</span>
                        <span class="c1"># Only read Validator from first file</span>
                        <span class="n">validator_lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">gemm_lines</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

        <span class="n">FirstFile</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="n">output_file</span> <span class="o">=</span> <span class="n">filename_pattern</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"?"</span><span class="p">,</span> <span class="s2">"_full0"</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">out_file</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">validator_lines</span><span class="p">:</span>
            <span class="n">out_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">gemm_lines</span><span class="p">:</span>
            <span class="n">out_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

    <span class="c1"># Create num_matching_copies of the results file</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_matching_files</span><span class="p">):</span>
        <span class="n">duplicate_file</span> <span class="o">=</span> <span class="n">output_file</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">"0"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">duplicate_file</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_process_single_offline_gemm</span><span class="p">(</span><span class="n">untuned_gemm_line</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">gpu_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Process a single untuned GEMM."""</span>

    <span class="n">deviceid</span> <span class="o">=</span> <span class="s2">"cuda:"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gpu_id</span><span class="p">)</span>

    <span class="n">dtype_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"float"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="s2">"double"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
        <span class="s2">"BFloat16"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
        <span class="s2">"Half"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span>
        <span class="s2">"c10::complex&lt;double&gt;"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">,</span>
        <span class="s2">"c10::complex&lt;float&gt;"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span>
        <span class="s2">"Float8_e4m3fn"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
        <span class="s2">"Float8_e5m2"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">,</span>
        <span class="s2">"Float8_e4m3fnuz"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">,</span>
        <span class="s2">"Float8_e5m2fnuz"</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">untuned_gemm</span> <span class="o">=</span> <span class="n">untuned_gemm_line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">)[:]</span>

    <span class="n">underscore_count</span> <span class="o">=</span> <span class="n">untuned_gemm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s2">"_"</span><span class="p">)</span>

    <span class="c1"># Initialize dtype to make linter happy</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dtypeA</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dtypeB</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">dtypeC</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">underscore_count</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="p">[</span><span class="n">op_sig</span><span class="p">,</span> <span class="n">data_type</span><span class="p">,</span> <span class="n">layout</span><span class="p">]</span> <span class="o">=</span> <span class="n">untuned_gemm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"_"</span><span class="p">)</span>
        <span class="n">transA</span> <span class="o">=</span> <span class="n">layout</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"T"</span>
        <span class="n">transB</span> <span class="o">=</span> <span class="n">layout</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"T"</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># ScaledGEMM</span>
        <span class="n">untuned_gemm_temp</span> <span class="o">=</span> <span class="n">untuned_gemm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"_"</span><span class="p">)</span>
        <span class="c1"># dtypeC = might not be FP8 type, keep track</span>
        <span class="c1"># of the the number of underscores</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s2">"_"</span><span class="p">)</span>
        <span class="n">op_sig</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">data_typeA</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="s2">"_"</span> <span class="o">+</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">data_typeB</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="s2">"_"</span> <span class="o">+</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">7</span><span class="p">:</span>
            <span class="n">data_typeC</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="s2">"_"</span> <span class="o">+</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_typeC</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
        <span class="n">transA</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="n">count</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"T"</span>
        <span class="n">transB</span> <span class="o">=</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="n">count</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"T"</span>
        <span class="n">dtypeA</span> <span class="o">=</span> <span class="n">dtype_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_typeA</span><span class="p">)</span>
        <span class="n">dtypeB</span> <span class="o">=</span> <span class="n">dtype_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_typeB</span><span class="p">)</span>
        <span class="n">dtypeC</span> <span class="o">=</span> <span class="n">dtype_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_typeC</span><span class="p">)</span>

    <span class="n">untuned_gemm_temp</span> <span class="o">=</span> <span class="n">untuned_gemm</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"_"</span><span class="p">)</span>
    <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">4</span><span class="p">]]</span>
    <span class="k">if</span> <span class="n">op_sig</span> <span class="o">==</span> <span class="s2">"GemmTunableOp"</span><span class="p">:</span>
        <span class="n">matA</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">transB</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">matB</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">transA</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">matA</span><span class="p">,</span> <span class="n">matB</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">op_sig</span> <span class="o">==</span> <span class="s2">"GemmStridedBatchedTunableOp"</span><span class="p">:</span>
        <span class="p">[</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">6</span><span class="p">]]</span>
        <span class="n">matA</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">transB</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">matB</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">transA</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">matA</span> <span class="o">=</span> <span class="n">matA</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">transB</span> <span class="k">else</span> <span class="n">matA</span>
        <span class="n">matB</span> <span class="o">=</span> <span class="n">matB</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">transA</span> <span class="k">else</span> <span class="n">matB</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">matA</span><span class="p">,</span> <span class="n">matB</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">op_sig</span> <span class="o">==</span> <span class="s2">"ScaledGemmTunableOp"</span><span class="p">:</span>
        <span class="n">fillA</span> <span class="o">=</span> <span class="mf">0.25</span>
        <span class="n">fillB</span> <span class="o">=</span> <span class="mf">0.75</span>
        <span class="n">matA</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">fillA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypeA</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">transB</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">fillA</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypeA</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">matB</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">fillB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypeB</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">transA</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">fillB</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypeB</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"rw"</span>
        <span class="k">if</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"1"</span><span class="p">:</span>
            <span class="n">rowwise</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rowwise</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">rowwise</span><span class="p">:</span>
            <span class="n">scaleA</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">matA</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="n">scaleB</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">matB</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scaleA</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="n">scaleB</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"bias"</span>
        <span class="k">if</span> <span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">11</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"None"</span><span class="p">:</span>  <span class="c1"># no bias vector</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_scaled_mm</span><span class="p">(</span>
                <span class="n">matA</span><span class="p">,</span> <span class="n">matB</span><span class="p">,</span> <span class="n">scale_a</span><span class="o">=</span><span class="n">scaleA</span><span class="p">,</span> <span class="n">scale_b</span><span class="o">=</span><span class="n">scaleB</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="n">dtypeC</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># bias vector present</span>
            <span class="n">fillbias</span> <span class="o">=</span> <span class="mf">0.10</span>
            <span class="n">bias_dtype</span> <span class="o">=</span> <span class="n">dtype_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">untuned_gemm_temp</span><span class="p">[</span><span class="mi">11</span><span class="p">])</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">n</span><span class="p">,),</span> <span class="n">fillbias</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">bias_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">transA</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">m</span><span class="p">,),</span> <span class="n">fillbias</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">bias_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_scaled_mm</span><span class="p">(</span>
                <span class="n">matA</span><span class="p">,</span> <span class="n">matB</span><span class="p">,</span> <span class="n">scale_a</span><span class="o">=</span><span class="n">scaleA</span><span class="p">,</span> <span class="n">scale_b</span><span class="o">=</span><span class="n">scaleB</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="n">dtypeC</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span>
            <span class="p">)</span>

    <span class="k">elif</span> <span class="n">op_sig</span> <span class="o">==</span> <span class="s2">"GemmAndBiasTunableOp"</span><span class="p">:</span>
        <span class="c1"># y = x*A^T + b</span>
        <span class="k">assert</span> <span class="n">transA</span> <span class="o">!=</span> <span class="n">transB</span>

        <span class="n">X</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">transB</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">matA</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">transA</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">transA</span>
            <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">deviceid</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">matA</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s2">"error: unknown op </span><span class="si">{</span><span class="n">op_sig</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_tuning_assertions</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Helper function for multi-GPU tuning case. Need to check that TunableOp feature</span>
<span class="sd">    is enabled and that tuning is enabled.</span>
<span class="sd">    """</span>

    <span class="k">if</span> <span class="n">is_enabled</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">"TunableOp was disabled. Trying to enable now."</span><span class="p">)</span>
        <span class="n">enable</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">is_enabled</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">True</span>
    <span class="k">assert</span> <span class="n">tuning_is_enabled</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">True</span>
    <span class="k">assert</span> <span class="n">record_untuned_is_enabled</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">False</span>


<div class="viewcode-block" id="mgpu_tune_gemm_in_file"><a class="viewcode-back" href="../../../cuda.tunable.html#torch.cuda.tunable.mgpu_tune_gemm_in_file">[docs]</a><span class="k">def</span> <span class="nf">mgpu_tune_gemm_in_file</span><span class="p">(</span><span class="n">filename_pattern</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_gpus</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">"""Process one or more files and distribute work over one or more GPUs."""</span>
    <span class="n">unique_gemm_entries</span> <span class="o">=</span> <span class="n">_gather_unique_untuned_gemm_from_files</span><span class="p">(</span><span class="n">filename_pattern</span><span class="p">)</span>

    <span class="n">total_gpus</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>

    <span class="k">assert</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">num_gpus</span> <span class="o">&lt;=</span> <span class="n">total_gpus</span>

    <span class="n">mp_context</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">"spawn"</span><span class="p">)</span>

    <span class="n">futures</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># empty list to hold futures</span>
    <span class="n">flush_results</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># empty list to hold futures</span>

    <span class="c1"># GEMM are assigned to GPUs in a round robin manner</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ProcessPoolExecutor</span><span class="p">(</span>
        <span class="n">max_workers</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">,</span>
        <span class="n">mp_context</span><span class="o">=</span><span class="n">mp_context</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">_check_tuning_assertions</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="c1"># The workers are a separate process. TunableOp will be</span>
        <span class="c1"># enabled in the child processes if PYTORCH_TUNABLEOP_ENABLED=1</span>
        <span class="c1"># In the initializer, we also try to enable TunableOP if th</span>
        <span class="c1"># environment variable was NOT set.</span>

        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">unique_gemm_entries</span><span class="p">:</span>
            <span class="n">future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">_process_single_offline_gemm</span><span class="p">,</span> <span class="n">line</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
            <span class="n">futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_gpus</span>

        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">as_completed</span><span class="p">(</span><span class="n">futures</span><span class="p">):</span>
            <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">):</span>
            <span class="n">flush_result</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">write_file</span><span class="p">)</span>
            <span class="n">flush_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flush_result</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">flush_result</span> <span class="ow">in</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">as_completed</span><span class="p">(</span><span class="n">flush_results</span><span class="p">):</span>
            <span class="n">flush_result</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

    <span class="n">_gather_tunableop_results</span><span class="p">()</span></div>
</pre>
        </div>
       </article>
      </div>
      <footer>
       <hr/>
       <div role="contentinfo">
        <p>
         Â© Copyright PyTorch Contributors.
        </p>
       </div>
       <div>
        Built with
        <a href="http://sphinx-doc.org/">
         Sphinx
        </a>
        using a
        <a href="https://github.com/rtfd/sphinx_rtd_theme">
         theme
        </a>
        provided by
        <a href="https://readthedocs.org">
         Read the Docs
        </a>
        .
       </div>
      </footer>
     </div>
     <script>
      var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
     </script>
    </div>
    <div class="pytorch-content-right" id="pytorch-content-right">
     <div class="pytorch-right-menu" id="pytorch-right-menu">
      <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
      </div>
     </div>
    </div>
   </section>
  </div>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js" type="text/javascript">
  </script>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js">
  </script>
  <script src="../../../_static/jquery.js">
  </script>
  <script src="../../../_static/underscore.js">
  </script>
  <script src="../../../_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="../../../_static/doctools.js">
  </script>
  <script src="../../../_static/sphinx_highlight.js">
  </script>
  <script src="../../../_static/clipboard.min.js">
  </script>
  <script src="../../../_static/copybutton.js">
  </script>
  <script src="../../../_static/js/vendor/popper.min.js" type="text/javascript">
  </script>
  <script src="../../../_static/js/vendor/bootstrap.min.js" type="text/javascript">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js">
  </script>
  <script src="../../../_static/js/theme.js" type="text/javascript">
  </script>
  <script type="text/javascript">
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <script script="" type="text/javascript">
   var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
  </script>
  <img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1"/>
  <!-- Begin Footer -->
  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
   <div class="container">
    <div class="row">
     <div class="col-md-4 text-center">
      <h2>
       Docs
      </h2>
      <p>
       Access comprehensive developer documentation for PyTorch
      </p>
      <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">
       View Docs
      </a>
     </div>
     <div class="col-md-4 text-center">
      <h2>
       Tutorials
      </h2>
      <p>
       Get in-depth tutorials for beginners and advanced developers
      </p>
      <a class="with-right-arrow" href="https://pytorch.org/tutorials">
       View Tutorials
      </a>
     </div>
     <div class="col-md-4 text-center">
      <h2>
       Resources
      </h2>
      <p>
       Find development resources and get your questions answered
      </p>
      <a class="with-right-arrow" href="https://pytorch.org/resources">
       View Resources
      </a>
     </div>
    </div>
   </div>
  </div>
  <footer class="site-footer">
   <div class="container footer-container">
    <div class="footer-logo-wrapper">
     <a class="footer-logo" href="https://pytorch.org/">
     </a>
    </div>
    <div class="footer-links-wrapper">
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        <a href="https://pytorch.org/">
         PyTorch
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/get-started">
         Get Started
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/features">
         Features
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/ecosystem">
         Ecosystem
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/blog/">
         Blog
        </a>
       </li>
       <li>
        <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">
         Contributing
        </a>
       </li>
      </ul>
     </div>
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        <a href="https://pytorch.org/resources">
         Resources
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials">
         Tutorials
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/docs/stable/index.html">
         Docs
        </a>
       </li>
       <li>
        <a href="https://discuss.pytorch.org" target="_blank">
         Discuss
        </a>
       </li>
       <li>
        <a href="https://github.com/pytorch/pytorch/issues" target="_blank">
         Github Issues
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">
         Brand Guidelines
        </a>
       </li>
      </ul>
     </div>
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        Stay up to date
       </li>
       <li>
        <a href="https://www.facebook.com/pytorch" target="_blank">
         Facebook
        </a>
       </li>
       <li>
        <a href="https://twitter.com/pytorch" target="_blank">
         Twitter
        </a>
       </li>
       <li>
        <a href="https://www.youtube.com/pytorch" target="_blank">
         YouTube
        </a>
       </li>
       <li>
        <a href="https://www.linkedin.com/company/pytorch" target="_blank">
         LinkedIn
        </a>
       </li>
      </ul>
     </div>
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        PyTorch Podcasts
       </li>
       <li>
        <a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">
         Spotify
        </a>
       </li>
       <li>
        <a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">
         Apple
        </a>
       </li>
       <li>
        <a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">
         Google
        </a>
       </li>
       <li>
        <a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">
         Amazon
        </a>
       </li>
      </ul>
     </div>
    </div>
    <div class="privacy-policy">
     <ul>
      <li class="privacy-policy-links">
       <a href="https://www.linuxfoundation.org/terms/" target="_blank">
        Terms
       </a>
      </li>
      <li class="privacy-policy-links">
       |
      </li>
      <li class="privacy-policy-links">
       <a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">
        Privacy
       </a>
      </li>
     </ul>
    </div>
    <div class="copyright">
     <p>
      Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
      <a href="https://www.linuxfoundation.org/policies/">
       www.linuxfoundation.org/policies/
      </a>
      . The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see
      <a href="https://www.lfprojects.org/policies/">
       www.lfprojects.org/policies/
      </a>
      .
     </p>
    </div>
   </div>
  </footer>
  <div class="cookie-banner-wrapper">
   <div class="container">
    <p class="gdpr-notice">
     To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls:
     <a href="https://www.facebook.com/policies/cookies/">
      Cookies Policy
     </a>
     .
    </p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg"/>
   </div>
  </div>
  <!-- End Footer -->
  <!-- Begin Mobile Menu -->
  <div class="mobile-main-menu">
   <div class="container-fluid">
    <div class="container">
     <div class="mobile-main-menu-header-container">
      <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
      </a>
      <a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
      </a>
     </div>
    </div>
   </div>
   <div class="mobile-main-menu-links-container">
    <div class="main-menu">
     <ul>
      <li class="resources-mobile-menu-title">
       <a>
        Learn
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/get-started">
         Get Started
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials">
         Tutorials
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">
         Learn the Basics
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">
         PyTorch Recipes
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials/beginner/introyt.html">
         Introduction to PyTorch - YouTube Series
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Ecosystem
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/ecosystem">
         Tools
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/#community-module">
         Community
        </a>
       </li>
       <li>
        <a href="https://discuss.pytorch.org/">
         Forums
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/resources">
         Developer Resources
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/ecosystem/contributor-awards-2023">
         Contributor Awards - 2024
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Edge
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/edge">
         About PyTorch Edge
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/executorch-overview">
         ExecuTorch
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/executorch/stable/index.html">
         ExecuTorch Documentation
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Docs
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/docs/stable/index.html">
         PyTorch
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/pytorch-domains">
         PyTorch Domains
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Blog &amp; News
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/blog/">
         PyTorch Blog
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/community-blog">
         Community Blog
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/videos">
         Videos
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/community-stories">
         Community Stories
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/events">
         Events
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/newsletter">
         Newsletter
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        About
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/foundation">
         PyTorch Foundation
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/governing-board">
         Governing Board
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/credits">
         Cloud Credit Program
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tac">
         Technical Advisory Council
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/staff">
         Staff
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/contact-us">
         Contact Us
        </a>
       </li>
      </ul>
     </ul>
    </div>
   </div>
  </div>
  <!-- End Mobile Menu -->
  <script src="../../../_static/js/vendor/anchor.min.js" type="text/javascript">
  </script>
  <script type="text/javascript">
   $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
 </body>
</html>
