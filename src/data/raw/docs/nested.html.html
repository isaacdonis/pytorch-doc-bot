<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.nested — PyTorch 2.7 documentation
  </title>
  <link href="https://pytorch.org/docs/stable/nested.html" rel="canonical"/>
  <link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link href="_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="_static/katex-math.css" rel="stylesheet" type="text/css"/>
  <link href="_static/sphinx-dropdown.css" rel="stylesheet" type="text/css"/>
  <link href="_static/panels-bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/jit.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/custom.css" rel="stylesheet" type="text/css"/>
  <link href="genindex.html" rel="index" title="Index"/>
  <link href="search.html" rel="search" title="Search"/>
  <link href="size.html" rel="next" title="torch.Size"/>
  <link href="masked.html" rel="prev" title="torch.masked"/>
  <!-- Google Tag Manager -->
  <script>
   (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');
  </script>
  <!-- End Google Tag Manager -->
  <script src="_static/js/modernizr.min.js">
  </script>
  <!-- Preload the theme fonts -->
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
  <!-- Preload the katex fonts -->
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
  <link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
 </head>
 <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
   <div class="header-container">
    <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
    </a>
    <div class="main-menu">
     <ul>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Learn
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
          <span class="dropdown-title">
           Get Started
          </span>
          <p>
           Run PyTorch locally or get started quickly with one of the supported cloud platforms
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
          <span class="dropdown-title">
           Tutorials
          </span>
          <p>
           Whats new in PyTorch tutorials
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          <span class="dropdown-title">
           Learn the Basics
          </span>
          <p>
           Familiarize yourself with PyTorch concepts and modules
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          <span class="dropdown-title">
           PyTorch Recipes
          </span>
          <p>
           Bite-size, ready-to-deploy PyTorch code examples
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
          <span class="dropdown-title">
           Intro to PyTorch - YouTube Series
          </span>
          <p>
           Master PyTorch basics with our engaging YouTube tutorial series
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Ecosystem
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
          <span class="dropdown-title">
           Tools
          </span>
          <p>
           Learn about the tools and frameworks in the PyTorch Ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
          <span class="dropdown-title">
           Community
          </span>
          <p>
           Join the PyTorch developer community to contribute, learn, and get your questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
          <span class="dropdown-title">
           Forums
          </span>
          <p>
           A place to discuss PyTorch code, issues, install, research
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/resources">
          <span class="dropdown-title">
           Developer Resources
          </span>
          <p>
           Find resources and get questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
          <span class="dropdown-title">
           Contributor Awards - 2024
          </span>
          <p>
           Award winners announced at this year's PyTorch Conference
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Edge
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/edge">
          <span class="dropdown-title">
           About PyTorch Edge
          </span>
          <p>
           Build innovative and privacy-aware AI experiences for edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
          <span class="dropdown-title">
           ExecuTorch
          </span>
          <p>
           End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
          <span class="dropdown-title">
           ExecuTorch Docs
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Docs
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
          <span class="dropdown-title">
           PyTorch
          </span>
          <p>
           Explore the documentation for comprehensive guidance on how to use PyTorch
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
          <span class="dropdown-title">
           PyTorch Domains
          </span>
          <p>
           Read the PyTorch Domains documentation to learn more about domain-specific libraries
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Blogs &amp; News
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
          <span class="dropdown-title">
           PyTorch Blog
          </span>
          <p>
           Catch up on the latest technical news and happenings
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
          <span class="dropdown-title">
           Community Blog
          </span>
          <p>
           Stories from the PyTorch ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/videos">
          <span class="dropdown-title">
           Videos
          </span>
          <p>
           Learn about the latest PyTorch tutorials, new, and more
          </p>
          <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
           <span class="dropdown-title">
            Community Stories
           </span>
           <p>
            Learn how our community solves real, everyday machine learning problems with PyTorch
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/events">
           <span class="dropdown-title">
            Events
           </span>
           <p>
            Find events, webinars, and podcasts
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
           <span class="dropdown-title">
            Newsletter
           </span>
           <p>
            Stay up-to-date with the latest updates
           </p>
          </a>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         About
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
          <span class="dropdown-title">
           PyTorch Foundation
          </span>
          <p>
           Learn more about the PyTorch Foundation
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
          <span class="dropdown-title">
           Governing Board
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/credits">
          <span class="dropdown-title">
           Cloud Credit Program
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tac">
          <span class="dropdown-title">
           Technical Advisory Council
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/staff">
          <span class="dropdown-title">
           Staff
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
          <span class="dropdown-title">
           Contact Us
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="no-dropdown">
        <a data-cta="join" href="https://pytorch.org/join">
         Become a Member
        </a>
       </div>
      </li>
      <li>
       <div class="main-menu-item">
        <a class="github-icon" href="https://github.com/pytorch/pytorch">
        </a>
       </div>
      </li>
      <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
     </ul>
    </div>
    <a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
    </a>
   </div>
  </div>
 </div>
 <body class="pytorch-body">
  <div class="table-of-contents-link-wrapper">
   <span>
    Table of Contents
   </span>
   <a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#">
   </a>
  </div>
  <nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
   <div class="pytorch-side-scroll">
    <div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
     <div class="pytorch-left-menu-search">
      <div class="version">
       <a href="https://pytorch.org/docs/versions.html">
        2.7 ▼
       </a>
      </div>
      <div id="searchBox">
       <div class="searchbox" id="googleSearchBox">
        <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e">
        </script>
        <div class="gcse-search">
        </div>
       </div>
       <div id="sphinxSearchBox" style="display: none;">
        <div role="search">
         <form action="search.html" class="wy-form" id="rtd-search-form" method="get">
          <input name="q" placeholder="Search Docs" type="text"/>
          <input name="check_keywords" type="hidden" value="yes"/>
          <input name="area" type="hidden" value="default"/>
         </form>
        </div>
       </div>
      </div>
      <form id="searchForm">
       <label style="margin-bottom: 1rem">
        <input checked="" name="searchType" type="radio" value="google"/>
        Google Search
       </label>
       <label style="margin-bottom: 1rem">
        <input name="searchType" type="radio" value="sphinx"/>
        Classic Search
       </label>
      </form>
      <script>
       document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
      </script>
     </div>
     <p class="caption" role="heading">
      <span class="caption-text">
       Community
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="community/build_ci_governance.html">
        PyTorch Governance | Build + CI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/contribution_guide.html">
        PyTorch Contribution Guide
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/design.html">
        PyTorch Design Philosophy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/governance.html">
        PyTorch Governance | Mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/persons_of_interest.html">
        PyTorch Governance | Maintainers
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Developer Notes
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/amp_examples.html">
        Automatic Mixed Precision examples
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/autograd.html">
        Autograd mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/broadcasting.html">
        Broadcasting semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">
        CPU threading and TorchScript inference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/cuda.html">
        CUDA semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/custom_operators.html">
        PyTorch Custom Operators Landing Page
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/ddp.html">
        Distributed Data Parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/extending.html">
        Extending PyTorch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/extending.func.html">
        Extending torch.func with autograd.Function
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/faq.html">
        Frequently Asked Questions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/fsdp.html">
        FSDP Notes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/get_start_xpu.html">
        Getting Started on Intel GPU
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/gradcheck.html">
        Gradcheck mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/hip.html">
        HIP (ROCm) semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/large_scale_deployments.html">
        Features for large-scale deployments
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/libtorch_stable_abi.html">
        LibTorch Stable ABI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/modules.html">
        Modules
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/mps.html">
        MPS backend
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/multiprocessing.html">
        Multiprocessing best practices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/numerical_accuracy.html">
        Numerical accuracy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/randomness.html">
        Reproducibility
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/serialization.html">
        Serialization semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/windows.html">
        Windows FAQ
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Language Bindings
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="cpp_index.html">
        C++
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/javadoc/">
        Javadoc
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="deploy.html">
        torch::deploy
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Python API
      </span>
     </p>
     <ul class="current">
      <li class="toctree-l1">
       <a class="reference internal" href="torch.html">
        torch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.html">
        torch.nn
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.functional.html">
        torch.nn.functional
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensors.html">
        torch.Tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensor_attributes.html">
        Tensor Attributes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensor_view.html">
        Tensor Views
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="amp.html">
        torch.amp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="autograd.html">
        torch.autograd
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="library.html">
        torch.library
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="accelerator.html">
        torch.accelerator
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cpu.html">
        torch.cpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cuda.html">
        torch.cuda
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html">
        Understanding CUDA Memory Usage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">
        Generating a Snapshot
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">
        Using the visualizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">
        Snapshot API Reference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mps.html">
        torch.mps
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="xpu.html">
        torch.xpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mtia.html">
        torch.mtia
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mtia.memory.html">
        torch.mtia.memory
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="meta.html">
        Meta device
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="backends.html">
        torch.backends
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="export.html">
        torch.export
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.html">
        torch.distributed
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.tensor.html">
        torch.distributed.tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.algorithms.join.html">
        torch.distributed.algorithms.join
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.elastic.html">
        torch.distributed.elastic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fsdp.html">
        torch.distributed.fsdp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.fsdp.fully_shard.html">
        torch.distributed.fsdp.fully_shard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.tensor.parallel.html">
        torch.distributed.tensor.parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.optim.html">
        torch.distributed.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.pipelining.html">
        torch.distributed.pipelining
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.checkpoint.html">
        torch.distributed.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributions.html">
        torch.distributions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch.compiler.html">
        torch.compiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fft.html">
        torch.fft
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="func.html">
        torch.func
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="futures.html">
        torch.futures
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fx.html">
        torch.fx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fx.experimental.html">
        torch.fx.experimental
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="hub.html">
        torch.hub
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="jit.html">
        torch.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="linalg.html">
        torch.linalg
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="monitor.html">
        torch.monitor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="signal.html">
        torch.signal
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="special.html">
        torch.special
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch.overrides.html">
        torch.overrides
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="package.html">
        torch.package
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="profiler.html">
        torch.profiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.init.html">
        torch.nn.init
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.attention.html">
        torch.nn.attention
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="onnx.html">
        torch.onnx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="optim.html">
        torch.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="complex_numbers.html">
        Complex Numbers
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="ddp_comm_hooks.html">
        DDP Communication Hooks
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="quantization.html">
        Quantization
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="rpc.html">
        Distributed RPC Framework
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="random.html">
        torch.random
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="masked.html">
        torch.masked
       </a>
      </li>
      <li class="toctree-l1 current">
       <a class="current reference internal" href="#">
        torch.nested
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="size.html">
        torch.Size
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="sparse.html">
        torch.sparse
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="storage.html">
        torch.Storage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="testing.html">
        torch.testing
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="utils.html">
        torch.utils
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="benchmark_utils.html">
        torch.utils.benchmark
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="bottleneck.html">
        torch.utils.bottleneck
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="checkpoint.html">
        torch.utils.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cpp_extension.html">
        torch.utils.cpp_extension
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="data.html">
        torch.utils.data
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="deterministic.html">
        torch.utils.deterministic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="jit_utils.html">
        torch.utils.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="dlpack.html">
        torch.utils.dlpack
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mobile_optimizer.html">
        torch.utils.mobile_optimizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="model_zoo.html">
        torch.utils.model_zoo
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensorboard.html">
        torch.utils.tensorboard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="module_tracker.html">
        torch.utils.module_tracker
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="type_info.html">
        Type Info
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="named_tensor.html">
        Named Tensors
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="name_inference.html">
        Named Tensors operator coverage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="config_mod.html">
        torch.__config__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="future_mod.html">
        torch.__future__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="logging.html">
        torch._logging
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_environment_variables.html">
        Torch Environment Variables
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Libraries
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/audio/stable">
        torchaudio
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/data">
        TorchData
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/torchrec">
        TorchRec
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/serve">
        TorchServe
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/text/stable">
        torchtext
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/vision/stable">
        torchvision
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/xla/">
        PyTorch on XLA Devices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/ao">
        torchao
       </a>
      </li>
     </ul>
    </div>
   </div>
  </nav>
  <div class="pytorch-container">
   <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
    <div class="pytorch-breadcrumbs-wrapper">
     <div aria-label="breadcrumbs navigation" role="navigation">
      <ul class="pytorch-breadcrumbs">
       <li>
        <a href="index.html">
         Docs
        </a>
        &gt;
       </li>
       <li>
        torch.nested
       </li>
       <li class="pytorch-breadcrumbs-aside">
        <a href="_sources/nested.rst.txt" rel="nofollow">
         <img src="_static/images/view-page-source-icon.svg"/>
        </a>
       </li>
      </ul>
     </div>
    </div>
    <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
     Shortcuts
    </div>
   </div>
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <!-- Google Tag Manager (noscript) -->
     <noscript>
      <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0">
      </iframe>
     </noscript>
     <!-- End Google Tag Manager (noscript) -->
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="module-torch.nested">
         <span id="torch-nested">
         </span>
         <h1>
          torch.nested
          <a class="headerlink" href="#module-torch.nested" title="Permalink to this heading">
           ¶
          </a>
         </h1>
         <div class="section" id="introduction">
          <h2>
           Introduction
           <a class="headerlink" href="#introduction" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <div class="admonition warning">
           <p class="admonition-title">
            Warning
           </p>
           <p>
            The PyTorch API of nested tensors is in prototype stage and will change in the near future.
           </p>
          </div>
          <p>
           Nested tensors allow for ragged-shaped data to be contained within and operated upon as a
single tensor. Such data is stored underneath in an efficient packed representation, while exposing
a standard PyTorch tensor interface for applying operations.
          </p>
          <p>
           A common application of nested tensors is for expressing batches of variable-length sequential data
present in various domains, such as varying sentence lengths, image sizes, and audio / video clip
lengths. Traditionally, such data has been handled by padding sequences to that of the max length
within a batch, performing computation on the padded form, and subsequently masking to remove
padding. This is inefficient and error-prone, and nested tensors exist to address these problems.
          </p>
          <p>
           The API for calling operations on a nested tensor is no different from that of a regular
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.Tensor
            </span>
           </code>
           , allowing for seamless integration with existing models, with the main
difference being
           <a class="reference internal" href="#construction">
            <span class="std std-ref">
             construction of the inputs
            </span>
           </a>
           .
          </p>
          <p>
           As this is a prototype feature, the set of
           <a class="reference internal" href="#supported-operations">
            <span class="std std-ref">
             operations supported
            </span>
           </a>
           is
limited, but growing. We welcome issues, feature requests, and contributions.
More information on contributing can be found
           <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/nested/README.md">
            in this Readme
           </a>
           .
          </p>
         </div>
         <div class="section" id="construction">
          <span id="id1">
          </span>
          <h2>
           Construction
           <a class="headerlink" href="#construction" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <div class="admonition note">
           <p class="admonition-title">
            Note
           </p>
           <p>
            There are two forms of nested tensors present within PyTorch, distinguished by layout as
specified during construction. Layout can be one of
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.strided
             </span>
            </code>
            or
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.jagged
             </span>
            </code>
            .
We recommend utilizing the
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.jagged
             </span>
            </code>
            layout whenever possible. While it currently only
supports a single ragged dimension, it has better op coverage, receives active development, and
integrates well with
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.compile
             </span>
            </code>
            . These docs adhere to this recommendation and refer to
nested tensors with the
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.jagged
             </span>
            </code>
            layout as “NJTs” for brevity throughout.
           </p>
          </div>
          <p>
           Construction is straightforward and involves passing a list of tensors to the
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.nested.nested_tensor
            </span>
           </code>
           constructor. A nested tensor with the
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.jagged
            </span>
           </code>
           layout
(AKA an “NJT”) supports a single ragged dimension. This constructor will copy the input tensors
into a packed, contiguous block of memory according to the layout described in the
           <a class="reference internal" href="#data-layout">
            data_layout
           </a>
           section below.
          </p>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span>
<span class="go">tensor([0, 1, 2])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span>
<span class="go">tensor([3, 4, 5, 6, 7])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">([</span><span class="n">component</span> <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="n">nt</span><span class="p">])</span>
<span class="go">[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]</span>
</pre>
           </div>
          </div>
          <p>
           Each tensor in the list must have the same number of dimensions, but the shapes can otherwise vary
along a single dimension. If the dimensionalities of the input components don’t match, the
constructor throws an error.
          </p>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># 2D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># 3D tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">...</span>
<span class="go">RuntimeError: When constructing a nested tensor, all tensors in list must have the same dim</span>
</pre>
           </div>
          </div>
          <p>
           During construction, dtype, device, and whether gradients are required can be chosen via the
usual keyword arguments.
          </p>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">([</span><span class="n">component</span> <span class="k">for</span> <span class="n">component</span> <span class="ow">in</span> <span class="n">nt</span><span class="p">])</span>
<span class="go">[tensor([0., 1., 2.], device='cuda:0',</span>
<span class="go">       grad_fn=&lt;UnbindBackwardAutogradNestedTensor0&gt;), tensor([3., 4., 5., 6., 7.], device='cuda:0',</span>
<span class="go">       grad_fn=&lt;UnbindBackwardAutogradNestedTensor0&gt;)]</span>
</pre>
           </div>
          </div>
          <p>
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.nested.as_nested_tensor
            </span>
           </code>
           can be used to preserve autograd history from the tensors passed
to the constructor. When this constructor is utilized, gradients will flow through the nested tensor
back into the original components. Note that this constructor still copies the input components into
a packed, contiguous block of memory.
          </p>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        ...,</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([[1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        ...,</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.],</span>
<span class="go">        [1., 1., 1.,  ..., 1., 1., 1.]])</span>
</pre>
           </div>
          </div>
          <p>
           The above functions all create contiguous NJTs, where a chunk of memory is allocated to store
a packed form of the underlying components (see the
           <a class="reference internal" href="#data-layout">
            data_layout
           </a>
           section below for more
details).
          </p>
          <p>
           It is also possible to create a non-contiguous NJT view over a pre-existing dense tensor
with padding, avoiding the memory allocation and copying.
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.nested.narrow()
            </span>
           </code>
           is the tool
for accomplishing this.
          </p>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">padded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">seq_lens</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j1, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
</pre>
           </div>
          </div>
          <p>
           Note that the nested tensor acts as a view over the original padded dense tensor, referencing the
same memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more
limited, so if you run into support gaps, it’s always possible to convert to a contiguous NJT
using
           <code class="docutils literal notranslate">
            <span class="pre">
             contiguous()
            </span>
           </code>
           .
          </p>
         </div>
         <div class="section" id="data-layout-and-shape">
          <span id="data-layout">
          </span>
          <h2>
           Data Layout and Shape
           <a class="headerlink" href="#data-layout-and-shape" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           For efficiency, nested tensors generally pack their tensor components into a contiguous chunk of
memory and maintain additional metadata to specify batch item boundaries. For the
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.jagged
            </span>
           </code>
           layout, the contiguous chunk of memory is stored in the
           <code class="docutils literal notranslate">
            <span class="pre">
             values
            </span>
           </code>
           component, with the
           <code class="docutils literal notranslate">
            <span class="pre">
             offsets
            </span>
           </code>
           component delineating batch item boundaries for the ragged dimension.
          </p>
          <img alt="_images/njt_visual.png" src="_images/njt_visual.png">
           <p>
            It’s possible to directly access the underlying NJT components when necessary.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># text 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="c1"># text 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">values</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># note the "packing" of the ragged dimension; no padding needed</span>
<span class="go">torch.Size([82, 128])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span>
<span class="go">tensor([ 0, 50, 82])</span>
</pre>
            </div>
           </div>
           <p>
            It can also be useful to construct an NJT from the jagged
            <code class="docutils literal notranslate">
             <span class="pre">
              values
             </span>
            </code>
            and
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            constituents directly; the
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.nested.nested_tensor_from_jagged()
             </span>
            </code>
            constructor serves
this purpose.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">82</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">82</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="n">offsets</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            An NJT has a well-defined shape with dimensionality 1 greater than that of its components. The
underlying structure of the ragged dimension is represented by a symbolic value (
            <code class="docutils literal notranslate">
             <span class="pre">
              j1
             </span>
            </code>
            in the
example below).
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 128])</span>
</pre>
            </div>
           </div>
           <p>
            NJTs must have the same ragged structure to be compatible with each other. For example, to run a
binary operation involving two NJTs, the ragged structures must match (i.e. they must have the
same ragged shape symbol in their shapes). In the details, each symbol corresponds with an exact
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            tensor, so both NJTs must have the same
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            tensor to be compatible with
each other.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span> <span class="ow">is</span> <span class="n">nt2</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt3</span> <span class="o">=</span> <span class="n">nt1</span> <span class="o">+</span> <span class="n">nt2</span>
<span class="go">RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)</span>
</pre>
            </div>
           </div>
           <p>
            In the above example, even though the conceptual shapes of the two NJTs are the same, they don’t
share a reference to the same
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            tensor, so their shapes differ, and they are not
compatible. We recognize that this behavior is unintuitive and are working hard to relax this
restriction for the beta release of nested tensors. For a workaround, see the
            <a class="reference internal" href="#ragged-structure-incompatibility">
             <span class="std std-ref">
              Troubleshooting
             </span>
            </a>
            section of this document.
           </p>
           <p>
            In addition to the
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            metadata, NJTs can also compute and cache the minimum and maximum
sequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA).
There are currently no public APIs for accessing these, but this will change for the beta release.
           </p>
          </img>
         </div>
         <div class="section" id="supported-operations">
          <span id="id2">
          </span>
          <h2>
           Supported Operations
           <a class="headerlink" href="#supported-operations" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This section contains a list of common operations over nested tensors that you may find useful.
It is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While
a sizeable subset of these are supported for nested tensors today, full support is a large task.
The ideal state for nested tensors is full support of all PyTorch operations that are available
for non-nested tensors. To help us accomplish this, please consider:
          </p>
          <ul class="simple">
           <li>
            <p>
             Requesting particular ops needed for your use case
             <a class="reference external" href="https://github.com/pytorch/pytorch/issues/118107">
              here
             </a>
             to help us prioritize.
            </p>
           </li>
           <li>
            <p>
             Contributing! It’s not too hard to add nested tensor support for a given PyTorch op; see
the
             <a class="reference external" href="contributions">
              Contributions
             </a>
             section below for details.
            </p>
           </li>
          </ul>
          <div class="section" id="viewing-nested-tensor-constituents">
           <h3>
            Viewing nested tensor constituents
            <a class="headerlink" href="#viewing-nested-tensor-constituents" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <p>
            <code class="docutils literal notranslate">
             <span class="pre">
              unbind()
             </span>
            </code>
            allows you to retrieve a view of the nested tensor’s constituents.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span>
<span class="go">(tensor([[-0.9916, -0.3363, -0.2799],</span>
<span class="go">        [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104,  1.4841],</span>
<span class="go">        [ 2.0952,  0.2973,  0.2516],</span>
<span class="go">        [ 0.9035,  1.3623,  0.2026]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">a</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="go">tensor([[ 3.6858, -3.7030, -4.4525],</span>
<span class="go">        [-2.3481,  2.0236,  0.1975]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span>
<span class="go">(tensor([[-2.9747, -1.0089, -0.8396],</span>
<span class="go">        [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104,  1.4841],</span>
<span class="go">        [ 2.0952,  0.2973,  0.2516],</span>
<span class="go">        [ 0.9035,  1.3623,  0.2026]]))</span>
</pre>
            </div>
           </div>
           <p>
            Note that
            <code class="docutils literal notranslate">
             <span class="pre">
              nt.unbind()[0]
             </span>
            </code>
            is not a copy, but rather a slice of the underlying memory, which
represents the first entry or constituent of the nested tensor.
           </p>
          </div>
          <div class="section" id="conversions-to-from-padded">
           <h3>
            Conversions to / from padded
            <a class="headerlink" href="#conversions-to-from-padded" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <p>
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.nested.to_padded_tensor()
             </span>
            </code>
            converts an NJT to a padded dense tensor with the specified
padding value. The ragged dimension will be padded out to the size of the maximum sequence length.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mf">4.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span>
<span class="go">tensor([[[ 1.6107,  0.5723,  0.3913],</span>
<span class="go">         [ 0.0700, -0.4954,  1.8663],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000],</span>
<span class="go">         [ 4.2000,  4.2000,  4.2000]],</span>
<span class="go">        [[-0.0479, -0.7610, -0.3484],</span>
<span class="go">         [ 1.1345,  1.0556,  0.3634],</span>
<span class="go">         [-1.7122, -0.5921,  0.0540],</span>
<span class="go">         [-0.5506,  0.7608,  2.0606],</span>
<span class="go">         [ 1.5658, -1.1934,  0.3041],</span>
<span class="go">         [ 0.1483, -1.1284,  0.6957]]])</span>
</pre>
            </div>
           </div>
           <p>
            This can be useful as an escape hatch to work around NJT support gaps, but ideally such
conversions should be avoided when possible for optimal memory usage and performance, as the
more efficient nested tensor layout does not materialize padding.
           </p>
           <p>
            The reverse conversion can be accomplished using
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.nested.narrow()
             </span>
            </code>
            , which applies
ragged structure to a given dense tensor to produce an NJT. Note that by default, this operation
does not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be
useful to explicitly call
            <code class="docutils literal notranslate">
             <span class="pre">
              contiguous()
             </span>
            </code>
            here if a contiguous NJT is desired.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">padded</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq_lens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">padded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">seq_lens</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j1, 4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">nt</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j2, 4])</span>
</pre>
            </div>
           </div>
          </div>
          <div class="section" id="shape-manipulations">
           <h3>
            Shape manipulations
            <a class="headerlink" href="#shape-manipulations" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <p>
            Nested tensors support a wide array of operations for shape manipulation, including views.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 6, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 2, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">nt</span><span class="p">,</span> <span class="n">nt</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 12])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">nt</span><span class="p">,</span> <span class="n">nt</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 2, 6])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, 6, j1])</span>
</pre>
            </div>
           </div>
          </div>
          <div class="section" id="attention-mechanisms">
           <h3>
            Attention mechanisms
            <a class="headerlink" href="#attention-mechanisms" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <p>
            As variable-length sequences are common inputs to attention mechanisms, nested tensors support
important attention operators
            <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">
             Scaled Dot Product Attention (SDPA)
            </a>
            and
            <a class="reference external" href="https://pytorch.org/docs/stable/nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention">
             FlexAttention
            </a>
            .
See
            <a class="reference external" href="https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html#multiheadattention">
             here
            </a>
            for usage examples of NJT with SDPA and
            <a class="reference external" href="https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html#flexattention-njt">
             here
            </a>
            for usage examples of NJT with FlexAttention.
           </p>
          </div>
         </div>
         <div class="section" id="usage-with-torch-compile">
          <span id="id3">
          </span>
          <h2>
           Usage with torch.compile
           <a class="headerlink" href="#usage-with-torch-compile" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           NJTs are designed to be used with
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.compile()
            </span>
           </code>
           for optimal performance, and we always
recommend utilizing
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.compile()
            </span>
           </code>
           with NJTs when possible. NJTs work out-of-the-box and
graph-break-free both when passed as inputs to a compiled function or module OR when
instantiated in-line within the function.
          </p>
          <div class="admonition note">
           <p class="admonition-title">
            Note
           </p>
           <p>
            If you’re not able to utilize
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.compile()
             </span>
            </code>
            for your use case, performance and memory
usage may still benefit from the use of NJTs, but it’s not as clear-cut whether this will be
the case. It is important that the tensors being operated on are large enough so the
performance gains are not outweighed by the overhead of python tensor subclasses.
           </p>
          </div>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 3])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span> <span class="o">=</span> <span class="n">compiled_g</span><span class="p">(</span><span class="n">nt</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 3])</span>
</pre>
           </div>
          </div>
          <p>
           Note that NJTs support
           <a class="reference external" href="https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html">
            Dynamic Shapes
           </a>
           to avoid unnecessary recompiles with changing ragged structure.
          </p>
          <div class="doctest highlight-default notranslate">
           <div class="highlight">
            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output1</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output2</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt2</span><span class="p">)</span>  <span class="c1"># NB: No recompile needed even though ragged structure differs</span>
</pre>
           </div>
          </div>
          <p>
           If you run into problems or arcane errors when utilizing NJT +
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.compile
            </span>
           </code>
           , please file a
PyTorch issue. Full subclass support within
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.compile
            </span>
           </code>
           is a long-term effort and there may
be some rough edges at this time.
          </p>
         </div>
         <div class="section" id="troubleshooting">
          <span id="id4">
          </span>
          <h2>
           Troubleshooting
           <a class="headerlink" href="#troubleshooting" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           This section contains common errors that you may run into when utilizing nested tensors, alongside
the reason for these errors and suggestions for how to address them.
          </p>
          <div class="section" id="unimplemented-ops">
           <span id="unimplemented-op">
           </span>
           <h3>
            Unimplemented ops
            <a class="headerlink" href="#unimplemented-ops" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <p>
            This error is becoming rarer as nested tensor op support grows, but it’s still possible to hit it
today given that there are a couple thousand ops within PyTorch.
           </p>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="ne">NotImplementedError</span><span class="p">:</span> <span class="n">aten</span><span class="o">.</span><span class="n">view_as_real</span><span class="o">.</span><span class="n">default</span>
</pre>
            </div>
           </div>
           <p>
            The error is straightforward; we haven’t gotten around to adding op support for this particular op
yet. If you’d like, you can
            <a class="reference external" href="contributions">
             contribute
            </a>
            an implementation yourself OR simply
            <a class="reference external" href="https://github.com/pytorch/pytorch/issues/118107">
             request
            </a>
            that we add support for this op
in a future PyTorch release.
           </p>
          </div>
          <div class="section" id="ragged-structure-incompatibility">
           <span id="id5">
           </span>
           <h3>
            Ragged structure incompatibility
            <a class="headerlink" href="#ragged-structure-incompatibility" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">cannot</span> <span class="n">call</span> <span class="n">binary</span> <span class="n">pointwise</span> <span class="n">function</span> <span class="n">add</span><span class="o">.</span><span class="n">Tensor</span> <span class="k">with</span> <span class="n">inputs</span> <span class="n">of</span> <span class="n">shapes</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">j2</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">j3</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            This error occurs when calling an op that operates over multiple NJTs with incompatible ragged
structures. Currently, it is required that input NJTs have the exact same
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            constituent
in order to have the same symbolic ragged structure symbol (e.g.
            <code class="docutils literal notranslate">
             <span class="pre">
              j1
             </span>
            </code>
            ).
           </p>
           <p>
            As a workaround for this situation, it is possible to construct NJTs from the
            <code class="docutils literal notranslate">
             <span class="pre">
              values
             </span>
            </code>
            and
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            components directly. With both NJTs referencing the same
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            components, they
are considered to have the same ragged structure and are thus compatible.
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">82</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">offsets</span><span class="o">=</span><span class="n">nt1</span><span class="o">.</span><span class="n">offsets</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt3</span> <span class="o">=</span> <span class="n">nt1</span> <span class="o">+</span> <span class="n">nt2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt3</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([2, j1, 128])</span>
</pre>
            </div>
           </div>
          </div>
          <div class="section" id="data-dependent-operation-within-torch-compile">
           <h3>
            Data dependent operation within torch.compile
            <a class="headerlink" href="#data-dependent-operation-within-torch-compile" title="Permalink to this heading">
             ¶
            </a>
           </h3>
           <div class="highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">exc</span><span class="o">.</span><span class="n">Unsupported</span><span class="p">:</span> <span class="n">data</span> <span class="n">dependent</span> <span class="n">operator</span><span class="p">:</span> <span class="n">aten</span><span class="o">.</span><span class="n">_local_scalar_dense</span><span class="o">.</span><span class="n">default</span><span class="p">;</span> <span class="n">to</span> <span class="n">enable</span><span class="p">,</span> <span class="nb">set</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_scalar_outputs</span> <span class="o">=</span> <span class="kc">True</span>
</pre>
            </div>
           </div>
           <p>
            This error occurs when calling an op that does data-dependent operation within torch.compile; this
commonly occurs for ops that need to examine the values of the NJT’s
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            to determine the
output shape. For example:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">nt</span><span class="p">):</span> <span class="k">return</span> <span class="n">nt</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">compiled_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">fullgraph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">compiled_f</span><span class="p">(</span><span class="n">nt</span><span class="p">)</span>
</pre>
            </div>
           </div>
           <p>
            In this example, calling
            <code class="docutils literal notranslate">
             <span class="pre">
              chunk()
             </span>
            </code>
            on the batch dimension of the NJT requires examination of the
NJT’s
            <code class="docutils literal notranslate">
             <span class="pre">
              offsets
             </span>
            </code>
            data to delineate batch item boundaries within the packed ragged dimension. As a
workaround, there are a couple torch.compile flags that can be set:
           </p>
           <div class="doctest highlight-default notranslate">
            <div class="highlight">
             <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_dynamic_output_shape_ops</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">capture_scalar_outputs</span> <span class="o">=</span> <span class="kc">True</span>
</pre>
            </div>
           </div>
           <p>
            If, after setting these, you still see data-dependent operator errors, please file an issue with
PyTorch. This area of
            <code class="docutils literal notranslate">
             <span class="pre">
              torch.compile()
             </span>
            </code>
            is still in heavy development and certain aspects of
NJT support may be incomplete.
           </p>
          </div>
         </div>
         <div class="section" id="contributions">
          <span id="id6">
          </span>
          <h2>
           Contributions
           <a class="headerlink" href="#contributions" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           If you’d like to contribute to nested tensor development, one of the most impactful ways to do
so is to add nested tensor support for a currently-unsupported PyTorch op. This process generally
consists of a couple simple steps:
          </p>
          <ol class="arabic simple">
           <li>
            <p>
             Determine the name of the op to add; this should be something like
             <code class="docutils literal notranslate">
              <span class="pre">
               aten.view_as_real.default
              </span>
             </code>
             .
The signature for this op can be found in
             <code class="docutils literal notranslate">
              <span class="pre">
               aten/src/ATen/native/native_functions.yaml
              </span>
             </code>
             .
            </p>
           </li>
           <li>
            <p>
             Register an op implementation in
             <code class="docutils literal notranslate">
              <span class="pre">
               torch/nested/_internal/ops.py
              </span>
             </code>
             , following the pattern
established there for other ops. Use the signature from
             <code class="docutils literal notranslate">
              <span class="pre">
               native_functions.yaml
              </span>
             </code>
             for schema
validation.
            </p>
           </li>
          </ol>
          <p>
           The most common way to implement an op is to unwrap the NJT into its constituents, redispatch the
op on the underlying
           <code class="docutils literal notranslate">
            <span class="pre">
             values
            </span>
           </code>
           buffer, and propagate the relevant NJT metadata (including
           <code class="docutils literal notranslate">
            <span class="pre">
             offsets
            </span>
           </code>
           ) to a new output NJT. If the output of the op is expected to have a different shape
from the input, new
           <code class="docutils literal notranslate">
            <span class="pre">
             offsets
            </span>
           </code>
           , etc. metadata must be computed.
          </p>
          <p>
           When an op is applied over the batch or ragged dimension, these tricks can help quickly get a
working implementation:
          </p>
          <ul class="simple">
           <li>
            <p>
             For
             <em>
              non-batchwise
             </em>
             operation, an
             <code class="docutils literal notranslate">
              <span class="pre">
               unbind()
              </span>
             </code>
             -based fallback should work.
            </p>
           </li>
           <li>
            <p>
             For operation on the ragged dimension, consider converting to padded dense with a properly-selected
padding value that won’t negatively bias the output, running the op, and converting back to NJT.
Within
             <code class="docutils literal notranslate">
              <span class="pre">
               torch.compile
              </span>
             </code>
             , these conversions can be fused to avoid materializing the padded
intermediate.
            </p>
           </li>
          </ul>
         </div>
         <div class="section" id="detailed-docs-for-construction-and-conversion-functions">
          <span id="construction-and-conversion">
          </span>
          <h2>
           Detailed Docs for Construction and Conversion Functions
           <a class="headerlink" href="#detailed-docs-for-construction-and-conversion-functions" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <dl class="py function">
           <dt class="sig sig-object py" id="torch.nested.nested_tensor">
            <span class="sig-prename descclassname">
             <span class="pre">
              torch.nested.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              nested_tensor
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               tensor_list
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="o">
              <span class="pre">
               *
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               dtype
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layout
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               device
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               requires_grad
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               pin_memory
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               False
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/nested.html#nested_tensor">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nested/__init__.py#L210">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.nested.nested_tensor" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Constructs a nested tensor with no autograd history (also known as a “leaf tensor”, see
             <a class="reference internal" href="notes/autograd.html#autograd-mechanics">
              <span class="std std-ref">
               Autograd mechanics
              </span>
             </a>
             ) from
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               tensor_list
              </span>
             </code>
             a list of tensors.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  tensor_list
                 </strong>
                 (
                 <em>
                  List
                 </em>
                 <em>
                  [
                 </em>
                 <em>
                  array_like
                 </em>
                 <em>
                  ]
                 </em>
                 ) – a list of tensors, or anything that can be passed to torch.tensor,
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  dimensionality.
                 </strong>
                 (
                 <em>
                  where each element
                 </em>
                 <em>
                  of
                 </em>
                 <em>
                  the list has the same
                 </em>
                 ) –
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Keyword Arguments
             </dt>
             <dd class="field-even">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  dtype
                 </strong>
                 (
                 <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.dtype
                   </span>
                  </code>
                 </a>
                 , optional) – the desired type of returned nested tensor.
Default: if None, same
                 <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.dtype
                   </span>
                  </code>
                 </a>
                 as leftmost tensor in the list.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  layout
                 </strong>
                 (
                 <a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.layout
                   </span>
                  </code>
                 </a>
                 , optional) – the desired layout of returned nested tensor.
Only strided and jagged layouts are supported. Default: if None, the strided layout.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  device
                 </strong>
                 (
                 <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.device
                   </span>
                  </code>
                 </a>
                 , optional) – the desired device of returned nested tensor.
Default: if None, same
                 <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.device
                   </span>
                  </code>
                 </a>
                 as leftmost tensor in the list
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  requires_grad
                 </strong>
                 (
                 <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">
                  <em>
                   bool
                  </em>
                 </a>
                 <em>
                  ,
                 </em>
                 <em>
                  optional
                 </em>
                 ) – If autograd should record operations on the
returned nested tensor. Default:
                 <code class="docutils literal notranslate">
                  <span class="pre">
                   False
                  </span>
                 </code>
                 .
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  pin_memory
                 </strong>
                 (
                 <a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)">
                  <em>
                   bool
                  </em>
                 </a>
                 <em>
                  ,
                 </em>
                 <em>
                  optional
                 </em>
                 ) – If set, returned nested tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default:
                 <code class="docutils literal notranslate">
                  <span class="pre">
                   False
                  </span>
                 </code>
                 .
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-odd">
              Return type
             </dt>
             <dd class="field-odd">
              <p>
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
              </p>
             </dd>
            </dl>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">True</span>
</pre>
             </div>
            </div>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="torch.nested.nested_tensor_from_jagged">
            <span class="sig-prename descclassname">
             <span class="pre">
              torch.nested.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              nested_tensor_from_jagged
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               values
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               offsets
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               lengths
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               jagged_dim
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               min_seqlen
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               max_seqlen
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/nested.html#nested_tensor_from_jagged">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nested/__init__.py#L359">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.nested.nested_tensor_from_jagged" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Constructs a jagged layout nested tensor from the given jagged components. The jagged layout
consists of a required values buffer with the jagged dimension packed into a single dimension.
The offsets / lengths metadata determines how this dimension is split into batch elements
and are expected to be allocated on the same device as the values buffer.
            </p>
            <dl class="simple">
             <dt>
              Expected metadata formats:
             </dt>
             <dd>
              <ul class="simple">
               <li>
                <p>
                 offsets: Indices within the packed dimension splitting it into heterogeneously-sized
batch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6
should be conceptually split into batch elements of length [2, 1, 3]. Note that both the
beginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).
                </p>
               </li>
               <li>
                <p>
                 lengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3]
indicates that a packed jagged dim of size 6 should be conceptually split into batch
elements of length [2, 1, 3].
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <p>
             Note that it can be useful to provide both offsets and lengths. This describes a nested tensor
with “holes”, where the offsets indicate the start position of each batch item and the length
specifies the total number of elements (see example below).
            </p>
            <p>
             The returned jagged layout nested tensor will be a view of the input values tensor.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  values
                 </strong>
                 (
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.Tensor
                   </span>
                  </code>
                 </a>
                 ) – The underlying buffer in the shape of
(sum_B(*), D_1, …, D_N). The jagged dimension is packed into a single dimension,
with the offsets / lengths metadata used to distinguish batch elements.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  offsets
                 </strong>
                 (optional
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.Tensor
                   </span>
                  </code>
                 </a>
                 ) – Offsets into the jagged dimension of shape B + 1.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  lengths
                 </strong>
                 (optional
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.Tensor
                   </span>
                  </code>
                 </a>
                 ) – Lengths of the batch elements of shape B.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  jagged_dim
                 </strong>
                 (
                 <em>
                  optional python:int
                 </em>
                 ) – Indicates which dimension in values is the packed jagged
dimension. If None, this is set to dim=1 (i.e. the dimension immediately following
the batch dimension). Default: None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  min_seqlen
                 </strong>
                 (
                 <em>
                  optional python:int
                 </em>
                 ) – If set, uses the specified value as the cached minimum sequence
length for the returned nested tensor. This can be a useful alternative to computing
this value on-demand, possibly avoiding a GPU -&gt; CPU sync. Default: None
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  max_seqlen
                 </strong>
                 (
                 <em>
                  optional python:int
                 </em>
                 ) – If set, uses the specified value as the cached maximum sequence
length for the returned nested tensor. This can be a useful alternative to computing
this value on-demand, possibly avoiding a GPU -&gt; CPU sync. Default: None
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Return type
             </dt>
             <dd class="field-even">
              <p>
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
              </p>
             </dd>
            </dl>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 3D shape with the middle dimension jagged</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([5, j2, 5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Length of each item in the batch:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>
<span class="go">tensor([3, 2, 1, 4, 2])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># NT with holes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">nested_tensor_from_jagged</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">nt</span><span class="o">.</span><span class="n">unbind</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batch item 1 consists of indices [0, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batch item 2 consists of indices [2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">values</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Batch item 3 consists of indices [3, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">values</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">True</span>
</pre>
             </div>
            </div>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="torch.nested.as_nested_tensor">
            <span class="sig-prename descclassname">
             <span class="pre">
              torch.nested.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              as_nested_tensor
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               ts
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               dtype
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               device
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layout
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/nested.html#as_nested_tensor">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nested/__init__.py#L27">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.nested.as_nested_tensor" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Constructs a nested tensor preserving autograd history from a tensor or a list / tuple of
tensors.
            </p>
            <p>
             If a nested tensor is passed, it will be returned directly unless the device / dtype / layout
differ. Note that converting device / dtype will result in a copy, while converting layout
is not currently supported by this function.
            </p>
            <p>
             If a non-nested tensor is passed, it is treated as a batch of constituents of consistent size.
A copy will be incurred if the passed device / dtype differ from those of the input OR if
the input is non-contiguous. Otherwise, the input’s storage will be used directly.
            </p>
            <p>
             If a tensor list is provided, tensors in the list are always copied during construction of
the nested tensor.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <p>
               <strong>
                ts
               </strong>
               (
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
               <em>
                or
               </em>
               <em>
                List
               </em>
               <em>
                [
               </em>
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
               <em>
                ] or
               </em>
               <em>
                Tuple
               </em>
               <em>
                [
               </em>
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
               <em>
                ]
               </em>
               ) – a tensor to treat as a nested tensor OR a
list / tuple of tensors with the same ndim
              </p>
             </dd>
             <dt class="field-even">
              Keyword Arguments
             </dt>
             <dd class="field-even">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  dtype
                 </strong>
                 (
                 <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.dtype
                   </span>
                  </code>
                 </a>
                 , optional) – the desired type of returned nested tensor.
Default: if None, same
                 <a class="reference internal" href="tensor_attributes.html#torch.dtype" title="torch.dtype">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.dtype
                   </span>
                  </code>
                 </a>
                 as leftmost tensor in the list.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  device
                 </strong>
                 (
                 <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.device
                   </span>
                  </code>
                 </a>
                 , optional) – the desired device of returned nested tensor.
Default: if None, same
                 <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.device
                   </span>
                  </code>
                 </a>
                 as leftmost tensor in the list
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  layout
                 </strong>
                 (
                 <a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.layout
                   </span>
                  </code>
                 </a>
                 , optional) – the desired layout of returned nested tensor.
Only strided and jagged layouts are supported. Default: if None, the strided layout.
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-odd">
              Return type
             </dt>
             <dd class="field-odd">
              <p>
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
              </p>
             </dd>
            </dl>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">is_leaf</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fake_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">fake_grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([1., 1., 1.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">grad</span>
<span class="go">tensor([0., 0., 0., 0., 0.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre>
             </div>
            </div>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="torch.nested.to_padded_tensor">
            <span class="sig-prename descclassname">
             <span class="pre">
              torch.nested.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              to_padded_tensor
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               input
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               padding
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               output_size
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               out
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <span class="sig-return">
             <span class="sig-return-icon">
              →
             </span>
             <span class="sig-return-typehint">
              <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
               <span class="pre">
                Tensor
               </span>
              </a>
             </span>
            </span>
            <a class="headerlink" href="#torch.nested.to_padded_tensor" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Returns a new (non-nested) Tensor by padding the
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               input
              </span>
             </code>
             nested tensor.
The leading entries will be filled with the nested data,
while the trailing entries will be padded.
            </p>
            <div class="admonition warning">
             <p class="admonition-title">
              Warning
             </p>
             <p>
              <a class="reference internal" href="#torch.nested.to_padded_tensor" title="torch.nested.to_padded_tensor">
               <code class="xref py py-func docutils literal notranslate">
                <span class="pre">
                 to_padded_tensor()
                </span>
               </code>
              </a>
              always copies the underlying data,
since the nested and the non-nested tensors differ in memory layout.
             </p>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <p>
               <strong>
                padding
               </strong>
               (
               <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)">
                <em>
                 float
                </em>
               </a>
               ) – The padding value for the trailing entries.
              </p>
             </dd>
             <dt class="field-even">
              Keyword Arguments
             </dt>
             <dd class="field-even">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  output_size
                 </strong>
                 (
                 <em>
                  Tuple
                 </em>
                 <em>
                  [
                 </em>
                 <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">
                  <em>
                   int
                  </em>
                 </a>
                 <em>
                  ]
                 </em>
                 ) – The size of the output tensor.
If given, it must be large enough to contain all nested data;
else, will infer by taking the max size of each nested sub-tensor along each dimension.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  out
                 </strong>
                 (
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <em>
                   Tensor
                  </em>
                 </a>
                 <em>
                  ,
                 </em>
                 <em>
                  optional
                 </em>
                 ) – the output tensor.
                </p>
               </li>
              </ul>
             </dd>
            </dl>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))])</span>
<span class="go">nested_tensor([</span>
<span class="go">  tensor([[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],</span>
<span class="go">          [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995]]),</span>
<span class="go">  tensor([[-1.8546, -0.7194, -0.2918, -0.1846],</span>
<span class="go">          [ 0.2773,  0.8793, -0.5183, -0.6447],</span>
<span class="go">          [ 1.8009,  1.8468, -0.9832, -1.5272]])</span>
<span class="go">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_infer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="go">tensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],</span>
<span class="go">         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995],</span>
<span class="go">         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],</span>
<span class="go">        [[-1.8546, -0.7194, -0.2918, -0.1846,  0.0000],</span>
<span class="go">         [ 0.2773,  0.8793, -0.5183, -0.6447,  0.0000],</span>
<span class="go">         [ 1.8009,  1.8468, -0.9832, -1.5272,  0.0000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="go">tensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276,  1.0000],</span>
<span class="go">         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995,  1.0000],</span>
<span class="go">         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],</span>
<span class="go">         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]],</span>
<span class="go">        [[-1.8546, -0.7194, -0.2918, -0.1846,  1.0000,  1.0000],</span>
<span class="go">         [ 0.2773,  0.8793, -0.5183, -0.6447,  1.0000,  1.0000],</span>
<span class="go">         [ 1.8009,  1.8468, -0.9832, -1.5272,  1.0000,  1.0000],</span>
<span class="go">         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt_small</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="n">nt</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">RuntimeError: Value in output_size is less than NestedTensor padded size. Truncation is not supported.</span>
</pre>
             </div>
            </div>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="torch.nested.masked_select">
            <span class="sig-prename descclassname">
             <span class="pre">
              torch.nested.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              masked_select
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               tensor
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               mask
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/nested.html#masked_select">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nested/__init__.py#L468">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.nested.masked_select" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Constructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor
will have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is
represented with the offsets, this is unlike
             <a class="reference internal" href="#torch.nested.masked_select" title="torch.nested.masked_select">
              <code class="xref py py-func docutils literal notranslate">
               <span class="pre">
                masked_select()
               </span>
              </code>
             </a>
             where the output is collapsed to a 1D tensor.
            </p>
            <p>
             Args:
tensor (
             <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                torch.Tensor
               </span>
              </code>
             </a>
             ): a strided tensor from which the jagged layout nested tensor is constructed from.
mask (
             <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
              <code class="xref py py-class docutils literal notranslate">
               <span class="pre">
                torch.Tensor
               </span>
              </code>
             </a>
             ): a strided mask tensor which is applied to the tensor input
            </p>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([3, j4])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Length of each item in the batch:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>
<span class="go">tensor([1, 2, 1])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">shape</span>
<span class="go">torch.Size([6, j5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Length of each item in the batch:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt</span><span class="o">.</span><span class="n">offsets</span><span class="p">()</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span>
<span class="go">tensor([0, 0, 0, 0, 0, 0])</span>
</pre>
             </div>
            </div>
            <dl class="field-list simple">
             <dt class="field-odd">
              Return type
             </dt>
             <dd class="field-odd">
              <p>
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
              </p>
             </dd>
            </dl>
           </dd>
          </dl>
          <dl class="py function">
           <dt class="sig sig-object py" id="torch.nested.narrow">
            <span class="sig-prename descclassname">
             <span class="pre">
              torch.nested.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              narrow
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               tensor
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               dim
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               start
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               length
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               layout
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               torch.strided
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/nested.html#narrow">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nested/__init__.py#L281">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.nested.narrow" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             Constructs a nested tensor (which might be a view) from
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               tensor
              </span>
             </code>
             , a strided tensor. This follows
similar semantics to torch.Tensor.narrow, where in the
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               dim
              </span>
             </code>
             -th dimension the new nested tensor
shows only the elements in the interval
             <cite>
              [start, start+length)
             </cite>
             . As nested representations
allow for a different
             <cite>
              start
             </cite>
             and
             <cite>
              length
             </cite>
             at each ‘row’ of that dimension,
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               start
              </span>
             </code>
             and
             <code class="xref py py-attr docutils literal notranslate">
              <span class="pre">
               length
              </span>
             </code>
             can also be tensors of shape
             <cite>
              tensor.shape[0]
             </cite>
             .
            </p>
            <p>
             There’s some differences depending on the layout you use for the nested tensor. If using strided layout,
torch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while
jagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular
representation is really useful for representing kv-caches in Transformer models, as specialized
SDPA kernels can deal with format easily, resulting in performance improvements.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  tensor
                 </strong>
                 (
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.Tensor
                   </span>
                  </code>
                 </a>
                 ) – a strided tensor, which will be used as the underlying data
for the nested tensor if using the jagged layout or will be copied for the strided layout.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  dim
                 </strong>
                 (
                 <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">
                  <em>
                   int
                  </em>
                 </a>
                 ) – the dimension where narrow will be applied. Only
                 <cite>
                  dim=1
                 </cite>
                 is supported for the
jagged layout, while strided supports all dim
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  start
                 </strong>
                 (Union[int,
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.Tensor
                   </span>
                  </code>
                 </a>
                 ]) – starting element for the narrow operation
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  length
                 </strong>
                 (Union[int,
                 <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                  <code class="xref py py-class docutils literal notranslate">
                   <span class="pre">
                    torch.Tensor
                   </span>
                  </code>
                 </a>
                 ]) – number of elements taken during the narrow op
                </p>
               </li>
              </ul>
             </dd>
             <dt class="field-even">
              Keyword Arguments
             </dt>
             <dd class="field-even">
              <p>
               <strong>
                layout
               </strong>
               (
               <a class="reference internal" href="tensor_attributes.html#torch.layout" title="torch.layout">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  torch.layout
                 </span>
                </code>
               </a>
               , optional) – the desired layout of returned nested tensor.
Only strided and jagged layouts are supported. Default: if None, the strided layout.
              </p>
             </dd>
             <dt class="field-odd">
              Return type
             </dt>
             <dd class="field-odd">
              <p>
               <a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor">
                <em>
                 Tensor
                </em>
               </a>
              </p>
             </dd>
            </dl>
            <p>
             Example:
            </p>
            <div class="highlight-default notranslate">
             <div class="highlight">
              <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">narrow_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt_narrowed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">narrow</span><span class="p">(</span><span class="n">narrow_base</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">jagged</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nt_narrowed</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="go">False</span>
</pre>
             </div>
            </div>
           </dd>
          </dl>
         </div>
        </div>
       </article>
      </div>
      <footer>
       <div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
        <a accesskey="n" class="btn btn-neutral float-right" href="size.html" rel="next" title="torch.Size">
         Next
         <img class="next-page" src="_static/images/chevron-right-orange.svg"/>
        </a>
        <a accesskey="p" class="btn btn-neutral" href="masked.html" rel="prev" title="torch.masked">
         <img class="previous-page" src="_static/images/chevron-right-orange.svg"/>
         Previous
        </a>
       </div>
       <hr/>
       <div role="contentinfo">
        <p>
         © Copyright PyTorch Contributors.
        </p>
       </div>
       <div>
        Built with
        <a href="http://sphinx-doc.org/">
         Sphinx
        </a>
        using a
        <a href="https://github.com/rtfd/sphinx_rtd_theme">
         theme
        </a>
        provided by
        <a href="https://readthedocs.org">
         Read the Docs
        </a>
        .
       </div>
      </footer>
     </div>
     <script>
      var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
     </script>
    </div>
    <div class="pytorch-content-right" id="pytorch-content-right">
     <div class="pytorch-right-menu" id="pytorch-right-menu">
      <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
       <ul>
        <li>
         <a class="reference internal" href="#">
          torch.nested
         </a>
         <ul>
          <li>
           <a class="reference internal" href="#introduction">
            Introduction
           </a>
          </li>
          <li>
           <a class="reference internal" href="#construction">
            Construction
           </a>
          </li>
          <li>
           <a class="reference internal" href="#data-layout-and-shape">
            Data Layout and Shape
           </a>
          </li>
          <li>
           <a class="reference internal" href="#supported-operations">
            Supported Operations
           </a>
           <ul>
            <li>
             <a class="reference internal" href="#viewing-nested-tensor-constituents">
              Viewing nested tensor constituents
             </a>
            </li>
            <li>
             <a class="reference internal" href="#conversions-to-from-padded">
              Conversions to / from padded
             </a>
            </li>
            <li>
             <a class="reference internal" href="#shape-manipulations">
              Shape manipulations
             </a>
            </li>
            <li>
             <a class="reference internal" href="#attention-mechanisms">
              Attention mechanisms
             </a>
            </li>
           </ul>
          </li>
          <li>
           <a class="reference internal" href="#usage-with-torch-compile">
            Usage with torch.compile
           </a>
          </li>
          <li>
           <a class="reference internal" href="#troubleshooting">
            Troubleshooting
           </a>
           <ul>
            <li>
             <a class="reference internal" href="#unimplemented-ops">
              Unimplemented ops
             </a>
            </li>
            <li>
             <a class="reference internal" href="#ragged-structure-incompatibility">
              Ragged structure incompatibility
             </a>
            </li>
            <li>
             <a class="reference internal" href="#data-dependent-operation-within-torch-compile">
              Data dependent operation within torch.compile
             </a>
            </li>
           </ul>
          </li>
          <li>
           <a class="reference internal" href="#contributions">
            Contributions
           </a>
          </li>
          <li>
           <a class="reference internal" href="#detailed-docs-for-construction-and-conversion-functions">
            Detailed Docs for Construction and Conversion Functions
           </a>
           <ul>
            <li>
             <a class="reference internal" href="#torch.nested.nested_tensor">
              <code class="docutils literal notranslate">
               <span class="pre">
                nested_tensor()
               </span>
              </code>
             </a>
            </li>
            <li>
             <a class="reference internal" href="#torch.nested.nested_tensor_from_jagged">
              <code class="docutils literal notranslate">
               <span class="pre">
                nested_tensor_from_jagged()
               </span>
              </code>
             </a>
            </li>
            <li>
             <a class="reference internal" href="#torch.nested.as_nested_tensor">
              <code class="docutils literal notranslate">
               <span class="pre">
                as_nested_tensor()
               </span>
              </code>
             </a>
            </li>
            <li>
             <a class="reference internal" href="#torch.nested.to_padded_tensor">
              <code class="docutils literal notranslate">
               <span class="pre">
                to_padded_tensor()
               </span>
              </code>
             </a>
            </li>
            <li>
             <a class="reference internal" href="#torch.nested.masked_select">
              <code class="docutils literal notranslate">
               <span class="pre">
                masked_select()
               </span>
              </code>
             </a>
            </li>
            <li>
             <a class="reference internal" href="#torch.nested.narrow">
              <code class="docutils literal notranslate">
               <span class="pre">
                narrow()
               </span>
              </code>
             </a>
            </li>
           </ul>
          </li>
         </ul>
        </li>
       </ul>
      </div>
     </div>
    </div>
   </section>
  </div>
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js" type="text/javascript">
  </script>
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js">
  </script>
  <script src="_static/jquery.js">
  </script>
  <script src="_static/underscore.js">
  </script>
  <script src="_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="_static/doctools.js">
  </script>
  <script src="_static/sphinx_highlight.js">
  </script>
  <script src="_static/clipboard.min.js">
  </script>
  <script src="_static/copybutton.js">
  </script>
  <script src="_static/js/vendor/popper.min.js" type="text/javascript">
  </script>
  <script src="_static/js/vendor/bootstrap.min.js" type="text/javascript">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js">
  </script>
  <script src="_static/js/theme.js" type="text/javascript">
  </script>
  <script type="text/javascript">
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <script script="" type="text/javascript">
   var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
  </script>
  <img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1">
   <!-- Begin Footer -->
   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
     <div class="row">
      <div class="col-md-4 text-center">
       <h2>
        Docs
       </h2>
       <p>
        Access comprehensive developer documentation for PyTorch
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">
        View Docs
       </a>
      </div>
      <div class="col-md-4 text-center">
       <h2>
        Tutorials
       </h2>
       <p>
        Get in-depth tutorials for beginners and advanced developers
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/tutorials">
        View Tutorials
       </a>
      </div>
      <div class="col-md-4 text-center">
       <h2>
        Resources
       </h2>
       <p>
        Find development resources and get your questions answered
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/resources">
        View Resources
       </a>
      </div>
     </div>
    </div>
   </div>
   <footer class="site-footer">
    <div class="container footer-container">
     <div class="footer-logo-wrapper">
      <a class="footer-logo" href="https://pytorch.org/">
      </a>
     </div>
     <div class="footer-links-wrapper">
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         <a href="https://pytorch.org/">
          PyTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/get-started">
          Get Started
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/features">
          Features
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/ecosystem">
          Ecosystem
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/blog/">
          Blog
         </a>
        </li>
        <li>
         <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">
          Contributing
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         <a href="https://pytorch.org/resources">
          Resources
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials">
          Tutorials
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/docs/stable/index.html">
          Docs
         </a>
        </li>
        <li>
         <a href="https://discuss.pytorch.org" target="_blank">
          Discuss
         </a>
        </li>
        <li>
         <a href="https://github.com/pytorch/pytorch/issues" target="_blank">
          Github Issues
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">
          Brand Guidelines
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         Stay up to date
        </li>
        <li>
         <a href="https://www.facebook.com/pytorch" target="_blank">
          Facebook
         </a>
        </li>
        <li>
         <a href="https://twitter.com/pytorch" target="_blank">
          Twitter
         </a>
        </li>
        <li>
         <a href="https://www.youtube.com/pytorch" target="_blank">
          YouTube
         </a>
        </li>
        <li>
         <a href="https://www.linkedin.com/company/pytorch" target="_blank">
          LinkedIn
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         PyTorch Podcasts
        </li>
        <li>
         <a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">
          Spotify
         </a>
        </li>
        <li>
         <a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">
          Apple
         </a>
        </li>
        <li>
         <a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">
          Google
         </a>
        </li>
        <li>
         <a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">
          Amazon
         </a>
        </li>
       </ul>
      </div>
     </div>
     <div class="privacy-policy">
      <ul>
       <li class="privacy-policy-links">
        <a href="https://www.linuxfoundation.org/terms/" target="_blank">
         Terms
        </a>
       </li>
       <li class="privacy-policy-links">
        |
       </li>
       <li class="privacy-policy-links">
        <a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">
         Privacy
        </a>
       </li>
      </ul>
     </div>
     <div class="copyright">
      <p>
       © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
       <a href="https://www.linuxfoundation.org/policies/">
        www.linuxfoundation.org/policies/
       </a>
       . The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see
       <a href="https://www.lfprojects.org/policies/">
        www.lfprojects.org/policies/
       </a>
       .
      </p>
     </div>
    </div>
   </footer>
   <div class="cookie-banner-wrapper">
    <div class="container">
     <p class="gdpr-notice">
      To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls:
      <a href="https://www.facebook.com/policies/cookies/">
       Cookies Policy
      </a>
      .
     </p>
     <img class="close-button" src="_static/images/pytorch-x.svg"/>
    </div>
   </div>
   <!-- End Footer -->
   <!-- Begin Mobile Menu -->
   <div class="mobile-main-menu">
    <div class="container-fluid">
     <div class="container">
      <div class="mobile-main-menu-header-container">
       <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
       </a>
       <a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
       </a>
      </div>
     </div>
    </div>
    <div class="mobile-main-menu-links-container">
     <div class="main-menu">
      <ul>
       <li class="resources-mobile-menu-title">
        <a>
         Learn
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/get-started">
          Get Started
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials">
          Tutorials
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          Learn the Basics
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          PyTorch Recipes
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/beginner/introyt.html">
          Introduction to PyTorch - YouTube Series
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Ecosystem
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/ecosystem">
          Tools
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/#community-module">
          Community
         </a>
        </li>
        <li>
         <a href="https://discuss.pytorch.org/">
          Forums
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/resources">
          Developer Resources
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/ecosystem/contributor-awards-2023">
          Contributor Awards - 2024
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Edge
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/edge">
          About PyTorch Edge
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/executorch-overview">
          ExecuTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/executorch/stable/index.html">
          ExecuTorch Documentation
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Docs
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/docs/stable/index.html">
          PyTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/pytorch-domains">
          PyTorch Domains
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Blog &amp; News
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/blog/">
          PyTorch Blog
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/community-blog">
          Community Blog
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/videos">
          Videos
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/community-stories">
          Community Stories
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/events">
          Events
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/newsletter">
          Newsletter
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         About
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/foundation">
          PyTorch Foundation
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/governing-board">
          Governing Board
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/credits">
          Cloud Credit Program
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tac">
          Technical Advisory Council
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/staff">
          Staff
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/contact-us">
          Contact Us
         </a>
        </li>
       </ul>
      </ul>
     </div>
    </div>
   </div>
   <!-- End Mobile Menu -->
   <script src="_static/js/vendor/anchor.min.js" type="text/javascript">
   </script>
   <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
   </script>
  </img>
 </body>
</html>
