<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.cuda — PyTorch 2.7 documentation
  </title>
  <link href="https://pytorch.org/docs/stable/cuda.html" rel="canonical"/>
  <link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link href="_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="_static/katex-math.css" rel="stylesheet" type="text/css"/>
  <link href="_static/sphinx-dropdown.css" rel="stylesheet" type="text/css"/>
  <link href="_static/panels-bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/jit.css" rel="stylesheet" type="text/css"/>
  <link href="_static/css/custom.css" rel="stylesheet" type="text/css"/>
  <link href="genindex.html" rel="index" title="Index"/>
  <link href="search.html" rel="search" title="Search"/>
  <link href="generated/torch.cuda.StreamContext.html" rel="next" title="StreamContext"/>
  <link href="generated/torch.cpu.Stream.html" rel="prev" title="Stream"/>
  <!-- Google Tag Manager -->
  <script>
   (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');
  </script>
  <!-- End Google Tag Manager -->
  <script src="_static/js/modernizr.min.js">
  </script>
  <!-- Preload the theme fonts -->
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
  <!-- Preload the katex fonts -->
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
  <link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
 </head>
 <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
   <div class="header-container">
    <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
    </a>
    <div class="main-menu">
     <ul>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Learn
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
          <span class="dropdown-title">
           Get Started
          </span>
          <p>
           Run PyTorch locally or get started quickly with one of the supported cloud platforms
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
          <span class="dropdown-title">
           Tutorials
          </span>
          <p>
           Whats new in PyTorch tutorials
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          <span class="dropdown-title">
           Learn the Basics
          </span>
          <p>
           Familiarize yourself with PyTorch concepts and modules
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          <span class="dropdown-title">
           PyTorch Recipes
          </span>
          <p>
           Bite-size, ready-to-deploy PyTorch code examples
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
          <span class="dropdown-title">
           Intro to PyTorch - YouTube Series
          </span>
          <p>
           Master PyTorch basics with our engaging YouTube tutorial series
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Ecosystem
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
          <span class="dropdown-title">
           Tools
          </span>
          <p>
           Learn about the tools and frameworks in the PyTorch Ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
          <span class="dropdown-title">
           Community
          </span>
          <p>
           Join the PyTorch developer community to contribute, learn, and get your questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
          <span class="dropdown-title">
           Forums
          </span>
          <p>
           A place to discuss PyTorch code, issues, install, research
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/resources">
          <span class="dropdown-title">
           Developer Resources
          </span>
          <p>
           Find resources and get questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
          <span class="dropdown-title">
           Contributor Awards - 2024
          </span>
          <p>
           Award winners announced at this year's PyTorch Conference
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Edge
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/edge">
          <span class="dropdown-title">
           About PyTorch Edge
          </span>
          <p>
           Build innovative and privacy-aware AI experiences for edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
          <span class="dropdown-title">
           ExecuTorch
          </span>
          <p>
           End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
          <span class="dropdown-title">
           ExecuTorch Docs
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Docs
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
          <span class="dropdown-title">
           PyTorch
          </span>
          <p>
           Explore the documentation for comprehensive guidance on how to use PyTorch
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
          <span class="dropdown-title">
           PyTorch Domains
          </span>
          <p>
           Read the PyTorch Domains documentation to learn more about domain-specific libraries
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Blogs &amp; News
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
          <span class="dropdown-title">
           PyTorch Blog
          </span>
          <p>
           Catch up on the latest technical news and happenings
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
          <span class="dropdown-title">
           Community Blog
          </span>
          <p>
           Stories from the PyTorch ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/videos">
          <span class="dropdown-title">
           Videos
          </span>
          <p>
           Learn about the latest PyTorch tutorials, new, and more
          </p>
          <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
           <span class="dropdown-title">
            Community Stories
           </span>
           <p>
            Learn how our community solves real, everyday machine learning problems with PyTorch
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/events">
           <span class="dropdown-title">
            Events
           </span>
           <p>
            Find events, webinars, and podcasts
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
           <span class="dropdown-title">
            Newsletter
           </span>
           <p>
            Stay up-to-date with the latest updates
           </p>
          </a>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         About
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
          <span class="dropdown-title">
           PyTorch Foundation
          </span>
          <p>
           Learn more about the PyTorch Foundation
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
          <span class="dropdown-title">
           Governing Board
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/credits">
          <span class="dropdown-title">
           Cloud Credit Program
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tac">
          <span class="dropdown-title">
           Technical Advisory Council
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/staff">
          <span class="dropdown-title">
           Staff
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
          <span class="dropdown-title">
           Contact Us
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="no-dropdown">
        <a data-cta="join" href="https://pytorch.org/join">
         Become a Member
        </a>
       </div>
      </li>
      <li>
       <div class="main-menu-item">
        <a class="github-icon" href="https://github.com/pytorch/pytorch">
        </a>
       </div>
      </li>
      <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
     </ul>
    </div>
    <a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
    </a>
   </div>
  </div>
 </div>
 <body class="pytorch-body">
  <div class="table-of-contents-link-wrapper">
   <span>
    Table of Contents
   </span>
   <a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#">
   </a>
  </div>
  <nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
   <div class="pytorch-side-scroll">
    <div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
     <div class="pytorch-left-menu-search">
      <div class="version">
       <a href="https://pytorch.org/docs/versions.html">
        2.7 ▼
       </a>
      </div>
      <div id="searchBox">
       <div class="searchbox" id="googleSearchBox">
        <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e">
        </script>
        <div class="gcse-search">
        </div>
       </div>
       <div id="sphinxSearchBox" style="display: none;">
        <div role="search">
         <form action="search.html" class="wy-form" id="rtd-search-form" method="get">
          <input name="q" placeholder="Search Docs" type="text"/>
          <input name="check_keywords" type="hidden" value="yes"/>
          <input name="area" type="hidden" value="default"/>
         </form>
        </div>
       </div>
      </div>
      <form id="searchForm">
       <label style="margin-bottom: 1rem">
        <input checked="" name="searchType" type="radio" value="google"/>
        Google Search
       </label>
       <label style="margin-bottom: 1rem">
        <input name="searchType" type="radio" value="sphinx"/>
        Classic Search
       </label>
      </form>
      <script>
       document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
      </script>
     </div>
     <p class="caption" role="heading">
      <span class="caption-text">
       Community
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="community/build_ci_governance.html">
        PyTorch Governance | Build + CI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/contribution_guide.html">
        PyTorch Contribution Guide
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/design.html">
        PyTorch Design Philosophy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/governance.html">
        PyTorch Governance | Mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="community/persons_of_interest.html">
        PyTorch Governance | Maintainers
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Developer Notes
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/amp_examples.html">
        Automatic Mixed Precision examples
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/autograd.html">
        Autograd mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/broadcasting.html">
        Broadcasting semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">
        CPU threading and TorchScript inference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/cuda.html">
        CUDA semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/custom_operators.html">
        PyTorch Custom Operators Landing Page
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/ddp.html">
        Distributed Data Parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/extending.html">
        Extending PyTorch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/extending.func.html">
        Extending torch.func with autograd.Function
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/faq.html">
        Frequently Asked Questions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/fsdp.html">
        FSDP Notes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/get_start_xpu.html">
        Getting Started on Intel GPU
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/gradcheck.html">
        Gradcheck mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/hip.html">
        HIP (ROCm) semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/large_scale_deployments.html">
        Features for large-scale deployments
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/libtorch_stable_abi.html">
        LibTorch Stable ABI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/modules.html">
        Modules
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/mps.html">
        MPS backend
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/multiprocessing.html">
        Multiprocessing best practices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/numerical_accuracy.html">
        Numerical accuracy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/randomness.html">
        Reproducibility
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/serialization.html">
        Serialization semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="notes/windows.html">
        Windows FAQ
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Language Bindings
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="cpp_index.html">
        C++
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/javadoc/">
        Javadoc
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="deploy.html">
        torch::deploy
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Python API
      </span>
     </p>
     <ul class="current">
      <li class="toctree-l1">
       <a class="reference internal" href="torch.html">
        torch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.html">
        torch.nn
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.functional.html">
        torch.nn.functional
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensors.html">
        torch.Tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensor_attributes.html">
        Tensor Attributes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensor_view.html">
        Tensor Views
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="amp.html">
        torch.amp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="autograd.html">
        torch.autograd
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="library.html">
        torch.library
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="accelerator.html">
        torch.accelerator
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cpu.html">
        torch.cpu
       </a>
      </li>
      <li class="toctree-l1 current">
       <a class="current reference internal" href="#">
        torch.cuda
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html">
        Understanding CUDA Memory Usage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#generating-a-snapshot">
        Generating a Snapshot
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#using-the-visualizer">
        Using the visualizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_cuda_memory.html#snapshot-api-reference">
        Snapshot API Reference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mps.html">
        torch.mps
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="xpu.html">
        torch.xpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mtia.html">
        torch.mtia
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mtia.memory.html">
        torch.mtia.memory
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="meta.html">
        Meta device
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="backends.html">
        torch.backends
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="export.html">
        torch.export
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.html">
        torch.distributed
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.tensor.html">
        torch.distributed.tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.algorithms.join.html">
        torch.distributed.algorithms.join
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.elastic.html">
        torch.distributed.elastic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fsdp.html">
        torch.distributed.fsdp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.fsdp.fully_shard.html">
        torch.distributed.fsdp.fully_shard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.tensor.parallel.html">
        torch.distributed.tensor.parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.optim.html">
        torch.distributed.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.pipelining.html">
        torch.distributed.pipelining
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributed.checkpoint.html">
        torch.distributed.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="distributions.html">
        torch.distributions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch.compiler.html">
        torch.compiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fft.html">
        torch.fft
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="func.html">
        torch.func
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="futures.html">
        torch.futures
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fx.html">
        torch.fx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="fx.experimental.html">
        torch.fx.experimental
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="hub.html">
        torch.hub
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="jit.html">
        torch.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="linalg.html">
        torch.linalg
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="monitor.html">
        torch.monitor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="signal.html">
        torch.signal
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="special.html">
        torch.special
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch.overrides.html">
        torch.overrides
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="package.html">
        torch.package
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="profiler.html">
        torch.profiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.init.html">
        torch.nn.init
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nn.attention.html">
        torch.nn.attention
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="onnx.html">
        torch.onnx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="optim.html">
        torch.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="complex_numbers.html">
        Complex Numbers
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="ddp_comm_hooks.html">
        DDP Communication Hooks
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="quantization.html">
        Quantization
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="rpc.html">
        Distributed RPC Framework
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="random.html">
        torch.random
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="masked.html">
        torch.masked
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="nested.html">
        torch.nested
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="size.html">
        torch.Size
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="sparse.html">
        torch.sparse
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="storage.html">
        torch.Storage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="testing.html">
        torch.testing
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="utils.html">
        torch.utils
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="benchmark_utils.html">
        torch.utils.benchmark
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="bottleneck.html">
        torch.utils.bottleneck
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="checkpoint.html">
        torch.utils.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="cpp_extension.html">
        torch.utils.cpp_extension
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="data.html">
        torch.utils.data
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="deterministic.html">
        torch.utils.deterministic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="jit_utils.html">
        torch.utils.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="dlpack.html">
        torch.utils.dlpack
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="mobile_optimizer.html">
        torch.utils.mobile_optimizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="model_zoo.html">
        torch.utils.model_zoo
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="tensorboard.html">
        torch.utils.tensorboard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="module_tracker.html">
        torch.utils.module_tracker
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="type_info.html">
        Type Info
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="named_tensor.html">
        Named Tensors
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="name_inference.html">
        Named Tensors operator coverage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="config_mod.html">
        torch.__config__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="future_mod.html">
        torch.__future__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="logging.html">
        torch._logging
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="torch_environment_variables.html">
        Torch Environment Variables
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Libraries
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/audio/stable">
        torchaudio
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/data">
        TorchData
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/torchrec">
        TorchRec
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/serve">
        TorchServe
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/text/stable">
        torchtext
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/vision/stable">
        torchvision
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/xla/">
        PyTorch on XLA Devices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/ao">
        torchao
       </a>
      </li>
     </ul>
    </div>
   </div>
  </nav>
  <div class="pytorch-container">
   <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
    <div class="pytorch-breadcrumbs-wrapper">
     <div aria-label="breadcrumbs navigation" role="navigation">
      <ul class="pytorch-breadcrumbs">
       <li>
        <a href="index.html">
         Docs
        </a>
        &gt;
       </li>
       <li>
        torch.cuda
       </li>
       <li class="pytorch-breadcrumbs-aside">
        <a href="_sources/cuda.rst.txt" rel="nofollow">
         <img src="_static/images/view-page-source-icon.svg"/>
        </a>
       </li>
      </ul>
     </div>
    </div>
    <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
     Shortcuts
    </div>
   </div>
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <!-- Google Tag Manager (noscript) -->
     <noscript>
      <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0">
      </iframe>
     </noscript>
     <!-- End Google Tag Manager (noscript) -->
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <div class="section" id="module-torch.cuda">
         <span id="torch-cuda">
         </span>
         <h1>
          torch.cuda
          <a class="headerlink" href="#module-torch.cuda" title="Permalink to this heading">
           ¶
          </a>
         </h1>
         <p>
          This package adds support for CUDA tensor types.
         </p>
         <p>
          It implements the same function as CPU tensors, but they utilize
GPUs for computation.
         </p>
         <p>
          It is lazily initialized, so you can always import it, and use
          <a class="reference internal" href="generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available">
           <code class="xref py py-func docutils literal notranslate">
            <span class="pre">
             is_available()
            </span>
           </code>
          </a>
          to determine if your system supports CUDA.
         </p>
         <p>
          <a class="reference internal" href="notes/cuda.html#cuda-semantics">
           <span class="std std-ref">
            CUDA semantics
           </span>
          </a>
          has more details about working with CUDA.
         </p>
         <table class="autosummary longtable docutils colwidths-auto align-default">
          <tbody>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.StreamContext">
              </p>
              <a class="reference internal" href="generated/torch.cuda.StreamContext.html#torch.cuda.StreamContext" title="torch.cuda.StreamContext">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 StreamContext
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Context-manager that selects a given stream.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.can_device_access_peer">
              </p>
              <a class="reference internal" href="generated/torch.cuda.can_device_access_peer.html#torch.cuda.can_device_access_peer" title="torch.cuda.can_device_access_peer">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 can_device_access_peer
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Check if peer access between two devices is possible.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.current_blas_handle">
              </p>
              <a class="reference internal" href="generated/torch.cuda.current_blas_handle.html#torch.cuda.current_blas_handle" title="torch.cuda.current_blas_handle">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 current_blas_handle
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return cublasHandle_t pointer to current cuBLAS handle
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.current_device">
              </p>
              <a class="reference internal" href="generated/torch.cuda.current_device.html#torch.cuda.current_device" title="torch.cuda.current_device">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 current_device
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the index of a currently selected device.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.current_stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.current_stream.html#torch.cuda.current_stream" title="torch.cuda.current_stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 current_stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the currently selected
              <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Stream
                </span>
               </code>
              </a>
              for a given device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.cudart">
              </p>
              <a class="reference internal" href="generated/torch.cuda.cudart.html#torch.cuda.cudart" title="torch.cuda.cudart">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 cudart
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Retrieves the CUDA runtime API module.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.default_stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.default_stream.html#torch.cuda.default_stream" title="torch.cuda.default_stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 default_stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the default
              <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Stream
                </span>
               </code>
              </a>
              for a given device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.device">
              </p>
              <a class="reference internal" href="generated/torch.cuda.device.html#torch.cuda.device" title="torch.cuda.device">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 device
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Context-manager that changes the selected device.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.device_count">
              </p>
              <a class="reference internal" href="generated/torch.cuda.device_count.html#torch.cuda.device_count" title="torch.cuda.device_count">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 device_count
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the number of GPUs available.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.device_memory_used">
              </p>
              <a class="reference internal" href="generated/torch.cuda.device_memory_used.html#torch.cuda.device_memory_used" title="torch.cuda.device_memory_used">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 device_memory_used
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return used global (device) memory in bytes as given by
              <cite>
               nvidia-smi
              </cite>
              or
              <cite>
               amd-smi
              </cite>
              .
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.device_of">
              </p>
              <a class="reference internal" href="generated/torch.cuda.device_of.html#torch.cuda.device_of" title="torch.cuda.device_of">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 device_of
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Context-manager that changes the current device to that of given object.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.get_arch_list">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_arch_list.html#torch.cuda.get_arch_list" title="torch.cuda.get_arch_list">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_arch_list
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return list CUDA architectures this library was compiled for.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.get_device_capability">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_device_capability.html#torch.cuda.get_device_capability" title="torch.cuda.get_device_capability">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_device_capability
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Get the cuda capability of a device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.get_device_name">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_device_name.html#torch.cuda.get_device_name" title="torch.cuda.get_device_name">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_device_name
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Get the name of a device.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.get_device_properties">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_device_properties.html#torch.cuda.get_device_properties" title="torch.cuda.get_device_properties">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_device_properties
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Get the properties of a device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.get_gencode_flags">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_gencode_flags.html#torch.cuda.get_gencode_flags" title="torch.cuda.get_gencode_flags">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_gencode_flags
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return NVCC gencode flags this library was compiled with.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.get_stream_from_external">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_stream_from_external.html#torch.cuda.get_stream_from_external" title="torch.cuda.get_stream_from_external">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_stream_from_external
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return a
              <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream">
               <code class="xref py py-class docutils literal notranslate">
                <span class="pre">
                 Stream
                </span>
               </code>
              </a>
              from an externally allocated CUDA stream.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.get_sync_debug_mode">
              </p>
              <a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode.html#torch.cuda.get_sync_debug_mode" title="torch.cuda.get_sync_debug_mode">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 get_sync_debug_mode
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return current value of debug mode for cuda synchronizing operations.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.init">
              </p>
              <a class="reference internal" href="generated/torch.cuda.init.html#torch.cuda.init" title="torch.cuda.init">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 init
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Initialize PyTorch's CUDA state.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.ipc_collect">
              </p>
              <a class="reference internal" href="generated/torch.cuda.ipc_collect.html#torch.cuda.ipc_collect" title="torch.cuda.ipc_collect">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 ipc_collect
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Force collects GPU memory after it has been released by CUDA IPC.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.is_available">
              </p>
              <a class="reference internal" href="generated/torch.cuda.is_available.html#torch.cuda.is_available" title="torch.cuda.is_available">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 is_available
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return a bool indicating if CUDA is currently available.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.is_initialized">
              </p>
              <a class="reference internal" href="generated/torch.cuda.is_initialized.html#torch.cuda.is_initialized" title="torch.cuda.is_initialized">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 is_initialized
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return whether PyTorch's CUDA state has been initialized.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.is_tf32_supported">
              </p>
              <a class="reference internal" href="generated/torch.cuda.is_tf32_supported.html#torch.cuda.is_tf32_supported" title="torch.cuda.is_tf32_supported">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 is_tf32_supported
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return a bool indicating if the current CUDA/ROCm device supports dtype tf32.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.memory_usage">
              </p>
              <a class="reference internal" href="generated/torch.cuda.memory_usage.html#torch.cuda.memory_usage" title="torch.cuda.memory_usage">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 memory_usage
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the percent of time over the past sample period during which global (device) memory was being read or written as given by
              <cite>
               nvidia-smi
              </cite>
              .
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.set_device">
              </p>
              <a class="reference internal" href="generated/torch.cuda.set_device.html#torch.cuda.set_device" title="torch.cuda.set_device">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 set_device
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Set the current device.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.set_stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.set_stream.html#torch.cuda.set_stream" title="torch.cuda.set_stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 set_stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Set the current stream.This is a wrapper API to set the stream.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.set_sync_debug_mode">
              </p>
              <a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode.html#torch.cuda.set_sync_debug_mode" title="torch.cuda.set_sync_debug_mode">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 set_sync_debug_mode
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Set the debug mode for cuda synchronizing operations.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.stream">
              </p>
              <a class="reference internal" href="generated/torch.cuda.stream.html#torch.cuda.stream" title="torch.cuda.stream">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 stream
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Wrap around the Context-manager StreamContext that selects a given stream.
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.synchronize">
              </p>
              <a class="reference internal" href="generated/torch.cuda.synchronize.html#torch.cuda.synchronize" title="torch.cuda.synchronize">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 synchronize
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Wait for all kernels in all streams on a CUDA device to complete.
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.utilization">
              </p>
              <a class="reference internal" href="generated/torch.cuda.utilization.html#torch.cuda.utilization" title="torch.cuda.utilization">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 utilization
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by
              <cite>
               nvidia-smi
              </cite>
              .
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.temperature">
              </p>
              <a class="reference internal" href="generated/torch.cuda.temperature.html#torch.cuda.temperature" title="torch.cuda.temperature">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 temperature
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the average temperature of the GPU sensor in Degrees C (Centigrades).
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.power_draw">
              </p>
              <a class="reference internal" href="generated/torch.cuda.power_draw.html#torch.cuda.power_draw" title="torch.cuda.power_draw">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 power_draw
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the average power draw of the GPU sensor in mW (MilliWatts)
             </p>
            </td>
           </tr>
           <tr class="row-odd">
            <td>
             <p>
              <p id="torch.cuda.clock_rate">
              </p>
              <a class="reference internal" href="generated/torch.cuda.clock_rate.html#torch.cuda.clock_rate" title="torch.cuda.clock_rate">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 clock_rate
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Return the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by
              <cite>
               nvidia-smi
              </cite>
              .
             </p>
            </td>
           </tr>
           <tr class="row-even">
            <td>
             <p>
              <p id="torch.cuda.OutOfMemoryError">
              </p>
              <a class="reference internal" href="generated/torch.cuda.OutOfMemoryError.html#torch.cuda.OutOfMemoryError" title="torch.cuda.OutOfMemoryError">
               <code class="xref py py-obj docutils literal notranslate">
                <span class="pre">
                 OutOfMemoryError
                </span>
               </code>
              </a>
             </p>
            </td>
            <td>
             <p>
              Exception raised when device is out of memory
             </p>
            </td>
           </tr>
          </tbody>
         </table>
         <div class="section" id="random-number-generator">
          <h2>
           Random Number Generator
           <a class="headerlink" href="#random-number-generator" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.get_rng_state">
               </p>
               <a class="reference internal" href="generated/torch.cuda.get_rng_state.html#torch.cuda.get_rng_state" title="torch.cuda.get_rng_state">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_rng_state
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the random number generator state of the specified GPU as a ByteTensor.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.get_rng_state_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.get_rng_state_all.html#torch.cuda.get_rng_state_all" title="torch.cuda.get_rng_state_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_rng_state_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return a list of ByteTensor representing the random number states of all devices.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.set_rng_state">
               </p>
               <a class="reference internal" href="generated/torch.cuda.set_rng_state.html#torch.cuda.set_rng_state" title="torch.cuda.set_rng_state">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  set_rng_state
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the random number generator state of the specified GPU.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.set_rng_state_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.set_rng_state_all.html#torch.cuda.set_rng_state_all" title="torch.cuda.set_rng_state_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  set_rng_state_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the random number generator state of all devices.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.manual_seed">
               </p>
               <a class="reference internal" href="generated/torch.cuda.manual_seed.html#torch.cuda.manual_seed" title="torch.cuda.manual_seed">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  manual_seed
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the seed for generating random numbers for the current GPU.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.manual_seed_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.manual_seed_all.html#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  manual_seed_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the seed for generating random numbers on all GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.seed">
               </p>
               <a class="reference internal" href="generated/torch.cuda.seed.html#torch.cuda.seed" title="torch.cuda.seed">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  seed
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the seed for generating random numbers to a random number for the current GPU.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.seed_all">
               </p>
               <a class="reference internal" href="generated/torch.cuda.seed_all.html#torch.cuda.seed_all" title="torch.cuda.seed_all">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  seed_all
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set the seed for generating random numbers to a random number on all GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.initial_seed">
               </p>
               <a class="reference internal" href="generated/torch.cuda.initial_seed.html#torch.cuda.initial_seed" title="torch.cuda.initial_seed">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  initial_seed
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the current random seed of the current GPU.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="communication-collectives">
          <h2>
           Communication collectives
           <a class="headerlink" href="#communication-collectives" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.broadcast.html#torch.cuda.comm.broadcast" title="torch.cuda.comm.broadcast">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.broadcast
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Broadcasts a tensor to specified GPU devices.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced.html#torch.cuda.comm.broadcast_coalesced" title="torch.cuda.comm.broadcast_coalesced">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.broadcast_coalesced
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Broadcast a sequence of tensors to the specified GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.reduce_add.html#torch.cuda.comm.reduce_add" title="torch.cuda.comm.reduce_add">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.reduce_add
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Sum tensors from multiple GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.scatter.html#torch.cuda.comm.scatter" title="torch.cuda.comm.scatter">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.scatter
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Scatters tensor across multiple GPUs.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.comm.gather.html#torch.cuda.comm.gather" title="torch.cuda.comm.gather">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  comm.gather
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Gathers tensors from multiple GPU devices.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="streams-and-events">
          <h2>
           Streams and events
           <a class="headerlink" href="#streams-and-events" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.Stream">
               </p>
               <a class="reference internal" href="generated/torch.cuda.Stream.html#torch.cuda.Stream" title="torch.cuda.Stream">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Stream
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around a CUDA stream.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.ExternalStream">
               </p>
               <a class="reference internal" href="generated/torch.cuda.ExternalStream.html#torch.cuda.ExternalStream" title="torch.cuda.ExternalStream">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  ExternalStream
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around an externally allocated CUDA stream.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.Event">
               </p>
               <a class="reference internal" href="generated/torch.cuda.Event.html#torch.cuda.Event" title="torch.cuda.Event">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  Event
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around a CUDA event.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="graphs-beta">
          <h2>
           Graphs (beta)
           <a class="headerlink" href="#graphs-beta" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.is_current_stream_capturing">
               </p>
               <a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing.html#torch.cuda.is_current_stream_capturing" title="torch.cuda.is_current_stream_capturing">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  is_current_stream_capturing
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.graph_pool_handle">
               </p>
               <a class="reference internal" href="generated/torch.cuda.graph_pool_handle.html#torch.cuda.graph_pool_handle" title="torch.cuda.graph_pool_handle">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  graph_pool_handle
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return an opaque token representing the id of a graph memory pool.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.CUDAGraph">
               </p>
               <a class="reference internal" href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  CUDAGraph
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around a CUDA graph.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.graph">
               </p>
               <a class="reference internal" href="generated/torch.cuda.graph.html#torch.cuda.graph" title="torch.cuda.graph">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  graph
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Context-manager that captures CUDA work into a
               <a class="reference internal" href="generated/torch.cuda.CUDAGraph.html#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  torch.cuda.CUDAGraph
                 </span>
                </code>
               </a>
               object for later replay.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.make_graphed_callables">
               </p>
               <a class="reference internal" href="generated/torch.cuda.make_graphed_callables.html#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  make_graphed_callables
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Accept callables (functions or
               <a class="reference internal" href="generated/torch.nn.Module.html#torch.nn.Module" title="torch.nn.Module">
                <code class="xref py py-class docutils literal notranslate">
                 <span class="pre">
                  nn.Module
                 </span>
                </code>
               </a>
               s) and returns graphed versions.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="memory-management">
          <span id="cuda-memory-management-api">
          </span>
          <h2>
           Memory management
           <a class="headerlink" href="#memory-management" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.empty_cache">
               </p>
               <a class="reference internal" href="generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache" title="torch.cuda.empty_cache">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  empty_cache
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in
               <cite>
                nvidia-smi
               </cite>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.get_per_process_memory_fraction">
               </p>
               <a class="reference internal" href="generated/torch.cuda.get_per_process_memory_fraction.html#torch.cuda.get_per_process_memory_fraction" title="torch.cuda.get_per_process_memory_fraction">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_per_process_memory_fraction
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Get memory fraction for a process.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.list_gpu_processes">
               </p>
               <a class="reference internal" href="generated/torch.cuda.list_gpu_processes.html#torch.cuda.list_gpu_processes" title="torch.cuda.list_gpu_processes">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  list_gpu_processes
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return a human-readable printout of the running processes and their GPU memory use for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.mem_get_info">
               </p>
               <a class="reference internal" href="generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info" title="torch.cuda.mem_get_info">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  mem_get_info
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the global free and total GPU memory for a given device using cudaMemGetInfo.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory_stats">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_stats.html#torch.cuda.memory_stats" title="torch.cuda.memory_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return a dictionary of CUDA memory allocator statistics for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.host_memory_stats">
               </p>
               <a class="reference internal" href="generated/torch.cuda.host_memory_stats.html#torch.cuda.host_memory_stats" title="torch.cuda.host_memory_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  host_memory_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return a dictionary of CUDA memory allocator statistics for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory_summary">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_summary.html#torch.cuda.memory_summary" title="torch.cuda.memory_summary">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_summary
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return a human-readable printout of the current memory allocator statistics for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.memory_snapshot">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_snapshot.html#torch.cuda.memory_snapshot" title="torch.cuda.memory_snapshot">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_snapshot
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return a snapshot of the CUDA memory allocator state across all devices.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory_allocated">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated" title="torch.cuda.memory_allocated">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_allocated
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the current GPU memory occupied by tensors in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.max_memory_allocated">
               </p>
               <a class="reference internal" href="generated/torch.cuda.max_memory_allocated.html#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_memory_allocated
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the maximum GPU memory occupied by tensors in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.reset_max_memory_allocated">
               </p>
               <a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated.html#torch.cuda.reset_max_memory_allocated" title="torch.cuda.reset_max_memory_allocated">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  reset_max_memory_allocated
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.memory_reserved">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_reserved
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the current GPU memory managed by the caching allocator in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.max_memory_reserved">
               </p>
               <a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_memory_reserved
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return the maximum GPU memory managed by the caching allocator in bytes for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.set_per_process_memory_fraction">
               </p>
               <a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction.html#torch.cuda.set_per_process_memory_fraction" title="torch.cuda.set_per_process_memory_fraction">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  set_per_process_memory_fraction
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Set memory fraction for a process.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory_cached">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory_cached.html#torch.cuda.memory_cached" title="torch.cuda.memory_cached">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  memory_cached
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Deprecated; see
               <a class="reference internal" href="generated/torch.cuda.memory_reserved.html#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  memory_reserved()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.max_memory_cached">
               </p>
               <a class="reference internal" href="generated/torch.cuda.max_memory_cached.html#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  max_memory_cached
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Deprecated; see
               <a class="reference internal" href="generated/torch.cuda.max_memory_reserved.html#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved">
                <code class="xref py py-func docutils literal notranslate">
                 <span class="pre">
                  max_memory_reserved()
                 </span>
                </code>
               </a>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.reset_max_memory_cached">
               </p>
               <a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached.html#torch.cuda.reset_max_memory_cached" title="torch.cuda.reset_max_memory_cached">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  reset_max_memory_cached
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.reset_peak_memory_stats">
               </p>
               <a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats.html#torch.cuda.reset_peak_memory_stats" title="torch.cuda.reset_peak_memory_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  reset_peak_memory_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Reset the "peak" stats tracked by the CUDA memory allocator.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.reset_peak_host_memory_stats">
               </p>
               <a class="reference internal" href="generated/torch.cuda.reset_peak_host_memory_stats.html#torch.cuda.reset_peak_host_memory_stats" title="torch.cuda.reset_peak_host_memory_stats">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  reset_peak_host_memory_stats
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Reset the "peak" stats tracked by the host memory allocator.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.caching_allocator_alloc">
               </p>
               <a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc.html#torch.cuda.caching_allocator_alloc" title="torch.cuda.caching_allocator_alloc">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  caching_allocator_alloc
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Perform a memory allocation using the CUDA memory allocator.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.caching_allocator_delete">
               </p>
               <a class="reference internal" href="generated/torch.cuda.caching_allocator_delete.html#torch.cuda.caching_allocator_delete" title="torch.cuda.caching_allocator_delete">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  caching_allocator_delete
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Delete memory allocated using the CUDA memory allocator.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.get_allocator_backend">
               </p>
               <a class="reference internal" href="generated/torch.cuda.get_allocator_backend.html#torch.cuda.get_allocator_backend" title="torch.cuda.get_allocator_backend">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  get_allocator_backend
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Return a string describing the active allocator backend as set by
               <code class="docutils literal notranslate">
                <span class="pre">
                 PYTORCH_CUDA_ALLOC_CONF
                </span>
               </code>
               .
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.CUDAPluggableAllocator">
               </p>
               <a class="reference internal" href="generated/torch.cuda.CUDAPluggableAllocator.html#torch.cuda.CUDAPluggableAllocator" title="torch.cuda.CUDAPluggableAllocator">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  CUDAPluggableAllocator
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               CUDA memory allocator loaded from a so file.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.change_current_allocator">
               </p>
               <a class="reference internal" href="generated/torch.cuda.change_current_allocator.html#torch.cuda.change_current_allocator" title="torch.cuda.change_current_allocator">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  change_current_allocator
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Change the currently used memory allocator to be the one provided.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.MemPool">
               </p>
               <a class="reference internal" href="generated/torch.cuda.MemPool.html#torch.cuda.MemPool" title="torch.cuda.MemPool">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  MemPool
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               MemPool represents a pool of memory in a caching allocator.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.MemPoolContext">
               </p>
               <a class="reference internal" href="generated/torch.cuda.MemPoolContext.html#torch.cuda.MemPoolContext" title="torch.cuda.MemPoolContext">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  MemPoolContext
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               MemPoolContext holds the currently active pool and stashes the previous pool.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.memory.caching_allocator_enable">
               </p>
               <a class="reference internal" href="generated/torch.cuda.memory.caching_allocator_enable.html#torch.cuda.memory.caching_allocator_enable" title="torch.cuda.memory.caching_allocator_enable">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  caching_allocator_enable
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Enable or disable the CUDA memory allocator.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
          <dl class="py class">
           <dt class="sig sig-object py" id="torch.cuda.use_mem_pool">
            <em class="property">
             <span class="pre">
              class
             </span>
             <span class="w">
             </span>
            </em>
            <span class="sig-prename descclassname">
             <span class="pre">
              torch.cuda.
             </span>
            </span>
            <span class="sig-name descname">
             <span class="pre">
              use_mem_pool
             </span>
            </span>
            <span class="sig-paren">
             (
            </span>
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               pool
              </span>
             </span>
            </em>
            ,
            <em class="sig-param">
             <span class="n">
              <span class="pre">
               device
              </span>
             </span>
             <span class="o">
              <span class="pre">
               =
              </span>
             </span>
             <span class="default_value">
              <span class="pre">
               None
              </span>
             </span>
            </em>
            <span class="sig-paren">
             )
            </span>
            <a class="reference internal" href="_modules/torch/cuda/memory.html#use_mem_pool">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="reference external" href="https://github.com/pytorch/pytorch/blob/v2.7.0/torch/cuda/memory.py#L1168">
             <span class="viewcode-link">
              <span class="pre">
               [source]
              </span>
             </span>
            </a>
            <a class="headerlink" href="#torch.cuda.use_mem_pool" title="Permalink to this definition">
             ¶
            </a>
           </dt>
           <dd>
            <p>
             A context manager that routes allocations to a given pool.
            </p>
            <dl class="field-list simple">
             <dt class="field-odd">
              Parameters
             </dt>
             <dd class="field-odd">
              <ul class="simple">
               <li>
                <p>
                 <strong>
                  pool
                 </strong>
                 (
                 <a class="reference internal" href="generated/torch.cuda.MemPool.html#torch.cuda.MemPool" title="torch.cuda.MemPool">
                  <em>
                   torch.cuda.MemPool
                  </em>
                 </a>
                 ) – a MemPool object to be made active so that
allocations route to this pool.
                </p>
               </li>
               <li>
                <p>
                 <strong>
                  device
                 </strong>
                 (
                 <a class="reference internal" href="tensor_attributes.html#torch.device" title="torch.device">
                  <em>
                   torch.device
                  </em>
                 </a>
                 <em>
                  or
                 </em>
                 <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">
                  <em>
                   int
                  </em>
                 </a>
                 <em>
                  ,
                 </em>
                 <em>
                  optional
                 </em>
                 ) – selected device. Uses MemPool on
the current device, given by
                 <a class="reference internal" href="generated/torch.cuda.current_device.html#torch.cuda.current_device" title="torch.cuda.current_device">
                  <code class="xref py py-func docutils literal notranslate">
                   <span class="pre">
                    current_device()
                   </span>
                  </code>
                 </a>
                 ,
if
                 <a class="reference internal" href="generated/torch.cuda.device.html#torch.cuda.device" title="torch.cuda.device">
                  <code class="xref py py-attr docutils literal notranslate">
                   <span class="pre">
                    device
                   </span>
                  </code>
                 </a>
                 is
                 <code class="docutils literal notranslate">
                  <span class="pre">
                   None
                  </span>
                 </code>
                 (default).
                </p>
               </li>
              </ul>
             </dd>
            </dl>
           </dd>
          </dl>
         </div>
         <div class="section" id="nvidia-tools-extension-nvtx">
          <h2>
           NVIDIA Tools Extension (NVTX)
           <a class="headerlink" href="#nvidia-tools-extension-nvtx" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.nvtx.mark.html#torch.cuda.nvtx.mark" title="torch.cuda.nvtx.mark">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  nvtx.mark
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Describe an instantaneous event that occurred at some point.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.nvtx.range_push.html#torch.cuda.nvtx.range_push" title="torch.cuda.nvtx.range_push">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  nvtx.range_push
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Push a range onto a stack of nested range span.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.nvtx.range_pop.html#torch.cuda.nvtx.range_pop" title="torch.cuda.nvtx.range_pop">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  nvtx.range_pop
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Pop a range off of a stack of nested range spans.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.nvtx.range.html#torch.cuda.nvtx.range" title="torch.cuda.nvtx.range">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  nvtx.range
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Context manager / decorator that pushes an NVTX range at the beginning of its scope, and pops it at the end.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="jiterator-beta">
          <h2>
           Jiterator (beta)
           <a class="headerlink" href="#jiterator-beta" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn.html#torch.cuda.jiterator._create_jit_fn" title="torch.cuda.jiterator._create_jit_fn">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  jiterator._create_jit_fn
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Create a jiterator-generated cuda kernel for an elementwise op.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <a class="reference internal" href="generated/torch.cuda.jiterator._create_multi_output_jit_fn.html#torch.cuda.jiterator._create_multi_output_jit_fn" title="torch.cuda.jiterator._create_multi_output_jit_fn">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  jiterator._create_multi_output_jit_fn
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
         </div>
         <div class="section" id="tunableop">
          <h2>
           TunableOp
           <a class="headerlink" href="#tunableop" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           Some operations could be implemented using more than one library or more than
one technique. For example, a GEMM could be implemented for CUDA or ROCm using
either the cublas/cublasLt libraries or hipblas/hipblasLt libraries,
respectively. How does one know which implementation is the fastest and should
be chosen? That’s what TunableOp provides. Certain operators have been
implemented using multiple strategies as Tunable Operators. At runtime, all
strategies are profiled and the fastest is selected for all subsequent
operations.
          </p>
          <p>
           See the
           <a class="reference internal" href="cuda.tunable.html">
            <span class="doc">
             documentation
            </span>
           </a>
           for information on how to use it.
          </p>
          <div class="toctree-wrapper compound">
          </div>
         </div>
         <div class="section" id="stream-sanitizer-prototype">
          <h2>
           Stream Sanitizer (prototype)
           <a class="headerlink" href="#stream-sanitizer-prototype" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch.
See the
           <a class="reference internal" href="cuda._sanitizer.html">
            <span class="doc">
             documentation
            </span>
           </a>
           for information on how to use it.
          </p>
          <div class="toctree-wrapper compound">
          </div>
         </div>
         <div class="section" id="gpudirect-storage-prototype">
          <h2>
           GPUDirect Storage (prototype)
           <a class="headerlink" href="#gpudirect-storage-prototype" title="Permalink to this heading">
            ¶
           </a>
          </h2>
          <p>
           The APIs in
           <code class="docutils literal notranslate">
            <span class="pre">
             torch.cuda.gds
            </span>
           </code>
           provide thin wrappers around certain cuFile APIs that allow
direct memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See the
           <a class="reference external" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api">
            cufile api documentation
           </a>
           for more details.
          </p>
          <p>
           These APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must
ensure that their system is appropriately configured to use GPUDirect Storage per the
           <a class="reference external" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/contents.html">
            GPUDirect Storage documentation
           </a>
           .
          </p>
          <p>
           See the docs for
           <a class="reference internal" href="generated/torch.cuda.gds.GdsFile.html#torch.cuda.gds.GdsFile" title="torch.cuda.gds.GdsFile">
            <code class="xref py py-class docutils literal notranslate">
             <span class="pre">
              GdsFile
             </span>
            </code>
           </a>
           for an example of how to use these.
          </p>
          <table class="autosummary longtable docutils colwidths-auto align-default">
           <tbody>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.gds.gds_register_buffer">
               </p>
               <a class="reference internal" href="generated/torch.cuda.gds.gds_register_buffer.html#torch.cuda.gds.gds_register_buffer" title="torch.cuda.gds.gds_register_buffer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  gds_register_buffer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Registers a storage on a CUDA device as a cufile buffer.
              </p>
             </td>
            </tr>
            <tr class="row-even">
             <td>
              <p>
               <p id="torch.cuda.gds.gds_deregister_buffer">
               </p>
               <a class="reference internal" href="generated/torch.cuda.gds.gds_deregister_buffer.html#torch.cuda.gds.gds_deregister_buffer" title="torch.cuda.gds.gds_deregister_buffer">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  gds_deregister_buffer
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Deregisters a previously registered storage on a CUDA device as a cufile buffer.
              </p>
             </td>
            </tr>
            <tr class="row-odd">
             <td>
              <p>
               <p id="torch.cuda.gds.GdsFile">
               </p>
               <a class="reference internal" href="generated/torch.cuda.gds.GdsFile.html#torch.cuda.gds.GdsFile" title="torch.cuda.gds.GdsFile">
                <code class="xref py py-obj docutils literal notranslate">
                 <span class="pre">
                  GdsFile
                 </span>
                </code>
               </a>
              </p>
             </td>
             <td>
              <p>
               Wrapper around cuFile.
              </p>
             </td>
            </tr>
           </tbody>
          </table>
          <span class="target" id="module-torch.cuda.comm">
          </span>
          <span class="target" id="module-torch.cuda.error">
          </span>
          <span class="target" id="module-torch.cuda.gds">
          </span>
          <span class="target" id="module-torch.cuda.graphs">
          </span>
          <span class="target" id="module-torch.cuda.jiterator">
          </span>
          <span class="target" id="module-torch.cuda.memory">
          </span>
          <span class="target" id="module-torch.cuda.nccl">
          </span>
          <span class="target" id="module-torch.cuda.nvtx">
          </span>
          <span class="target" id="module-torch.cuda.profiler">
          </span>
          <span class="target" id="module-torch.cuda.random">
          </span>
          <span class="target" id="module-torch.cuda.sparse">
          </span>
          <span class="target" id="module-torch.cuda.streams">
          </span>
         </div>
        </div>
       </article>
      </div>
      <footer>
       <div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
        <a accesskey="n" class="btn btn-neutral float-right" href="generated/torch.cuda.StreamContext.html" rel="next" title="StreamContext">
         Next
         <img class="next-page" src="_static/images/chevron-right-orange.svg"/>
        </a>
        <a accesskey="p" class="btn btn-neutral" href="generated/torch.cpu.Stream.html" rel="prev" title="Stream">
         <img class="previous-page" src="_static/images/chevron-right-orange.svg"/>
         Previous
        </a>
       </div>
       <hr/>
       <div role="contentinfo">
        <p>
         © Copyright PyTorch Contributors.
        </p>
       </div>
       <div>
        Built with
        <a href="http://sphinx-doc.org/">
         Sphinx
        </a>
        using a
        <a href="https://github.com/rtfd/sphinx_rtd_theme">
         theme
        </a>
        provided by
        <a href="https://readthedocs.org">
         Read the Docs
        </a>
        .
       </div>
      </footer>
     </div>
     <script>
      var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
     </script>
    </div>
    <div class="pytorch-content-right" id="pytorch-content-right">
     <div class="pytorch-right-menu" id="pytorch-right-menu">
      <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
       <ul>
        <li>
         <a class="reference internal" href="#">
          torch.cuda
         </a>
         <ul>
          <li>
           <a class="reference internal" href="#random-number-generator">
            Random Number Generator
           </a>
          </li>
          <li>
           <a class="reference internal" href="#communication-collectives">
            Communication collectives
           </a>
          </li>
          <li>
           <a class="reference internal" href="#streams-and-events">
            Streams and events
           </a>
          </li>
          <li>
           <a class="reference internal" href="#graphs-beta">
            Graphs (beta)
           </a>
          </li>
          <li>
           <a class="reference internal" href="#memory-management">
            Memory management
           </a>
           <ul>
            <li>
             <a class="reference internal" href="#torch.cuda.use_mem_pool">
              <code class="docutils literal notranslate">
               <span class="pre">
                use_mem_pool
               </span>
              </code>
             </a>
            </li>
           </ul>
          </li>
          <li>
           <a class="reference internal" href="#nvidia-tools-extension-nvtx">
            NVIDIA Tools Extension (NVTX)
           </a>
          </li>
          <li>
           <a class="reference internal" href="#jiterator-beta">
            Jiterator (beta)
           </a>
          </li>
          <li>
           <a class="reference internal" href="#tunableop">
            TunableOp
           </a>
          </li>
          <li>
           <a class="reference internal" href="#stream-sanitizer-prototype">
            Stream Sanitizer (prototype)
           </a>
          </li>
          <li>
           <a class="reference internal" href="#gpudirect-storage-prototype">
            GPUDirect Storage (prototype)
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </div>
     </div>
    </div>
   </section>
  </div>
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js" type="text/javascript">
  </script>
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js">
  </script>
  <script src="_static/jquery.js">
  </script>
  <script src="_static/underscore.js">
  </script>
  <script src="_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="_static/doctools.js">
  </script>
  <script src="_static/sphinx_highlight.js">
  </script>
  <script src="_static/clipboard.min.js">
  </script>
  <script src="_static/copybutton.js">
  </script>
  <script src="_static/js/vendor/popper.min.js" type="text/javascript">
  </script>
  <script src="_static/js/vendor/bootstrap.min.js" type="text/javascript">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js">
  </script>
  <script src="_static/js/theme.js" type="text/javascript">
  </script>
  <script type="text/javascript">
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <script script="" type="text/javascript">
   var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
  </script>
  <img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1">
   <!-- Begin Footer -->
   <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
     <div class="row">
      <div class="col-md-4 text-center">
       <h2>
        Docs
       </h2>
       <p>
        Access comprehensive developer documentation for PyTorch
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">
        View Docs
       </a>
      </div>
      <div class="col-md-4 text-center">
       <h2>
        Tutorials
       </h2>
       <p>
        Get in-depth tutorials for beginners and advanced developers
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/tutorials">
        View Tutorials
       </a>
      </div>
      <div class="col-md-4 text-center">
       <h2>
        Resources
       </h2>
       <p>
        Find development resources and get your questions answered
       </p>
       <a class="with-right-arrow" href="https://pytorch.org/resources">
        View Resources
       </a>
      </div>
     </div>
    </div>
   </div>
   <footer class="site-footer">
    <div class="container footer-container">
     <div class="footer-logo-wrapper">
      <a class="footer-logo" href="https://pytorch.org/">
      </a>
     </div>
     <div class="footer-links-wrapper">
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         <a href="https://pytorch.org/">
          PyTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/get-started">
          Get Started
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/features">
          Features
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/ecosystem">
          Ecosystem
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/blog/">
          Blog
         </a>
        </li>
        <li>
         <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">
          Contributing
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         <a href="https://pytorch.org/resources">
          Resources
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials">
          Tutorials
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/docs/stable/index.html">
          Docs
         </a>
        </li>
        <li>
         <a href="https://discuss.pytorch.org" target="_blank">
          Discuss
         </a>
        </li>
        <li>
         <a href="https://github.com/pytorch/pytorch/issues" target="_blank">
          Github Issues
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">
          Brand Guidelines
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         Stay up to date
        </li>
        <li>
         <a href="https://www.facebook.com/pytorch" target="_blank">
          Facebook
         </a>
        </li>
        <li>
         <a href="https://twitter.com/pytorch" target="_blank">
          Twitter
         </a>
        </li>
        <li>
         <a href="https://www.youtube.com/pytorch" target="_blank">
          YouTube
         </a>
        </li>
        <li>
         <a href="https://www.linkedin.com/company/pytorch" target="_blank">
          LinkedIn
         </a>
        </li>
       </ul>
      </div>
      <div class="footer-links-col">
       <ul>
        <li class="list-title">
         PyTorch Podcasts
        </li>
        <li>
         <a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">
          Spotify
         </a>
        </li>
        <li>
         <a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">
          Apple
         </a>
        </li>
        <li>
         <a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">
          Google
         </a>
        </li>
        <li>
         <a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">
          Amazon
         </a>
        </li>
       </ul>
      </div>
     </div>
     <div class="privacy-policy">
      <ul>
       <li class="privacy-policy-links">
        <a href="https://www.linuxfoundation.org/terms/" target="_blank">
         Terms
        </a>
       </li>
       <li class="privacy-policy-links">
        |
       </li>
       <li class="privacy-policy-links">
        <a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">
         Privacy
        </a>
       </li>
      </ul>
     </div>
     <div class="copyright">
      <p>
       © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
       <a href="https://www.linuxfoundation.org/policies/">
        www.linuxfoundation.org/policies/
       </a>
       . The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see
       <a href="https://www.lfprojects.org/policies/">
        www.lfprojects.org/policies/
       </a>
       .
      </p>
     </div>
    </div>
   </footer>
   <div class="cookie-banner-wrapper">
    <div class="container">
     <p class="gdpr-notice">
      To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls:
      <a href="https://www.facebook.com/policies/cookies/">
       Cookies Policy
      </a>
      .
     </p>
     <img class="close-button" src="_static/images/pytorch-x.svg"/>
    </div>
   </div>
   <!-- End Footer -->
   <!-- Begin Mobile Menu -->
   <div class="mobile-main-menu">
    <div class="container-fluid">
     <div class="container">
      <div class="mobile-main-menu-header-container">
       <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
       </a>
       <a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
       </a>
      </div>
     </div>
    </div>
    <div class="mobile-main-menu-links-container">
     <div class="main-menu">
      <ul>
       <li class="resources-mobile-menu-title">
        <a>
         Learn
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/get-started">
          Get Started
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials">
          Tutorials
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          Learn the Basics
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          PyTorch Recipes
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tutorials/beginner/introyt.html">
          Introduction to PyTorch - YouTube Series
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Ecosystem
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/ecosystem">
          Tools
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/#community-module">
          Community
         </a>
        </li>
        <li>
         <a href="https://discuss.pytorch.org/">
          Forums
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/resources">
          Developer Resources
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/ecosystem/contributor-awards-2023">
          Contributor Awards - 2024
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Edge
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/edge">
          About PyTorch Edge
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/executorch-overview">
          ExecuTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/executorch/stable/index.html">
          ExecuTorch Documentation
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Docs
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/docs/stable/index.html">
          PyTorch
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/pytorch-domains">
          PyTorch Domains
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         Blog &amp; News
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/blog/">
          PyTorch Blog
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/community-blog">
          Community Blog
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/videos">
          Videos
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/community-stories">
          Community Stories
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/events">
          Events
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/newsletter">
          Newsletter
         </a>
        </li>
       </ul>
       <li class="resources-mobile-menu-title">
        <a>
         About
        </a>
       </li>
       <ul class="resources-mobile-menu-items">
        <li>
         <a href="https://pytorch.org/foundation">
          PyTorch Foundation
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/governing-board">
          Governing Board
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/credits">
          Cloud Credit Program
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/tac">
          Technical Advisory Council
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/staff">
          Staff
         </a>
        </li>
        <li>
         <a href="https://pytorch.org/contact-us">
          Contact Us
         </a>
        </li>
       </ul>
      </ul>
     </div>
    </div>
   </div>
   <!-- End Mobile Menu -->
   <script src="_static/js/vendor/anchor.min.js" type="text/javascript">
   </script>
   <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
   </script>
  </img>
 </body>
</html>
