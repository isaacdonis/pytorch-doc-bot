<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head>
  <meta content="noindex" name="robots"/>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   torch.storage — PyTorch 2.7 documentation
  </title>
  <link href="https://pytorch.org/docs/stable/_modules/torch/storage.html" rel="canonical"/>
  <link href="../../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link href="../../_static/pygments.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/css/theme.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/katex-math.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/sphinx-dropdown.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/panels-bootstrap.min.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/css/jit.css" rel="stylesheet" type="text/css"/>
  <link href="../../_static/css/custom.css" rel="stylesheet" type="text/css"/>
  <link href="../../genindex.html" rel="index" title="Index"/>
  <link href="../../search.html" rel="search" title="Search"/>
  <!-- Google Tag Manager -->
  <script>
   (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');
  </script>
  <!-- End Google Tag Manager -->
  <script src="../../_static/js/modernizr.min.js">
  </script>
  <!-- Preload the theme fonts -->
  <link as="font" crossorigin="anonymous" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
  <!-- Preload the katex fonts -->
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
  <link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
  <link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
 </head>
 <div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
   <div class="header-container">
    <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
    </a>
    <div class="main-menu">
     <ul>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Learn
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
          <span class="dropdown-title">
           Get Started
          </span>
          <p>
           Run PyTorch locally or get started quickly with one of the supported cloud platforms
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
          <span class="dropdown-title">
           Tutorials
          </span>
          <p>
           Whats new in PyTorch tutorials
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
          <span class="dropdown-title">
           Learn the Basics
          </span>
          <p>
           Familiarize yourself with PyTorch concepts and modules
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
          <span class="dropdown-title">
           PyTorch Recipes
          </span>
          <p>
           Bite-size, ready-to-deploy PyTorch code examples
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
          <span class="dropdown-title">
           Intro to PyTorch - YouTube Series
          </span>
          <p>
           Master PyTorch basics with our engaging YouTube tutorial series
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Ecosystem
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
          <span class="dropdown-title">
           Tools
          </span>
          <p>
           Learn about the tools and frameworks in the PyTorch Ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
          <span class="dropdown-title">
           Community
          </span>
          <p>
           Join the PyTorch developer community to contribute, learn, and get your questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
          <span class="dropdown-title">
           Forums
          </span>
          <p>
           A place to discuss PyTorch code, issues, install, research
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/resources">
          <span class="dropdown-title">
           Developer Resources
          </span>
          <p>
           Find resources and get questions answered
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
          <span class="dropdown-title">
           Contributor Awards - 2024
          </span>
          <p>
           Award winners announced at this year's PyTorch Conference
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Edge
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/edge">
          <span class="dropdown-title">
           About PyTorch Edge
          </span>
          <p>
           Build innovative and privacy-aware AI experiences for edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
          <span class="dropdown-title">
           ExecuTorch
          </span>
          <p>
           End-to-end solution for enabling on-device inference capabilities across mobile and edge devices
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
          <span class="dropdown-title">
           ExecuTorch Docs
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Docs
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
          <span class="dropdown-title">
           PyTorch
          </span>
          <p>
           Explore the documentation for comprehensive guidance on how to use PyTorch
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
          <span class="dropdown-title">
           PyTorch Domains
          </span>
          <p>
           Read the PyTorch Domains documentation to learn more about domain-specific libraries
          </p>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         Blogs &amp; News
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
          <span class="dropdown-title">
           PyTorch Blog
          </span>
          <p>
           Catch up on the latest technical news and happenings
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
          <span class="dropdown-title">
           Community Blog
          </span>
          <p>
           Stories from the PyTorch ecosystem
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/videos">
          <span class="dropdown-title">
           Videos
          </span>
          <p>
           Learn about the latest PyTorch tutorials, new, and more
          </p>
          <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
           <span class="dropdown-title">
            Community Stories
           </span>
           <p>
            Learn how our community solves real, everyday machine learning problems with PyTorch
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/events">
           <span class="dropdown-title">
            Events
           </span>
           <p>
            Find events, webinars, and podcasts
           </p>
          </a>
          <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
           <span class="dropdown-title">
            Newsletter
           </span>
           <p>
            Stay up-to-date with the latest updates
           </p>
          </a>
         </a>
        </div>
       </div>
      </li>
      <li>
       <div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
        <a class="with-down-arrow">
         About
        </a>
        <div class="resources-dropdown-menu">
         <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
          <span class="dropdown-title">
           PyTorch Foundation
          </span>
          <p>
           Learn more about the PyTorch Foundation
          </p>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
          <span class="dropdown-title">
           Governing Board
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/credits">
          <span class="dropdown-title">
           Cloud Credit Program
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/tac">
          <span class="dropdown-title">
           Technical Advisory Council
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/staff">
          <span class="dropdown-title">
           Staff
          </span>
         </a>
         <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
          <span class="dropdown-title">
           Contact Us
          </span>
         </a>
        </div>
       </div>
      </li>
      <li class="main-menu-item">
       <div class="no-dropdown">
        <a data-cta="join" href="https://pytorch.org/join">
         Become a Member
        </a>
       </div>
      </li>
      <li>
       <div class="main-menu-item">
        <a class="github-icon" href="https://github.com/pytorch/pytorch">
        </a>
       </div>
      </li>
      <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
     </ul>
    </div>
    <a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#">
    </a>
   </div>
  </div>
 </div>
 <body class="pytorch-body">
  <div class="table-of-contents-link-wrapper">
   <span>
    Table of Contents
   </span>
   <a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#">
   </a>
  </div>
  <nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
   <div class="pytorch-side-scroll">
    <div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
     <div class="pytorch-left-menu-search">
      <div class="version">
       <a href="https://pytorch.org/docs/versions.html">
        2.7 ▼
       </a>
      </div>
      <div id="searchBox">
       <div class="searchbox" id="googleSearchBox">
        <script async="" src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e">
        </script>
        <div class="gcse-search">
        </div>
       </div>
       <div id="sphinxSearchBox" style="display: none;">
        <div role="search">
         <form action="../../search.html" class="wy-form" id="rtd-search-form" method="get">
          <input name="q" placeholder="Search Docs" type="text"/>
          <input name="check_keywords" type="hidden" value="yes"/>
          <input name="area" type="hidden" value="default"/>
         </form>
        </div>
       </div>
      </div>
      <form id="searchForm">
       <label style="margin-bottom: 1rem">
        <input checked="" name="searchType" type="radio" value="google"/>
        Google Search
       </label>
       <label style="margin-bottom: 1rem">
        <input name="searchType" type="radio" value="sphinx"/>
        Classic Search
       </label>
      </form>
      <script>
       document.addEventListener('DOMContentLoaded', function() {
      const searchForm = document.getElementById('searchForm');
      const googleSearchBox = document.getElementById('googleSearchBox');
      const sphinxSearchBox = document.getElementById('sphinxSearchBox');
      // Function to toggle search box visibility
      function toggleSearchBox(searchType) {
        googleSearchBox.style.display = searchType === 'google' ? 'block' : 'none';
        sphinxSearchBox.style.display = searchType === 'sphinx' ? 'block' : 'none';
      }
      // Determine the default search type
      let defaultSearchType;
      const currentUrl = window.location.href;
      if (currentUrl.startsWith('https://pytorch.org/docs/stable')) {
        // For the stable documentation, default to Google
        defaultSearchType = localStorage.getItem('searchType') || 'google';
      } else {
        // For any other version, including docs-preview, default to Sphinx
        defaultSearchType = 'sphinx';
      }
      // Set the default search type
      document.querySelector(`input[name="searchType"][value="${defaultSearchType}"]`).checked = true;
      toggleSearchBox(defaultSearchType);
      // Event listener for changes in search type
      searchForm.addEventListener('change', function(event) {
        const selectedSearchType = event.target.value;
        localStorage.setItem('searchType', selectedSearchType);
        toggleSearchBox(selectedSearchType);
      });
      // Set placeholder text for Google search box
      window.onload = function() {
        var placeholderText = "Search Docs";
        var googleSearchboxText = document.querySelector("#gsc-i-id1");
        if (googleSearchboxText) {
          googleSearchboxText.placeholder = placeholderText;
          googleSearchboxText.style.fontFamily = 'FreightSans';
          googleSearchboxText.style.fontSize = "1.2rem";
          googleSearchboxText.style.color = '#262626';
        }
      };
    });
      </script>
     </div>
     <p class="caption" role="heading">
      <span class="caption-text">
       Community
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../community/build_ci_governance.html">
        PyTorch Governance | Build + CI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../community/contribution_guide.html">
        PyTorch Contribution Guide
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../community/design.html">
        PyTorch Design Philosophy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../community/governance.html">
        PyTorch Governance | Mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../community/persons_of_interest.html">
        PyTorch Governance | Maintainers
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Developer Notes
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/amp_examples.html">
        Automatic Mixed Precision examples
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/autograd.html">
        Autograd mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/broadcasting.html">
        Broadcasting semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">
        CPU threading and TorchScript inference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/cuda.html">
        CUDA semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/custom_operators.html">
        PyTorch Custom Operators Landing Page
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/ddp.html">
        Distributed Data Parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/extending.html">
        Extending PyTorch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/extending.func.html">
        Extending torch.func with autograd.Function
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/faq.html">
        Frequently Asked Questions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/fsdp.html">
        FSDP Notes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/get_start_xpu.html">
        Getting Started on Intel GPU
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/gradcheck.html">
        Gradcheck mechanics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/hip.html">
        HIP (ROCm) semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/large_scale_deployments.html">
        Features for large-scale deployments
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/libtorch_stable_abi.html">
        LibTorch Stable ABI
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/modules.html">
        Modules
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/mps.html">
        MPS backend
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/multiprocessing.html">
        Multiprocessing best practices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/numerical_accuracy.html">
        Numerical accuracy
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/randomness.html">
        Reproducibility
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/serialization.html">
        Serialization semantics
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../notes/windows.html">
        Windows FAQ
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Language Bindings
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../cpp_index.html">
        C++
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/javadoc/">
        Javadoc
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../deploy.html">
        torch::deploy
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Python API
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch.html">
        torch
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../nn.html">
        torch.nn
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../nn.functional.html">
        torch.nn.functional
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../tensors.html">
        torch.Tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../tensor_attributes.html">
        Tensor Attributes
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../tensor_view.html">
        Tensor Views
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../amp.html">
        torch.amp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../autograd.html">
        torch.autograd
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../library.html">
        torch.library
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../accelerator.html">
        torch.accelerator
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../cpu.html">
        torch.cpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../cuda.html">
        torch.cuda
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch_cuda_memory.html">
        Understanding CUDA Memory Usage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch_cuda_memory.html#generating-a-snapshot">
        Generating a Snapshot
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch_cuda_memory.html#using-the-visualizer">
        Using the visualizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch_cuda_memory.html#snapshot-api-reference">
        Snapshot API Reference
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../mps.html">
        torch.mps
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../xpu.html">
        torch.xpu
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../mtia.html">
        torch.mtia
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../mtia.memory.html">
        torch.mtia.memory
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../meta.html">
        Meta device
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../backends.html">
        torch.backends
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../export.html">
        torch.export
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.html">
        torch.distributed
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.tensor.html">
        torch.distributed.tensor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.algorithms.join.html">
        torch.distributed.algorithms.join
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.elastic.html">
        torch.distributed.elastic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../fsdp.html">
        torch.distributed.fsdp
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.fsdp.fully_shard.html">
        torch.distributed.fsdp.fully_shard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.tensor.parallel.html">
        torch.distributed.tensor.parallel
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.optim.html">
        torch.distributed.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.pipelining.html">
        torch.distributed.pipelining
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributed.checkpoint.html">
        torch.distributed.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../distributions.html">
        torch.distributions
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch.compiler.html">
        torch.compiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../fft.html">
        torch.fft
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../func.html">
        torch.func
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../futures.html">
        torch.futures
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../fx.html">
        torch.fx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../fx.experimental.html">
        torch.fx.experimental
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../hub.html">
        torch.hub
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../jit.html">
        torch.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../linalg.html">
        torch.linalg
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../monitor.html">
        torch.monitor
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../signal.html">
        torch.signal
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../special.html">
        torch.special
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch.overrides.html">
        torch.overrides
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../package.html">
        torch.package
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../profiler.html">
        torch.profiler
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../nn.init.html">
        torch.nn.init
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../nn.attention.html">
        torch.nn.attention
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../onnx.html">
        torch.onnx
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../optim.html">
        torch.optim
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../complex_numbers.html">
        Complex Numbers
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../ddp_comm_hooks.html">
        DDP Communication Hooks
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../quantization.html">
        Quantization
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../rpc.html">
        Distributed RPC Framework
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../random.html">
        torch.random
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../masked.html">
        torch.masked
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../nested.html">
        torch.nested
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../size.html">
        torch.Size
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../sparse.html">
        torch.sparse
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../storage.html">
        torch.Storage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../testing.html">
        torch.testing
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../utils.html">
        torch.utils
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../benchmark_utils.html">
        torch.utils.benchmark
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../bottleneck.html">
        torch.utils.bottleneck
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../checkpoint.html">
        torch.utils.checkpoint
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../cpp_extension.html">
        torch.utils.cpp_extension
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../data.html">
        torch.utils.data
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../deterministic.html">
        torch.utils.deterministic
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../jit_utils.html">
        torch.utils.jit
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../dlpack.html">
        torch.utils.dlpack
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../mobile_optimizer.html">
        torch.utils.mobile_optimizer
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../model_zoo.html">
        torch.utils.model_zoo
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../tensorboard.html">
        torch.utils.tensorboard
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../module_tracker.html">
        torch.utils.module_tracker
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../type_info.html">
        Type Info
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../named_tensor.html">
        Named Tensors
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../name_inference.html">
        Named Tensors operator coverage
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../config_mod.html">
        torch.__config__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../future_mod.html">
        torch.__future__
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../logging.html">
        torch._logging
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference internal" href="../../torch_environment_variables.html">
        Torch Environment Variables
       </a>
      </li>
     </ul>
     <p class="caption" role="heading">
      <span class="caption-text">
       Libraries
      </span>
     </p>
     <ul>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/audio/stable">
        torchaudio
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/data">
        TorchData
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/torchrec">
        TorchRec
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/serve">
        TorchServe
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/text/stable">
        torchtext
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/vision/stable">
        torchvision
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/xla/">
        PyTorch on XLA Devices
       </a>
      </li>
      <li class="toctree-l1">
       <a class="reference external" href="https://pytorch.org/ao">
        torchao
       </a>
      </li>
     </ul>
    </div>
   </div>
  </nav>
  <div class="pytorch-container">
   <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
    <div class="pytorch-breadcrumbs-wrapper">
     <div aria-label="breadcrumbs navigation" role="navigation">
      <ul class="pytorch-breadcrumbs">
       <li>
        <a href="../../index.html">
         Docs
        </a>
        &gt;
       </li>
       <li>
        <a href="../index.html">
         Module code
        </a>
        &gt;
       </li>
       <li>
        <a href="../torch.html">
         torch
        </a>
        &gt;
       </li>
       <li>
        torch.storage
       </li>
       <li class="pytorch-breadcrumbs-aside">
       </li>
      </ul>
     </div>
    </div>
    <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
     Shortcuts
    </div>
   </div>
   <section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
    <div class="pytorch-content-left">
     <!-- Google Tag Manager (noscript) -->
     <noscript>
      <iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0">
      </iframe>
     </noscript>
     <!-- End Google Tag Manager (noscript) -->
     <div class="rst-content">
      <div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
       <article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
        <h1>
         Source code for torch.storage
        </h1>
        <div class="highlight">
         <pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">cast</span><span class="p">,</span> <span class="n">Optional</span> <span class="k">as</span> <span class="n">_Optional</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">TypeVar</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Self</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._utils</span> <span class="kn">import</span> <span class="n">_to</span><span class="p">,</span> <span class="n">_type</span>
<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">_bool</span><span class="p">,</span> <span class="n">_int</span><span class="p">,</span> <span class="n">Storage</span>


<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torch._prims_common</span> <span class="kn">import</span> <span class="n">DeviceLikeType</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"TypedStorage"</span><span class="p">,</span> <span class="s2">"UntypedStorage"</span><span class="p">]</span>


<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

    <span class="n">HAS_NUMPY</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
    <span class="n">HAS_NUMPY</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">np</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore[assignment]</span>


<span class="n">_share_memory_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
<span class="n">_share_memory_map</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">"T"</span><span class="p">,</span> <span class="n">bound</span><span class="o">=</span><span class="s2">"Union[_StorageBase, TypedStorage]"</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_StorageBase</span><span class="p">:</span>
    <span class="n">_cdata</span><span class="p">:</span> <span class="n">Any</span>
    <span class="n">is_sparse</span><span class="p">:</span> <span class="n">_bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">is_sparse_csr</span><span class="p">:</span> <span class="n">_bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span>
    <span class="c1"># Used when</span>
    <span class="c1"># (1) stashing FakeTensor device onto storage in torch.serialization.skip_data</span>
    <span class="c1"># (2) stashing device onto storage to propagate to FakeTensor when torch.load under FakeTensorMode</span>
    <span class="n">_fake_device</span><span class="p">:</span> <span class="n">_Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Used when loading with FakeTensorMode to give information about offset of storage in torch.saved-file</span>
    <span class="n">_checkpoint_offset</span><span class="p">:</span> <span class="n">_Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">:</span> <span class="n">_Optional</span><span class="p">[</span><span class="n">_bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">new</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">nbytes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">type</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">_Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">:</span> <span class="n">_bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Returns a copy of this object in CUDA memory.</span>

<span class="sd">        If this object is already in CUDA memory and on the correct device, then</span>
<span class="sd">        no copy is performed and the original object is returned.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (int): The destination GPU id. Defaults to the current device.</span>
<span class="sd">            non_blocking (bool): If ``True`` and the source is in pinned memory,</span>
<span class="sd">                the copy will be asynchronous with respect to the host. Otherwise,</span>
<span class="sd">                the argument has no effect.</span>
<span class="sd">        """</span>
        <span class="n">device2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">device</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device2</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">hpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Returns a copy of this object in HPU memory.</span>

<span class="sd">        If this object is already in HPU memory and on the correct device, then</span>
<span class="sd">        no copy is performed and the original object is returned.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (int): The destination HPU id. Defaults to the current device.</span>
<span class="sd">            non_blocking (bool): If ``True`` and the source is in pinned memory,</span>
<span class="sd">                the copy will be asynchronous with respect to the host. Otherwise,</span>
<span class="sd">                the argument has no effect.</span>
<span class="sd">        """</span>
        <span class="n">device2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"hpu"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">device</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"hpu"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device2</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">element_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">get_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">index</span>

    <span class="k">def</span> <span class="nf">data_ptr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">resizable</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="c1"># Defined in torch/csrc/generic/StorageSharing.cpp</span>
    <span class="k">def</span> <span class="nf">_share_filename_cpu_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_share_fd_cpu_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_using_filename_cpu</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="n">_int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_using_fd_cpu</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="n">_int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_buffer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_shared_filename_cpu</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">manager</span><span class="p">,</span>
        <span class="n">obj</span><span class="p">,</span>
        <span class="n">size</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_release_ipc_counter_cuda</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_with_weak_ptr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_shared_decref</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_write_file</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">resize_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="n">_int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_weak_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_set_from_file</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_set_cdata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_share_cuda_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">is_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_bool</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_shared_cuda</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_shared_incref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_free_weak_ref</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_hpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_file</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">shared</span><span class="p">,</span> <span class="n">nbytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_expired</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_byteswap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_get_filename</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">info_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"[</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">(device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">) of size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">]"</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"meta"</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">"...</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="n">info_str</span>
        <span class="n">data_str</span> <span class="o">=</span> <span class="s2">" "</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2"> "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
        <span class="k">return</span> <span class="n">data_str</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="n">info_str</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="n">memo</span> <span class="o">=</span> <span class="n">memo</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">"torch"</span><span class="p">,</span> <span class="p">{})</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cdata</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cdata</span><span class="p">]</span>
        <span class="n">new_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">memo</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cdata</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_storage</span>
        <span class="k">return</span> <span class="n">new_storage</span>

    <span class="k">def</span> <span class="nf">__reduce__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_use_new_zipfile_serialization</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">_load_from_bytes</span><span class="p">,</span> <span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">getvalue</span><span class="p">(),))</span>

    <span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__sizeof__</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return a copy of this storage."""</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbytes</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tolist</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return a list containing the elements of this storage."""</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return a CPU copy of this storage if it's not already on the CPU."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">"cpu"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">mps</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return a MPS copy of this storage if it's not already on the MPS."""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">"mps"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="s2">"mps"</span><span class="p">)</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Argument 'dtype' must be torch.dtype, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">storage</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">Storage</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">storage</span>

    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">DeviceLikeType</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">:</span> <span class="n">_bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to double type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to half type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">long</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to long type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">int</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to int type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">short</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to short type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">short</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">char</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to char type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">byte</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to byte type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">bool</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to bool type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to bfloat16 type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">complex_double</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to complex double type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">complex_float</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to complex float type."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">float8_e5m2</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e5m2 type"""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">float8_e4m3fn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e4m3fn type"""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">float8_e5m2fnuz</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e5m2fnuz type"""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">float8_e4m3fnuz</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e4m3fnuz type"""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_pinned</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"cuda"</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""Determine whether the CPU storage is already pinned on device.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (str or torch.device): The device to pin memory on (default: ``'cuda'``).</span>
<span class="sd">                This argument is discouraged and subject to deprecated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A boolean variable.</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">Storage</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>
            <span class="o">.</span><span class="n">is_pinned</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">pin_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"cuda"</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""Copy the CPU storage to pinned memory, if it's not already pinned.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (str or torch.device): The device to pin memory on (default: ``'cuda'``).</span>
<span class="sd">                This argument is discouraged and subject to deprecated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A pinned CPU storage.</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">"cpu"</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"cannot pin '</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span><span class="si">}</span><span class="s2">' only CPU memory can be pinned"</span><span class="p">)</span>

        <span class="n">pinned_tensor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">Storage</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>
            <span class="o">.</span><span class="n">pin_memory</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">pinned_tensor</span><span class="o">.</span><span class="n">untyped_storage</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""See :meth:`torch.UntypedStorage.share_memory_`"""</span>
        <span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">get_sharing_strategy</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">()]:</span>
            <span class="k">pass</span>  <span class="c1"># CUDA or PrivateUse1 doesn't use POSIX shared memory</span>
        <span class="k">elif</span> <span class="n">get_sharing_strategy</span><span class="p">()</span> <span class="o">==</span> <span class="s2">"file_system"</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_share_filename_cpu_</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_share_fd_cpu_</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_shared</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Create a new storage in shared memory with the same data type."""</span>
        <span class="kn">from</span> <span class="nn">torch.multiprocessing</span> <span class="kn">import</span> <span class="n">get_sharing_strategy</span>

        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">(),</span> <span class="s2">"hpu"</span><span class="p">]:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">get_sharing_strategy</span><span class="p">()</span> <span class="o">==</span> <span class="s2">"file_system"</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_new_using_filename_cpu</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_new_using_fd_cpu</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">untyped</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">byteswap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Swap bytes in underlying data."""</span>
        <span class="n">elem_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># for complex types, don't swap first and second numbers</span>
        <span class="k">if</span> <span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="n">elem_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">elem_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_byteswap</span><span class="p">(</span><span class="n">elem_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_share_memory_lock_protected</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">to_free</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">to_wait</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">with</span> <span class="n">_share_memory_lock</span><span class="p">:</span>
            <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cdata</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">_share_memory_map</span><span class="p">:</span>
                <span class="n">to_wait</span> <span class="o">=</span> <span class="n">_share_memory_map</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_share_memory_map</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>
                <span class="n">_share_memory_map</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
                <span class="n">to_free</span> <span class="o">=</span> <span class="n">key</span>

        <span class="c1"># If we're already in the process of sharing the storage, wait</span>
        <span class="c1"># for it to be done.</span>
        <span class="k">if</span> <span class="n">to_wait</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">to_wait</span><span class="p">:</span>
                <span class="k">pass</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># If we acquired the storage lock here and we're done working on it</span>
            <span class="c1"># we can now release it and free the entry.</span>
            <span class="k">if</span> <span class="n">to_free</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Ensure that the cdata from the storage didn't change and only</span>
                <span class="c1"># the data_ptr did.</span>
                <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cdata</span> <span class="o">==</span> <span class="n">to_free</span>
                <span class="k">with</span> <span class="n">_share_memory_lock</span><span class="p">:</span>
                    <span class="n">_share_memory_map</span><span class="p">[</span><span class="n">to_free</span><span class="p">]</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
                    <span class="k">del</span> <span class="n">_share_memory_map</span><span class="p">[</span><span class="n">to_free</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">wrapper</span>


<div class="viewcode-block" id="UntypedStorage"><a class="viewcode-back" href="../../storage.html#torch.UntypedStorage">[docs]</a><span class="k">class</span> <span class="nc">UntypedStorage</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">StorageBase</span><span class="p">,</span> <span class="n">_StorageBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"meta"</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Not available for 'meta' device type"</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_hpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"hpu"</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">filename</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Returns the file name associated with this storage.</span>

<span class="sd">        The file name will be a string if the storage is on CPU and was created via</span>
<span class="sd">        :meth:`~torch.from_file()` with ``shared`` as ``True``. This attribute is ``None`` otherwise.</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_filename</span><span class="p">()</span>

<div class="viewcode-block" id="UntypedStorage.share_memory_"><a class="viewcode-back" href="../../storage.html#torch.UntypedStorage.share_memory_">[docs]</a>    <span class="nd">@_share_memory_lock_protected</span>
    <span class="k">def</span> <span class="nf">share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Moves the storage to shared memory.</span>

<span class="sd">        This is a no-op for storages already in shared memory and for CUDA</span>
<span class="sd">        storages, which do not need to be moved for sharing across processes.</span>
<span class="sd">        Storages in shared memory cannot be resized.</span>

<span class="sd">        Note that to mitigate issues like `this &lt;https://github.com/pytorch/pytorch/issues/95606&gt;`_</span>
<span class="sd">        it is thread safe to call this function from multiple threads on the same object.</span>
<span class="sd">        It is NOT thread safe though to call any other function on self without proper</span>
<span class="sd">        synchronization. Please see :doc:`/notes/multiprocessing` for more details.</span>

<span class="sd">        .. note::</span>
<span class="sd">            When all references to a storage in shared memory are deleted, the associated shared memory</span>
<span class="sd">            object will also be deleted. PyTorch has a special cleanup process to ensure that this happens</span>
<span class="sd">            even if the current process exits unexpectedly.</span>

<span class="sd">            It is worth noting the difference between :meth:`share_memory_` and :meth:`from_file` with ``shared = True``</span>

<span class="sd">            #. ``share_memory_`` uses `shm_open(3) &lt;https://man7.org/linux/man-pages/man3/shm_open.3.html&gt;`_ to create a</span>
<span class="sd">               POSIX shared memory object while :meth:`from_file` uses</span>
<span class="sd">               `open(2) &lt;https://man7.org/linux/man-pages/man2/open.2.html&gt;`_ to open the filename passed by the user.</span>
<span class="sd">            #. Both use an `mmap(2) call &lt;https://man7.org/linux/man-pages/man2/mmap.2.html&gt;`_ with ``MAP_SHARED``</span>
<span class="sd">               to map the file/object into the current virtual address space</span>
<span class="sd">            #. ``share_memory_`` will call ``shm_unlink(3)`` on the object after mapping it to make sure the shared memory</span>
<span class="sd">               object is freed when no process has the object open. ``torch.from_file(shared=True)`` does not unlink the</span>
<span class="sd">               file. This file is persistent and will remain until it is deleted by the user.</span>

<span class="sd">        Returns:</span>
<span class="sd">            ``self``</span>
<span class="sd">        """</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="nd">@_share_memory_lock_protected</span>
    <span class="k">def</span> <span class="nf">_share_fd_cpu_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_share_fd_cpu_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@_share_memory_lock_protected</span>
    <span class="k">def</span> <span class="nf">_share_filename_cpu_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_share_filename_cpu_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_load_from_bytes</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="nd">@functools</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">_new_dtypes</span><span class="p">():</span>
    <span class="c1"># These are dtypes serialized as UntypedStorage unlike those in</span>
    <span class="c1"># _dtype_to_storage_type_map</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float8_e8m0fnu</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bits8</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bits16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bits1x8</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bits2x4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bits4x2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">complex32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint64</span><span class="p">,</span>
    <span class="p">}</span>


<span class="nd">@functools</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">_dtype_to_storage_type_map</span><span class="p">():</span>
    <span class="c1"># NOTE: We should no longer add dtypes to this map. This map</span>
    <span class="c1"># is only used for BC/FC with older PyTorch versions. Going forward,</span>
    <span class="c1"># new dtypes of TypedStorage should not translate to a legacy</span>
    <span class="c1"># &lt;type&gt;Storage class. Instead, new dtypes of TypedStorage should</span>
    <span class="c1"># be serialized as an UntypedStorage paired with a torch.dtype</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">:</span> <span class="s2">"DoubleStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">:</span> <span class="s2">"FloatStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">:</span> <span class="s2">"HalfStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">:</span> <span class="s2">"LongStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">:</span> <span class="s2">"IntStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="s2">"ShortStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="s2">"CharStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="s2">"ByteStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span> <span class="s2">"BoolStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">:</span> <span class="s2">"BFloat16Storage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">:</span> <span class="s2">"ComplexDoubleStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">:</span> <span class="s2">"ComplexFloatStorage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">:</span> <span class="s2">"QInt8Storage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">:</span> <span class="s2">"QInt32Storage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">:</span> <span class="s2">"QUInt8Storage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">:</span> <span class="s2">"QUInt4x2Storage"</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">:</span> <span class="s2">"QUInt2x4Storage"</span><span class="p">,</span>
    <span class="p">}</span>


<span class="nd">@functools</span><span class="o">.</span><span class="n">cache</span>
<span class="k">def</span> <span class="nf">_storage_type_to_dtype_map</span><span class="p">():</span>
    <span class="n">dtype_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">val</span><span class="p">:</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">_dtype_to_storage_type_map</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">return</span> <span class="n">dtype_map</span>


<span class="k">def</span> <span class="nf">_get_storage_from_sequence</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="n">interpret_dtypes</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">tmp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">sequence</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">interpret_dtypes</span><span class="p">[</span><span class="n">dtype</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">tmp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tmp_tensor</span><span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span><span class="o">.</span><span class="n">_untyped_storage</span>


<span class="k">def</span> <span class="nf">_isint</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">HAS_NUMPY</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span>


<span class="n">_always_warn_typed_storage_removal</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_get_always_warn_typed_storage_removal</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">_always_warn_typed_storage_removal</span>


<span class="k">def</span> <span class="nf">_set_always_warn_typed_storage_removal</span><span class="p">(</span><span class="n">always_warn</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">_always_warn_typed_storage_removal</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">always_warn</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
    <span class="n">_always_warn_typed_storage_removal</span> <span class="o">=</span> <span class="n">always_warn</span>


<span class="k">def</span> <span class="nf">_warn_typed_storage_removal</span><span class="p">(</span><span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">_always_warn_typed_storage_removal</span>

    <span class="k">def</span> <span class="nf">is_first_time</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">_warn_typed_storage_removal</span><span class="p">,</span> <span class="s2">"has_warned"</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="ow">not</span> <span class="n">_warn_typed_storage_removal</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">"has_warned"</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">_get_always_warn_typed_storage_removal</span><span class="p">()</span> <span class="ow">or</span> <span class="n">is_first_time</span><span class="p">():</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">"TypedStorage is deprecated. It will be removed in the future and "</span>
            <span class="s2">"UntypedStorage will be the only storage class. This should only matter "</span>
            <span class="s2">"to you if you are using storages directly.  To access UntypedStorage "</span>
            <span class="s2">"directly, use tensor.untyped_storage() instead of tensor.storage()"</span>
        <span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="n">stacklevel</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">_warn_typed_storage_removal</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">"has_warned"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_reset_warn_typed_storage_removal</span><span class="p">():</span>
    <span class="n">_warn_typed_storage_removal</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">"has_warned"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span> <span class="nf">_get_device_from_module</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="n">last_part</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">"."</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">last_part</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"cuda"</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">(),</span> <span class="s2">"hpu"</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">last_part</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">"cpu"</span>


<div class="viewcode-block" id="TypedStorage"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage">[docs]</a><span class="k">class</span> <span class="nc">TypedStorage</span><span class="p">:</span>
    <span class="n">is_sparse</span><span class="p">:</span> <span class="n">_bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Used when stashing FakeTensor device onto storage in torch.save(metadata_only=True)</span>
    <span class="n">_fake_device</span><span class="p">:</span> <span class="n">_Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">filename</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Returns the file name associated with this storage if the storage was memory mapped from a file.</span>
<span class="sd">        or ``None`` if the storage was not created by memory mapping a file."""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">filename</span>

<div class="viewcode-block" id="TypedStorage.fill_"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.fill_">[docs]</a>    <span class="k">def</span> <span class="nf">fill_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setitem</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()),</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">wrap_storage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_internal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_internal</span><span class="p">:</span>
            <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">cls</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">_LegacyStorage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">"Only child classes of _LegacyStorage can be instantiated"</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">cls</span> <span class="o">==</span> <span class="n">TypedStorage</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">arg_error_msg</span> <span class="o">=</span> <span class="p">(</span>
                <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="bp">cls</span><span class="si">}</span><span class="s2">.__new__ received an invalid combination "</span>
                <span class="sa">f</span><span class="s2">"of arguments. Expected one of:</span><span class="se">\n</span><span class="s2">"</span>
                <span class="s2">" * no arguments</span><span class="se">\n</span><span class="s2">"</span>
                <span class="s2">" * (int size)</span><span class="se">\n</span><span class="s2">"</span>
                <span class="s2">" * (Sequence data)</span><span class="se">\n</span><span class="s2">"</span>
                <span class="s2">" * (*, UntypedStorage wrap_storage)"</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="n">arg_error_msg</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Keyword argument 'device' cannot be specified"</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="n">arg_error_msg</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Keyword argument 'dtype' cannot be specified"</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">wrap_storage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="n">arg_error_msg</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Too many positional arguments"</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">_isint</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Sequence</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="n">arg_error_msg</span>
                        <span class="o">+</span> <span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Argument type not recognized: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">"</span>
                    <span class="p">)</span>

                <span class="k">return</span> <span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">_dtype</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">_get_device_from_module</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__module__</span><span class="p">),</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="n">arg_error_msg</span>
                        <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">No positional arguments should be given when using "</span>
                        <span class="s2">"'wrap_storage'"</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrap_storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="n">arg_error_msg</span>
                        <span class="o">+</span> <span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Argument 'wrap_storage' must be UntypedStorage, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">wrap_storage</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
                    <span class="p">)</span>

                <span class="n">cls_device</span> <span class="o">=</span> <span class="n">_get_device_from_module</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__module__</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">wrap_storage</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="n">cls_device</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="n">arg_error_msg</span>
                        <span class="o">+</span> <span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Device of 'wrap_storage' must be </span><span class="si">{</span><span class="n">cls_device</span><span class="si">}</span><span class="s2">"</span>
                        <span class="sa">f</span><span class="s2">", but got </span><span class="si">{</span><span class="n">wrap_storage</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2">"</span>
                    <span class="p">)</span>

                <span class="k">return</span> <span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="n">wrap_storage</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">wrap_storage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_internal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_internal</span><span class="p">:</span>
            <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="n">arg_error_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">"TypedStorage.__init__ received an invalid combination "</span>
            <span class="s2">"of arguments. Expected one of:</span><span class="se">\n</span><span class="s2">"</span>
            <span class="s2">" * (*, torch.device device, torch.dtype dtype)</span><span class="se">\n</span><span class="s2">"</span>
            <span class="s2">" * (int size, *, torch.device device, torch.dtype dtype)</span><span class="se">\n</span><span class="s2">"</span>
            <span class="s2">" * (Sequence data, *, torch.device device, torch.dtype dtype)</span><span class="se">\n</span><span class="s2">"</span>
            <span class="s2">" * (*, UntypedStorage wrap_storage, torch.dtype dtype)"</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">wrap_storage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="n">arg_error_msg</span>
                    <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">No positional arguments should be given when using "</span>
                    <span class="s2">"'wrap_storage'"</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="n">arg_error_msg</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Argument 'dtype' must be specified"</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="n">arg_error_msg</span>
                    <span class="o">+</span> <span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Argument 'dtype' must be torch.dtype, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="n">arg_error_msg</span>
                    <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Argument 'device' should not be specified when 'wrap_storage' is given"</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">wrap_storage</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="n">arg_error_msg</span>
                    <span class="o">+</span> <span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Argument 'wrap_storage' must be UntypedStorage, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">wrap_storage</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span> <span class="o">=</span> <span class="n">wrap_storage</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">device</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
            <span class="p">]:</span>
                <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">"Cannot create CUDA storage with quantized dtype"</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">_isint</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="p">(</span>
                        <span class="nb">int</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">collections</span><span class="o">.</span><span class="n">abc</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span> <span class="o">=</span> <span class="n">_get_storage_from_sequence</span><span class="p">(</span>
                        <span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                        <span class="n">arg_error_msg</span>
                        <span class="o">+</span> <span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Argument type not recognized: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">"</span>
                    <span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">arg_error_msg</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Too many positional arguments"</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cuda"</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_hpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"hpu"</span>

<div class="viewcode-block" id="TypedStorage.untyped"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.untyped">[docs]</a>    <span class="k">def</span> <span class="nf">untyped</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return the internal :class:`torch.UntypedStorage`."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span></div>

    <span class="k">def</span> <span class="nf">_new_wrapped_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">untyped_storage</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">untyped_storage</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span>

        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="n">TypedStorage</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">cast</span><span class="p">(</span>
                <span class="n">Self</span><span class="p">,</span>
                <span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)(</span><span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_maybe_wrap_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">is_stop</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_stop</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="mi">0</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">int</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"can't index a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_stop</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">())</span> <span class="ow">or</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()):</span>
                    <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">"index </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> out of range for storage of size </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">idx</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="n">idx</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">())</span> <span class="ow">or</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()):</span>
                    <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">"index </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> out of range for storage of size </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span>
                    <span class="p">)</span>
                <span class="k">return</span> <span class="n">idx</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_setitem</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_setitem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">slice</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"can't index a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_storage</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"cannot set item with value type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">interpret_dtypes</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">tmp_dtype</span> <span class="o">=</span> <span class="n">interpret_dtypes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>
            <span class="n">tmp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tmp_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">tmp_tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span>
                <span class="n">TypedStorage</span><span class="p">(</span>
                    <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tmp_dtype</span><span class="p">,</span> <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tmp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">tmp_tensor</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_getitem</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_getitem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"meta"</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Not available for 'meta' device type"</span><span class="p">)</span>

        <span class="c1"># NOTE: Before TypedStorage existed, indexing with a slice used to be</span>
        <span class="c1"># possible for &lt;type&gt;Storage objects. However, it would return</span>
        <span class="c1"># a storage view, which would be a hassle to implement in TypedStorage,</span>
        <span class="c1"># so it was disabled</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">slice</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">"slices are only supported in UntypedStorage.__getitem__"</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"can't index a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">interpret_dtypes</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">TypedStorage</span><span class="p">(</span>
                <span class="n">wrap_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">interpret_dtypes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">],</span>
                <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">_getitem</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

        <span class="n">idx_wrapped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_wrap_index</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch._subclasses.fake_tensor</span> <span class="kn">import</span> <span class="n">unset_fake_temporarily</span>

        <span class="k">with</span> <span class="n">unset_fake_temporarily</span><span class="p">():</span>
            <span class="n">tmp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">tmp_tensor</span><span class="p">[</span><span class="n">idx_wrapped</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<div class="viewcode-block" id="TypedStorage.copy_"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.copy_">[docs]</a>    <span class="k">def</span> <span class="nf">copy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">:</span> <span class="n">_Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">source</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="TypedStorage.nbytes"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.nbytes">[docs]</a>    <span class="k">def</span> <span class="nf">nbytes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nbytes</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_nbytes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>

<div class="viewcode-block" id="TypedStorage.type"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.type">[docs]</a>    <span class="k">def</span> <span class="nf">type</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">_Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">_StorageBase</span><span class="p">,</span> <span class="n">TypedStorage</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">legacy_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_legacy_storage_class</span><span class="p">()</span>

            <span class="k">if</span> <span class="n">legacy_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">legacy_class</span><span class="o">.</span><span class="vm">__module__</span> <span class="o">+</span> <span class="s2">"."</span> <span class="o">+</span> <span class="n">legacy_class</span><span class="o">.</span><span class="vm">__name__</span>

            <span class="k">return</span> <span class="s2">"."</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="vm">__module__</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">])</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.cuda"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.cuda">[docs]</a>    <span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"Cannot create CUDA storage with quantized dtype"</span><span class="p">)</span>
        <span class="n">cuda_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="n">cuda_storage</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.hpu"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.hpu">[docs]</a>    <span class="k">def</span> <span class="nf">hpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"Cannot create HPU storage with quantized dtype"</span><span class="p">)</span>
        <span class="n">hpu_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">hpu</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="n">hpu_storage</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.to"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.to">[docs]</a>    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">DeviceLikeType</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint8</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint4x2</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">quint2x4</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint32</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">"Cannot create </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2"> storage with quantized dtype"</span>
            <span class="p">)</span>
        <span class="n">to_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="n">to_storage</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.element_size"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.element_size">[docs]</a>    <span class="k">def</span> <span class="nf">element_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_size</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_element_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<div class="viewcode-block" id="TypedStorage.get_device"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.get_device">[docs]</a>    <span class="k">def</span> <span class="nf">get_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">_int</span><span class="p">:</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span></div>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="n">info_str</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">"[</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">(dtype=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, "</span>
            <span class="sa">f</span><span class="s2">"device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">) of size </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">]"</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"meta"</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">"...</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="n">info_str</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data_str</span> <span class="o">=</span> <span class="s2">" "</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2"> "</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
            <span class="k">return</span> <span class="n">data_str</span> <span class="o">+</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="n">info_str</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="k">def</span> <span class="nf">__copy__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_deepcopy</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="p">,</span> <span class="n">memo</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__sizeof__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__sizeof__</span><span class="p">()</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span>

<div class="viewcode-block" id="TypedStorage.clone"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.clone">[docs]</a>    <span class="k">def</span> <span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return a copy of this storage."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span></div>

<div class="viewcode-block" id="TypedStorage.tolist"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.tolist">[docs]</a>    <span class="k">def</span> <span class="nf">tolist</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return a list containing the elements of this storage."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.cpu"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.cpu">[docs]</a>    <span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Return a CPU copy of this storage if it's not already on the CPU."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span></div>

<div class="viewcode-block" id="TypedStorage.is_pinned"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.is_pinned">[docs]</a>    <span class="k">def</span> <span class="nf">is_pinned</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"cuda"</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""Determine whether the CPU TypedStorage is already pinned on device.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (str or torch.device): The device to pin memory on (default: ``'cuda'``).</span>
<span class="sd">                This argument is discouraged and subject to deprecated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A boolean variable.</span>
<span class="sd">        """</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">is_pinned</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.pin_memory"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.pin_memory">[docs]</a>    <span class="k">def</span> <span class="nf">pin_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"cuda"</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">"""Copy the CPU TypedStorage to pinned memory, if it's not already pinned.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (str or torch.device): The device to pin memory on (default: ``'cuda'``).</span>
<span class="sd">                This argument is discouraged and subject to deprecated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A pinned CPU storage.</span>
<span class="sd">        """</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new_wrapped_storage</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.share_memory_"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.share_memory_">[docs]</a>    <span class="k">def</span> <span class="nf">share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""See :meth:`torch.UntypedStorage.share_memory_`"""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_share_memory_</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_new_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Create a new storage in shared memory with the same data type."""</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="s2">"cpu"</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">untyped_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_new_shared</span><span class="p">(</span>
            <span class="n">size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">TypedStorage</span><span class="p">(</span>
            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_cdata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_cdata</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">device</span>

<div class="viewcode-block" id="TypedStorage.size"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.size">[docs]</a>    <span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_size</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># NB: don't indirect through __len__, as that requires</span>
        <span class="c1"># an int to be returned</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">nbytes</span><span class="p">()</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_size</span><span class="p">()</span>

<div class="viewcode-block" id="TypedStorage.pickle_storage_type"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.pickle_storage_type">[docs]</a>    <span class="k">def</span> <span class="nf">pickle_storage_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pickle_storage_type</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_pickle_storage_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_dtype_to_storage_type_map</span><span class="p">()[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"dtype </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> is not recognized"</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>

    <span class="k">def</span> <span class="nf">__reduce__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">_use_new_zipfile_serialization</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">_load_from_bytes</span><span class="p">,</span> <span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">getvalue</span><span class="p">(),))</span>

<div class="viewcode-block" id="TypedStorage.data_ptr"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.data_ptr">[docs]</a>    <span class="k">def</span> <span class="nf">data_ptr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_ptr</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_data_ptr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span>

<div class="viewcode-block" id="TypedStorage.resizable"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.resizable">[docs]</a>    <span class="k">def</span> <span class="nf">resizable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">resizable</span><span class="p">()</span></div>

<div class="viewcode-block" id="TypedStorage.resize_"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.resize_">[docs]</a>    <span class="k">def</span> <span class="nf">resize_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_resize_</span><span class="p">(</span><span class="n">size</span><span class="p">)</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_resize_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_size</span><span class="p">())</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_free_weak_ref</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_free_weak_ref</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_weak_ref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_weak_ref</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="TypedStorage.from_buffer"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.from_buffer">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_buffer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_buffer</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_buffer</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">cls</span> <span class="o">==</span> <span class="n">TypedStorage</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cpu"</span> <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">!=</span> <span class="s2">"cpu"</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">"TypedStorage.from_buffer: Not available for device </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2">"</span>
                <span class="p">)</span>
            <span class="n">untyped_storage</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">from_buffer</span><span class="p">(</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">"from_buffer: 'dtype' can only be specified in "</span>
                    <span class="s2">"UntypedStorage.from_buffer and TypedStorage.from_buffer"</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">"from_buffer: 'device' can only be specified in "</span>
                    <span class="s2">"UntypedStorage.from_buffer and TypedStorage.from_buffer"</span>
                <span class="p">)</span>

            <span class="n">dtype</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_dtype</span>
            <span class="n">untyped_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">from_buffer</span><span class="p">(</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">TypedStorage</span><span class="p">(</span><span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_internal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Argument 'dtype' must be torch.dtype, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">storage</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="o">.</span><span class="n">_typed_storage</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">():</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">storage</span>

<div class="viewcode-block" id="TypedStorage.double"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.double">[docs]</a>    <span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to double type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.float"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.float">[docs]</a>    <span class="k">def</span> <span class="nf">float</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.half"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.half">[docs]</a>    <span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to half type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.long"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.long">[docs]</a>    <span class="k">def</span> <span class="nf">long</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to long type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.int"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.int">[docs]</a>    <span class="k">def</span> <span class="nf">int</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to int type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.short"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.short">[docs]</a>    <span class="k">def</span> <span class="nf">short</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to short type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">short</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.char"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.char">[docs]</a>    <span class="k">def</span> <span class="nf">char</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to char type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.byte"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.byte">[docs]</a>    <span class="k">def</span> <span class="nf">byte</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to byte type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.bool"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.bool">[docs]</a>    <span class="k">def</span> <span class="nf">bool</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to bool type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.bfloat16"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.bfloat16">[docs]</a>    <span class="k">def</span> <span class="nf">bfloat16</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to bfloat16 type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.complex_double"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.complex_double">[docs]</a>    <span class="k">def</span> <span class="nf">complex_double</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to complex double type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cdouble</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.complex_float"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.complex_float">[docs]</a>    <span class="k">def</span> <span class="nf">complex_float</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to complex float type."""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cfloat</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.float8_e5m2"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.float8_e5m2">[docs]</a>    <span class="k">def</span> <span class="nf">float8_e5m2</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e5m2 type"""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.float8_e4m3fn"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.float8_e4m3fn">[docs]</a>    <span class="k">def</span> <span class="nf">float8_e4m3fn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e4m3fn type"""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.float8_e5m2fnuz"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.float8_e5m2fnuz">[docs]</a>    <span class="k">def</span> <span class="nf">float8_e5m2fnuz</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e5m2fnuz type"""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e5m2fnuz</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.float8_e4m3fnuz"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.float8_e4m3fnuz">[docs]</a>    <span class="k">def</span> <span class="nf">float8_e4m3fnuz</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Casts this storage to float8_e4m3fnuz type"""</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span><span class="p">)</span></div>

<div class="viewcode-block" id="TypedStorage.from_file"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.from_file">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_file</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">shared</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""from_file(filename, shared=False, size=0) -&gt; Storage</span>

<span class="sd">        Creates a CPU storage backed by a memory-mapped file.</span>

<span class="sd">        If ``shared`` is ``True``, then memory is shared between all processes.</span>
<span class="sd">        All changes are written to the file. If ``shared`` is ``False``, then the changes on</span>
<span class="sd">        the storage do not affect the file.</span>

<span class="sd">        ``size`` is the number of elements in the storage. If ``shared`` is ``False``,</span>
<span class="sd">        then the file must contain at least ``size * sizeof(Type)`` bytes</span>
<span class="sd">        (``Type`` is the type of storage). If ``shared`` is ``True`` the file will be created if needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            filename (str): file name to map</span>
<span class="sd">            shared (bool): whether to share memory (whether ``MAP_SHARED`` or ``MAP_PRIVATE`` is passed to the</span>
<span class="sd">                            underlying `mmap(2) call &lt;https://man7.org/linux/man-pages/man2/mmap.2.html&gt;`_)</span>
<span class="sd">            size (int): number of elements in the storage</span>
<span class="sd">        """</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">cls</span> <span class="o">==</span> <span class="n">TypedStorage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">"from_file can only be called on derived classes"</span><span class="p">)</span>
        <span class="n">untyped_storage</span> <span class="o">=</span> <span class="n">UntypedStorage</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span>
            <span class="n">filename</span><span class="p">,</span> <span class="n">shared</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">storage</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">storage</span></div>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_expired</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_expired</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_write_file</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_write_file</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_from_file</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_set_from_file</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_cdata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_set_cdata</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_share_cuda_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_share_cuda_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="TypedStorage.is_shared"><a class="viewcode-back" href="../../storage.html#torch.TypedStorage.is_shared">[docs]</a>    <span class="k">def</span> <span class="nf">is_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">_warn_typed_storage_removal</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_shared</span><span class="p">()</span></div>

    <span class="c1"># For internal use only, to avoid deprecation warning</span>
    <span class="k">def</span> <span class="nf">_is_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">is_shared</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_shared_cuda</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_new_shared_cuda</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_share_filename_cpu_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="p">(</span>
            <span class="n">manager_handle</span><span class="p">,</span>
            <span class="n">storage_handle</span><span class="p">,</span>
            <span class="n">size</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_share_filename_cpu_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">manager_handle</span><span class="p">,</span> <span class="n">storage_handle</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_size</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_shared_decref</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_shared_decref</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_release_ipc_counter</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_release_ipc_counter_cuda</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_shared_incref</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_shared_incref</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_share_fd_cpu_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">fd</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_untyped_storage</span><span class="o">.</span><span class="n">_share_fd_cpu_</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">fd</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_element_size</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_get_legacy_storage_class</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_dtype_to_storage_type_map</span><span class="p">():</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">storage_name</span> <span class="o">=</span> <span class="n">_dtype_to_storage_type_map</span><span class="p">()[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">"cpu"</span><span class="p">,</span>
            <span class="s2">"cuda"</span><span class="p">,</span>
            <span class="s2">"hpu"</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_privateuse1_backend_name</span><span class="p">(),</span>
        <span class="p">]:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="n">module</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">"cpu"</span> <span class="k">else</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">storage_name</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span></div>


<span class="n">TypedStorage</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">_type</span><span class="o">.</span><span class="vm">__doc__</span>
<span class="n">TypedStorage</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">_StorageBase</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="vm">__doc__</span>
<span class="n">TypedStorage</span><span class="o">.</span><span class="n">hpu</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">_StorageBase</span><span class="o">.</span><span class="n">hpu</span><span class="o">.</span><span class="vm">__doc__</span>
<span class="n">TypedStorage</span><span class="o">.</span><span class="n">to</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">_to</span><span class="o">.</span><span class="vm">__doc__</span>


<span class="k">class</span> <span class="nc">_LegacyStorageMeta</span><span class="p">(</span><span class="nb">type</span><span class="p">):</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">def</span> <span class="fm">__instancecheck__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">instance</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">instance</span><span class="p">)</span> <span class="o">==</span> <span class="n">TypedStorage</span><span class="p">:</span>
            <span class="n">cls_device</span> <span class="o">=</span> <span class="n">_get_device_from_module</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__module__</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">cls_device</span> <span class="o">==</span> <span class="n">instance</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="bp">cls</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">instance</span><span class="o">.</span><span class="n">dtype</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">class</span> <span class="nc">_LegacyStorage</span><span class="p">(</span><span class="n">TypedStorage</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">_LegacyStorageMeta</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_shared</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""Create a new storage in shared memory with the same data type."""</span>
        <span class="n">untyped_storage</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_new_shared</span><span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="bp">cls</span><span class="p">()</span><span class="o">.</span><span class="n">_element_size</span><span class="p">())</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">wrap_storage</span><span class="o">=</span><span class="n">untyped_storage</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_release_ipc_counter</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_release_ipc_counter_cuda</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_new_shared_filename</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">manager</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="n">bytes_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_element_size</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">wrap_storage</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">UntypedStorage</span><span class="o">.</span><span class="n">_new_shared_filename_cpu</span><span class="p">(</span>
                <span class="n">manager</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">bytes_size</span>
            <span class="p">)</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_dtype_from_pickle_storage_type</span><span class="p">(</span><span class="n">pickle_storage_type</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_storage_type_to_dtype_map</span><span class="p">()[</span><span class="n">pickle_storage_type</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s1">'pickle storage type "</span><span class="si">{</span><span class="n">pickle_storage_type</span><span class="si">}</span><span class="s1">" is not recognized'</span>
        <span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
</pre>
        </div>
       </article>
      </div>
      <footer>
       <hr/>
       <div role="contentinfo">
        <p>
         © Copyright PyTorch Contributors.
        </p>
       </div>
       <div>
        Built with
        <a href="http://sphinx-doc.org/">
         Sphinx
        </a>
        using a
        <a href="https://github.com/rtfd/sphinx_rtd_theme">
         theme
        </a>
        provided by
        <a href="https://readthedocs.org">
         Read the Docs
        </a>
        .
       </div>
      </footer>
     </div>
     <script>
      var match = window.location.href.match(/\/_[a-zA-Z0-9_]*.html|_dynamo/gi);
var url = window.location.href.lastIndexOf(match[match.length-1]);

if (url)
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-exclamation-circle" aria-hidden="true">&nbsp</i> This page describes an internal API which is not intended to be used outside of the PyTorch codebase and can be modified or removed without notice.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
     </script>
    </div>
    <div class="pytorch-content-right" id="pytorch-content-right">
     <div class="pytorch-right-menu" id="pytorch-right-menu">
      <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
      </div>
     </div>
    </div>
   </section>
  </div>
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js" type="text/javascript">
  </script>
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js">
  </script>
  <script src="../../_static/jquery.js">
  </script>
  <script src="../../_static/underscore.js">
  </script>
  <script src="../../_static/_sphinx_javascript_frameworks_compat.js">
  </script>
  <script src="../../_static/doctools.js">
  </script>
  <script src="../../_static/sphinx_highlight.js">
  </script>
  <script src="../../_static/clipboard.min.js">
  </script>
  <script src="../../_static/copybutton.js">
  </script>
  <script src="../../_static/js/vendor/popper.min.js" type="text/javascript">
  </script>
  <script src="../../_static/js/vendor/bootstrap.min.js" type="text/javascript">
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js">
  </script>
  <script src="../../_static/js/theme.js" type="text/javascript">
  </script>
  <script type="text/javascript">
   jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
  <script script="" type="text/javascript">
   var collapsedSections = ['Developer Notes', 'Language Bindings', 'Libraries', 'Community'];
  </script>
  <img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1"/>
  <!-- Begin Footer -->
  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
   <div class="container">
    <div class="row">
     <div class="col-md-4 text-center">
      <h2>
       Docs
      </h2>
      <p>
       Access comprehensive developer documentation for PyTorch
      </p>
      <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">
       View Docs
      </a>
     </div>
     <div class="col-md-4 text-center">
      <h2>
       Tutorials
      </h2>
      <p>
       Get in-depth tutorials for beginners and advanced developers
      </p>
      <a class="with-right-arrow" href="https://pytorch.org/tutorials">
       View Tutorials
      </a>
     </div>
     <div class="col-md-4 text-center">
      <h2>
       Resources
      </h2>
      <p>
       Find development resources and get your questions answered
      </p>
      <a class="with-right-arrow" href="https://pytorch.org/resources">
       View Resources
      </a>
     </div>
    </div>
   </div>
  </div>
  <footer class="site-footer">
   <div class="container footer-container">
    <div class="footer-logo-wrapper">
     <a class="footer-logo" href="https://pytorch.org/">
     </a>
    </div>
    <div class="footer-links-wrapper">
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        <a href="https://pytorch.org/">
         PyTorch
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/get-started">
         Get Started
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/features">
         Features
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/ecosystem">
         Ecosystem
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/blog/">
         Blog
        </a>
       </li>
       <li>
        <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">
         Contributing
        </a>
       </li>
      </ul>
     </div>
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        <a href="https://pytorch.org/resources">
         Resources
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials">
         Tutorials
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/docs/stable/index.html">
         Docs
        </a>
       </li>
       <li>
        <a href="https://discuss.pytorch.org" target="_blank">
         Discuss
        </a>
       </li>
       <li>
        <a href="https://github.com/pytorch/pytorch/issues" target="_blank">
         Github Issues
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">
         Brand Guidelines
        </a>
       </li>
      </ul>
     </div>
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        Stay up to date
       </li>
       <li>
        <a href="https://www.facebook.com/pytorch" target="_blank">
         Facebook
        </a>
       </li>
       <li>
        <a href="https://twitter.com/pytorch" target="_blank">
         Twitter
        </a>
       </li>
       <li>
        <a href="https://www.youtube.com/pytorch" target="_blank">
         YouTube
        </a>
       </li>
       <li>
        <a href="https://www.linkedin.com/company/pytorch" target="_blank">
         LinkedIn
        </a>
       </li>
      </ul>
     </div>
     <div class="footer-links-col">
      <ul>
       <li class="list-title">
        PyTorch Podcasts
       </li>
       <li>
        <a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">
         Spotify
        </a>
       </li>
       <li>
        <a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">
         Apple
        </a>
       </li>
       <li>
        <a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">
         Google
        </a>
       </li>
       <li>
        <a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">
         Amazon
        </a>
       </li>
      </ul>
     </div>
    </div>
    <div class="privacy-policy">
     <ul>
      <li class="privacy-policy-links">
       <a href="https://www.linuxfoundation.org/terms/" target="_blank">
        Terms
       </a>
      </li>
      <li class="privacy-policy-links">
       |
      </li>
      <li class="privacy-policy-links">
       <a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">
        Privacy
       </a>
      </li>
     </ul>
    </div>
    <div class="copyright">
     <p>
      © Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
      <a href="https://www.linuxfoundation.org/policies/">
       www.linuxfoundation.org/policies/
      </a>
      . The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see
      <a href="https://www.lfprojects.org/policies/">
       www.lfprojects.org/policies/
      </a>
      .
     </p>
    </div>
   </div>
  </footer>
  <div class="cookie-banner-wrapper">
   <div class="container">
    <p class="gdpr-notice">
     To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls:
     <a href="https://www.facebook.com/policies/cookies/">
      Cookies Policy
     </a>
     .
    </p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg"/>
   </div>
  </div>
  <!-- End Footer -->
  <!-- Begin Mobile Menu -->
  <div class="mobile-main-menu">
   <div class="container-fluid">
    <div class="container">
     <div class="mobile-main-menu-header-container">
      <a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/">
      </a>
      <a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#">
      </a>
     </div>
    </div>
   </div>
   <div class="mobile-main-menu-links-container">
    <div class="main-menu">
     <ul>
      <li class="resources-mobile-menu-title">
       <a>
        Learn
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/get-started">
         Get Started
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials">
         Tutorials
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">
         Learn the Basics
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">
         PyTorch Recipes
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tutorials/beginner/introyt.html">
         Introduction to PyTorch - YouTube Series
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Ecosystem
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/ecosystem">
         Tools
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/#community-module">
         Community
        </a>
       </li>
       <li>
        <a href="https://discuss.pytorch.org/">
         Forums
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/resources">
         Developer Resources
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/ecosystem/contributor-awards-2023">
         Contributor Awards - 2024
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Edge
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/edge">
         About PyTorch Edge
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/executorch-overview">
         ExecuTorch
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/executorch/stable/index.html">
         ExecuTorch Documentation
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Docs
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/docs/stable/index.html">
         PyTorch
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/pytorch-domains">
         PyTorch Domains
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        Blog &amp; News
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/blog/">
         PyTorch Blog
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/community-blog">
         Community Blog
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/videos">
         Videos
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/community-stories">
         Community Stories
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/events">
         Events
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/newsletter">
         Newsletter
        </a>
       </li>
      </ul>
      <li class="resources-mobile-menu-title">
       <a>
        About
       </a>
      </li>
      <ul class="resources-mobile-menu-items">
       <li>
        <a href="https://pytorch.org/foundation">
         PyTorch Foundation
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/governing-board">
         Governing Board
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/credits">
         Cloud Credit Program
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/tac">
         Technical Advisory Council
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/staff">
         Staff
        </a>
       </li>
       <li>
        <a href="https://pytorch.org/contact-us">
         Contact Us
        </a>
       </li>
      </ul>
     </ul>
    </div>
   </div>
  </div>
  <!-- End Mobile Menu -->
  <script src="../../_static/js/vendor/anchor.min.js" type="text/javascript">
  </script>
  <script type="text/javascript">
   $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
 </body>
</html>
